\chapter{Results}\label{chap:results}

In this chapter, we will introduce the results of the experiments subjecting
protein representations to various relevant perturbation regimes highlighted in
chapter \ref{chap:methods}. We first discuss the results and implications of
frequently used \acrshort{mmd} configurations, namely by showing how it behaves
on protein graph descriptors. We then move on to show the results of the
sensitivity of the \acrshort{mmd} values depending on the underlying graph
representation used to extract the various graphs. Finally, we explore more
exotic configurations of \acrshort{mmd} that we hypothesize might be more
suitable for proteins. We conclude this chapter with a short section on the
runtimes of the various elements of the computational pipelines shown in this
chapter.

\section{Overall \acrshort{mmd} Behavior}

\paragraph{General observations on the correlation coefficients} Surprisingly,
we found that the behavior of \acrshort{mmd} was not as inconsistent for the
types of graphs extracted from proteins as was found on synthetic graphs by
\cite{obray2022evaluation}. Figure \ref{fig:mmd_consistent_eps} show
trajectories and correlations of \acrshort{mmd} values with different
perturbation types using $\varepsilon$-graphs with $\varepsilon$ set to $8$
\si{\angstrom}. Both the Spearman and Pearson correlation coefficients averaged
across runs are high. There is, however, an exception: the correlation between
the \acrshort{mmd} obtained from the degree histogram and the addition of edges
is comparatively low with $\rho_P=0.25$ and $\rho_S=-0.44$ versus that obtained
from the Laplacian spectrum histogram (with $\rho_P=0.95$, $\rho_S=0.98$), and
the clustering histogram (with $\rho_P=0.95$, $\rho_S=0.97$). Curiously, the
next lowest (Pearson) correlation coefficient from Figure
\ref{fig:mmd_consistent_eps} is also associated to the degree distribution
histogram under the rewiring perturbation regime ($\rho_p=0.82$ $\rho_S=0.98$).

\paragraph{General observations on the standard deviations} In parallel, Table
\ref{tab:1_1_std} also supports the fact that the standard deviation of the
normalized \acrshort{mmd} of the degree distribution descriptor is the highest,
suggesting that this is not a very robust descriptor. Since modelling graph
connectivity is one of the primary challenges of generative graph models
\citep{li2018learning}, based on these results we do not recommend using a
degree histogram as a descriptor function for \acrshort{mmd}. The remaining
descriptors do show high correlations ($\rho_P\geq 0.89$, $\rho_S\geq 0.97$) and
reasonably low standard deviations ($>0.64$), which make them good candidates.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_1_1.pdf}
  \caption[Overall behaviour of \acrshort{mmd} using graph-based
descriptors.]{\acrshort{mmd} vs. perturbation (in \% of the maximum values shown
  in Table \ref{tab:perturbation_ranges}) for various graph
descriptors of the 8\si{\angstrom}-graphs under different perturbations regimes.
The kernel used to obtain these graphs is the RBF kernel with bandwidth 0.01.
$\rho_{S}$: average Spearman correlation coefficient across runs. $\rho_{P}$:
average Pearson correlation coefficient across runs. Except for the degree
histogram behaviour when edges are added, we see that most \acrshort{mmd}
configurations behave well, i.e. there is a high correlation betweeen the
\acrshort{mmd} values and the perturbation.}
  \label{fig:mmd_consistent_eps}
\end{figure}

\begin{table}
  \centering
  \scalebox{0.75}{
  \begin{tabular}{llr}
    \toprule
    \textbf{Perturbation Type} & \textbf{Descriptor} & $\sigma_\MMD$ \\
    \midrule
Add Edges & Clustering Histogram &        0.023058 \\
      & Degree Histogram &        0.024943 \\
      & Laplacian Spectrum Histogram &        0.039320 \\
Gaussian Noise & Clustering Histogram &        0.010556 \\
      & Degree Histogram &        0.012793 \\
      & Laplacian Spectrum Histogram &        0.027411 \\
Remove Edges & Clustering Histogram &        0.009075 \\
      & Degree Histogram &        0.004400 \\
      & Laplacian Spectrum Histogram &        0.018033 \\
Rewire Edges & Clustering Histogram &        0.028672 \\
      & Degree Histogram &        \textbf{0.103612} \\
      & Laplacian Spectrum Histogram &        0.032276 \\
Shear & Clustering Histogram &        0.030793 \\
      & Degree Histogram &        0.041350 \\
      & Laplacian Spectrum Histogram &        0.040984 \\
Taper & Clustering Histogram &        0.025951 \\
      & Degree Histogram &        0.035282 \\
      & Laplacian Spectrum Histogram &        0.043468 \\
Twist & Clustering Histogram &        0.052784 \\
      & Degree Histogram &        0.081318 \\
    & Laplacian Spectrum Histogram &        0.063795 \\
    \bottomrule
  \end{tabular}
}
  \caption[Average standard deviation of the various MMD configurations
  shown in Figure \ref{fig:mmd_consistent_eps} under the same perturbation
  types.]{Average standard deviation of the various MMD configurations shown
    in Figure \ref{fig:mmd_consistent_eps} under the same perturbation
    types. The highest standard deviation is observed for the degree histogram
    descriptor under rewiring perturbations.}

  \label{tab:1_1_std}
\end{table}
% TODO: shave significant digits
\clearpage

\paragraph{Influence of the choice of kernel} We next investigate the overall
influence of the kernel on the correlations between perturbations and
\acrshort{mmd} shown in Figure \ref{fig:mmd_effect_kernel}. We can see that for
$\sigma\lessapprox 0.1$ ($\sigma$ being the hyperparameter of the Gaussian
kernel, see Section \ref{sec:kernels}) and for the linear kernel, \acrshort{mmd}
values behave as desired, i.e. $\rho_P, \rho_S\geq 0.8$ and, if one excludes the
Laplacian spectrum histogram at $\sigma=0.1$, we have $\rho_P,\rho_S>0.95$.
However, correlation coefficients drop sharply when increasing $\sigma>0.1$,
most likely due to oversmoothing, which is a phenomenon arising when the
bandwidth of the kernel is large enough to obscure any structure in the data
\citep{hwang1994nonparametric}. This can have unpredictable consequences on
resulting MMD values: in the case of the degree histogram or the clustering
histogram, this results in an overly sensitive kernel sharply increasing in
value at the slightest perturbation, while the clustering histogram remains
oblivious to large amounts of perturbation. This can be explained by the
relative scale of each of the embeddings and their pairwise distances, which we
catalogue in Appendix \ref{sec:distance_dist}. % TODO: flesh this out a bit further maybe.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_1_2.pdf}
  \caption[Influence of kernel parameters on \acrshort{mmd} behaviour.]{\acrshort{mmd} vs. Gaussian
    noise perturbation (in \% of the maximum values shown
    in Table \ref{tab:perturbation_ranges}) for various graph descriptors of the
    8\si{\angstrom}-graphs. $\rho_{S}$: average Spearman correlation coefficient
    across runs. $\rho_{P}$: average Pearson correlation coefficient across runs.
    The kernel here is shown on top of each subplots. We can see that reasonable
    behaviour of the RBF kernel can be seen when $\sigma<1$. The linear kernel also
    behaves well.}
  \label{fig:mmd_effect_kernel}
\end{figure}

\paragraph{Comparing graph construction techniques from proteins} To compare which graph
construction technique was overall most adviseable, we computed the correlation
coefficients of the various available combinations of graph type, graph
extraction parameter, and description function with an RBF kernel with
$\sigma=0.01$, which was shown to behave reasonably stably across descriptors
(Figure \ref{fig:mmd_effect_kernel}). We then compared the distributions of the
two correlation coefficients and computed a Mann-Whitney $\mathcal{U}$ test
\citep{fay2010wilcoxon}. Figure \ref{fig:correlations_graph_construction} shows
the distributions of both $\rho_S$ and $\rho_P$ for the $k$-NN and
$\varepsilon$-graphs. Both the test for the Pearson correlation coefficient and
the Spearman correlation coefficient were significant ($p=8.35\cdot 10^{-4}$ and
$p=1.765\cdot 10^{-2}$, respectively) indicating that one distribution is
stochastically greater than the other. In both cases, the distribution of the
correlation coefficients from the $\varepsilon$-graphs is higher, as we can see
in the legend of Figure \ref{fig:correlations_graph_construction}. This result
is intuitive, because $\epsilon$-graphs are likely more sensitive to the
underlying topology of the protein. We therefore proceed with $\epsilon$-graphs
in the subsequent results discussed below.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{./figures/results/compare_corrs_graph_construction.pdf}
  \caption[Violin plot of the distributions of the correlation coefficients of
various \acrshort{mmd} configurations derived from the two different graph
construction methods.]{Violin plot of the distributions of the correlation
coefficients of various \acrshort{mmd} configurations derived from the two
different graph construction methods. Each distribution contains various
combinations of descriptor functions, perturbation types, and various values of
the parameter used to extract the graphs (i.e. $k$ in the case of $k$-NN graphs
and $\varepsilon$ in the case of $\varepsilon$-graphs). The distributions are
then compared using a Mann-Whitney $U$ test, yielding significant $p$-values for
both $\rho_S$ and $\rho_P$ ($p=8.35\cdot 10^{-4}$ and $p=1.765\cdot 10^{-2}$,
respectively), indicating that the correlation coefficients obtained from $k$-NN
graphs are statisitcally significantly worse than those obtained from
$\varepsilon$-graphs. }
  \label{fig:correlations_graph_construction}
\end{figure}
% TODO: discuss the idea of the M-W U test with Tim




% $k$-NN graphs seem to behave similarly to $\varepsilon$-graphs, in that they are
% able to detect perturbations - see Figure \ref{fig:mmd_k_nn_graphs} for an
% example with 2-NN graphs. However, they are extremely sensitive to Gaussian
% noise and to the addition of edges (see the upper left and the upper mid pane of Figure
% \ref{fig:mmd_k_nn_graphs}). Presumably, this is because very small amounts of
% noise ($<4$\si{\angstrom}) affect the direct local neighborhood of each node
% very significantly in such a way that the resulting $k$-NN graph is rapidly
% disrupted, which is not necessarily the case with other perturbations. We note
% that overall the $\varepsilon$-graphs behave more consistently than $k$-NN
% graphs, with 13 out of 21 Pearson's correlation coefficients being higher in
% $\varepsilon$-graphs compared to $k$-NN graphs and 11 out of 21 Spearman's
% correlation coefficients being higher in 8\si{\angstrom}-graphs compared to 2-NN
% graphs. Even when the $\varepsilon$-graphs did not compare favorably, it was by
% a considerably lower margin than $k$-NN graphs in a similar setting. This effect
% seems to be mitigated by increasing $k$, but this comes at the cost of a lower
% Spearman correlation coefficient, which is even less desirable - see Figure
% \ref{fig:k_vs_turbulence_gaussian_noise}. Changing kernel parameters did not
% alleviate the pathologies (see Supplementary Figure
% \ref{fig:mmd_effect_kernel_knn}) and had the same overall effect as observed in
% Figure \ref{fig:mmd_effect_kernel}. We, therefore, proceed with further analysis
% using $\varepsilon$-graphs.


% \acrshort{mmd} is quite stable, somewhat contradicts what obray is saying. Maybe due to
% the different nature of the graphs.
\clearpage
\section{Influence of the Graph Representation on the Sensitivity to
Perturbations}\label{sec:results_sensitivity}

\paragraph{Lower values of $\varepsilon$ are more stable} In the previous paragraph, we established that $\varepsilon$-graphs were more
appropriate to compute the \acrshort{mmd}. Now, we investigate which specific
threshold(s) $\varepsilon$ are most appropriate using the same meta-metrics as
before. Figure \ref{fig:mmd_sensitivity_eps} shows the normalized MMD values
with the varying degree of different types of perturbations with various graph
descriptors. While most configurations exhibit high correlations: excluding
seven outliers of the 72 coefficients calculated, we have $\rho_P>0.97$ and
$\rho_S>0.98$. In Figure \ref{fig:mmd_sensitivity_eps}, we can also see that
lower coefficients tend to be reached when using a high threshold $\varepsilon$.
For instance, in the case of the twisting perturbation and using the clustering
histogram as a graph descriptor, we see $\rho_P=0.73$ and $\rho_S=0.71$ for 32
\si{\angstrom}-graphs vs $\rho_P=0.96$ and $\rho_S=1.0$ for both 16 and 8
\si{\angstrom} graphs. While this is the most extreme example, we also see a
similar pattern when using a different descriptor, such as the Laplacian
spectrum histogram, where we have $\rho_P=\rho_S=0.93$ for 32\si{\angstrom}
graphs vs $\rho_P=0.99 \rho_S=1.0$ for 16\si{\angstrom} graphs and $\rho_P=1.0
\rho_S=1.0$ for 8-\si{\angstrom} graphs. While there are some exceptions to this
pattern, the differences are not nearly as substantial (the highest difference
where the correlation coefficient for the 32-\si{\angstrom} graph is higher than
the other two $\varepsilon$ thresholds is 0.03, see lower mid pane of Figure
\ref{fig:mmd_sensitivity_eps}).

The finding that increasing $\varepsilon$ decreases the overall quality of MMD
is further supported by the changes in standard deviation of the different runs
averaged across the applied perturbation range, which is summarized in Table
\ref{tab:std_dev_epsilon_influence}. In this table, we can see that higher
values of $\varepsilon$ almost consistently incur a higher standard deviation,
i.e. we almost always have $\sigma_{\MMD, 32-\AA}>\sigma_{\MMD,
16-\AA}>\sigma_{\MMD, 8-\AA}$. The degree histogram descriptor under shearing
and tapering perturbations seems to be the two exceptions out of 12 cases.
Interestingly, the standard deviations were highest when subjecting proteins to
the twisting perturbation, most likely due to the fact that the spheres used to
construct the graphs most likely increasingly overlap when some degree of twist
is applied. In short, the conclusion of the last two paragraphs is that the
sparser the graph representation by lowering $\varepsilon$, the more stable the
resulting MMD.
% TODO: change MMD with acrshort.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_2_1.pdf}
  \caption[Influence of $\varepsilon$ on the sensitivity of \acrshort{mmd} to perturbations.]{\acrshort{mmd} vs. point cloud perturbation for various descriptors. In general,
when graphs are extracted with a lower $\varepsilon$ value, the \acrshort{mmd} curve
increases more rapidly. The only exception to this trend is the Laplacian
spectrum histogram descriptor under the tapering perturbation.}
  \label{fig:mmd_sensitivity_eps}
\end{figure}

\begin{table}
  \centering
  \scalebox{0.75}{
  \begin{tabular}{lllr}
    \toprule
    Perturbation Type & Descriptor Function & $\varepsilon$ & $\sigma_\MMD$ \\
    \midrule
    Gaussian Noise & Clustering Histogram & 8  &           0.011 \\
    &                              & 16 &           0.015 \\
    &                              & 32 &           0.043 \\
    & Degree Histogram & 8  &           0.013 \\
    &                              & 16 &           0.017 \\
    &                              & 32 &           0.024 \\
    & Laplacian Spectrum Histogram & 8  &           0.027 \\
    &                              & 16 &           0.030 \\
    &                              & 32 &           0.034 \\
    Shear & Clustering Histogram & 8  &           0.031 \\
    &                              & 16 &           0.049 \\
    &                              & 32 &           0.055 \\
    & Degree Histogram & 8  &           0.041 \\
    &                              & 16 &           0.038 \\
    &                              & 32 &           0.030 \\
    & Laplacian Spectrum Histogram & 8  &           0.041 \\
    &                              & 16 &           0.048 \\
    &                              & 32 &           0.050 \\
    Taper & Clustering Histogram & 8  &           0.026 \\
    &                              & 16 &           0.031 \\
    &                              & 32 &           0.088 \\
    & Degree Histogram & 8  &           0.035 \\
    &                              & 16 &           0.049 \\
    &                              & 32 &           0.044 \\
    & Laplacian Spectrum Histogram & 8  &           0.043 \\
    &                              & 16 &           0.053 \\
    &                              & 32 &           \textbf{0.089} \\
    Twist & Clustering Histogram & 8  &           0.053 \\
    &                              & 16 &           \textbf{0.082} \\
    &                              & 32 &           \textbf{0.179} \\
    & Degree Histogram & 8  &           \textbf{0.081} \\
    &                              & 16 &           \textbf{0.103} \\
    &                              & 32 &           \textbf{0.104} \\
    & Laplacian Spectrum Histogram & 8  &           0.064 \\
    &                              & 16 &           \textbf{0.145} \\
    &                              & 32 &           \textbf{0.229} \\
    \bottomrule
  \end{tabular}
}
\caption[Inter-run standard deviation values averaged across the whole
pertubation range for all combinations of perturbation type, descriptor
functions, and $\varepsilon$ values.]{Inter-run standard deviation values
averaged across the whole pertubation range for all combinations of perturbation
type, descriptor functions, and $\varepsilon$ values. Values higher than $0.08$
are in bold. Twisting perturbations show particularly high average standard
deviations $>0.05$, and higher $\varepsilon$ values also shows the highest
standard deviation values $>0.1$ for the twisting perturbations. In almost all
cases, we have $\sigma_{\MMD, 32-\AA}>\sigma_{\MMD, 16-\AA}>\sigma_{\MMD,
8-\AA}$. The degree histogram descriptor under shearing and tapering
perturbations seems to be the exceptions.}
  \label{tab:std_dev_epsilon_influence}
\end{table}


\paragraph{Lower $\varepsilon$ are more sensitive to smaller perturbations than
higher $\varepsilon$} While we noted that choosing a lower $\varepsilon$ value
to extract the graph would likely improve the stability of resulting MMDs, there
is also another consideration when choosing a graph representation. As shown in
Figure \ref{fig:mmd_sensitivity_eps}, when 20\% of the maximum amount of
perturbation applied\footnote{See Table \ref{tab:perturbation_ranges} for the
respective maxima.}, in seven out of 12 cases, the normalized MMD
8\si{\angstrom} is higher than that of the 16- or 32 \si{\angstrom}-graphs. In
addition, we find that, generally, in lower perturbation regimes, higher
percentages of the normalized MMD are reached with graphs constructed with a
lower $\varepsilon$. Figure \ref{fig:difference_sensitivity_20_percent}
illustrates this phenomenon. By grouping each configuration of MMD under
different perturbation types at the 20\% mark of the maximum perturbation
applied, we see that the 8\si{\angstrom}- and 16\si{\angstrom} graphs are
significantly higher than that of the 32\si{\angstrom} graphs (Mann-Whitney
$\mathcal{U}$ test: $p=1.08\cdot 10^{-4}$ and $p=5.56\cdot 10^{-3}$,
respectively). In short, in addition to being overall more stable, lower
$\varepsilon$ values and sparser subsequent graphs also tend to be better at
detecting smaller changes in protein topologies than larger $\varepsilon$ values
and denser graphs.


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/swarmplot_2_1.pdf}
  \caption{Normalized MMD value at 20\% of the maximum
    perturbation amount (as shown in Table \ref{tab:perturbation_ranges}) for
    various MMD configurations at different $\varepsilon$ values. The difference
    between the 8\si{\angstrom}- and 32\si{\angstrom} graphs is significant
    (Mann-Whitney $\mathcal{U}$-test $p=1.08\cdot 10^{-4}$). The difference
    between the 16\si{\angstrom}- and 32\si{\angstrom} graphs is also
    significant ($p=5.56\cdot 10^{-3}$). However, the difference between the
    8\si{\angstrom}- and 16\si{\angstrom} graphs was not signicant ($p=3.47\cdot
    10^{-1}, 5\cdot 10^{-2}$ is our highest significance threshold.)}
  \label{fig:difference_sensitivity_20_percent}
\end{figure}

\clearpage



% TODO: the sparser, the better?

% Among these outliers, we also see that the correlations are
% substantially lower when perturbing proteins with the twisting perturbation for
% the 32-\si{\angstrom}-graphs (the lowest are $\rho_P=0.73$ and $\rho_S=0.71$
% when using the clustering histogram as a graph descriptor).
% The overall quality
% of the \acrshort{mmd} configurations with various $\varepsilon$ thresholds can be further
% assessed using the standard deviation of the \acrshort{mmd} values across runs over the
% range of perturbations applied, which we summarize in Table
% \ref{tab:std_dev_epsilon_influence}. From

% we see that twisting
% perturbations show particularly high average standard deviations $>0.05$, and
% higher $\varepsilon$ values also show higher standard deviation values
% (culminating above $0.1$ for the twisting perturbation) for the twisting perturbations. In almost all cases, we have
% $\sigma_{\MMD, 32-\AA}>\sigma_{\MMD, 16-\AA}>\sigma_{\MMD, 8-\AA}$. The degree
% histogram descriptor under shearing and tapering perturbations seems to be the
% exceptions.


% TODO: erase/rewrite

% In the last section, we discussed the notion of sensitivity of a particular
% \acrshort{mmd} configuration to perturbations. We explore this idea further
% here. Specifically, we investigate the impact of the $\varepsilon$ threshold on
% the resulting sensitivity of the \acrshort{mmd} configuration to various
% perturbations affecting the underlying point cloud. Figure
% \ref{fig:mmd_sensitivity_eps} shows that, in general, the lower the
% $\varepsilon$, the more sensitive the \acrshort{mmd} to perturbations. Low
% sensitivity to perturbation might be useful in the early stages of modeling when
% generated samples only need to \emph{coarsely} match the reference samples.
% Conversely, a higher sensitivity to perturbations might be useful when trying to
% \emph{finely} distinguish a set of anomalous samples from the reference samples.
% Additionally, we can see that under the Gaussian noise, twisting and tapering
% perturbation, larger values of $\varepsilon$ introduce larger variations in
% \acrshort{mmd} values.



\section{Graph Kernels}\label{sec:results_graph_kernels}
% TODO compare WL with ESM (no modelling bias for WL on paper, might be a
% weakness since some mutations might not have a functional effect.)

The results of the perturbation experiments using the Weisfeiler-Lehman kernel
(see Sections \ref{sec:kernels} and \ref{sec:methods_kernels}) in MMD can be
seen in Figure \ref{fig:wlk}. Two important takeaways can be

% Lead time

% Correlations


Contrary to the tendencies observed in Section
\ref{sec:results_sensitivity}, \acrshort{mmd}s obtained using the
Weisfeiler-Lehman kernels on 8\si{\angstrom} graphs are very \emph{insensitive} to
medium levels of perturbations (the resulting curves only start to increase
anywhere starting from 5\% to 85\% of the perturbation range), and seems to even completely oblivious to rewiring of
graphs and generally insensitive to graph perturbations unless highly perturbed
(see upper row of Figure \ref{fig:wlk}).


The most likely explanation is that the
diversity of Weisfeiler-Lehman hashes (i.e. neighborhoods) observed within
perturbed or unperturbed samples is relatively high, which makes distinguishing
between the two a high threshold to overcome. This drawback could disappear if
one is working on a more targeted set of morphologically similar proteins with a
lower diversity of Weisfeiler-Lehman hashes, but this is beyond the scope of the
thesis.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_3.pdf}
  \caption[Normalized \acrshort{mmd} values using the Weisfeiler-Lehman kernel subject to
  various perturbations.]{\acrshort{mmd} vs.
perturbations using the Weisfeiler-Lehman kernel using the
8\si{\angstrom}-graphs as inputs. We see that high levels of perturbations are
required to raise the normalized \acrshort{mmd} values. We also see that \acrshort{mmd}s computed with
the Weisfeiler-Lehman kernel are insensitive to the rewiring of edges, and
results in \acrshort{mmd}s with a high inter-run variance.}
  \label{fig:wlk}
\end{figure}


\begin{table}
  \centering
  \scalebox{0.75}{
  \begin{tabular}{llr}
    \toprule
    \textbf{Perturbation Type} & \textbf{Iterations} & $\sigma_\MMD$ \\
    \midrule
    Add Edges & Iterations: 1 &           0.018 \\
    & Iterations: 5 &           0.007 \\
    & Iterations: 10 &           0.006 \\
    Gaussian Noise & Iterations: 1 &           0.016 \\
    & Iterations: 5 &           0.014 \\
    & Iterations: 10 &           0.013 \\
    Mutation & Iterations: 1 &           \textbf{0.147} \\
    & Iterations: 5 &           0.072 \\
    & Iterations: 10 &           0.061 \\
    Remove Edges & Iterations: 1 &           0.004 \\
    & Iterations: 5 &           0.003 \\
    & Iterations: 10 &           0.004 \\
    Rewire Edges & Iterations: 1 &           \textbf{0.296} \\
    & Iterations: 5 &           \textbf{0.290} \\
    & Iterations: 10 &           \textbf{0.287} \\
    Shear & Iterations: 1 &           0.079 \\
    & Iterations: 5 &           0.046 \\
    & Iterations: 10 &           0.043 \\
    Taper & Iterations: 1 &           0.037 \\
    & Iterations: 5 &           0.030 \\
    & Iterations: 10 &           0.029 \\
    Twist & Iterations: 1 &           \textbf{0.147} \\
    & Iterations: 5 &           0.072 \\
    & Iterations: 10 &           0.061 \\
    \bottomrule
  \end{tabular}
  }
  \caption{Standard deviations of the various Weisfeiler-Lehman configurations
under different perturbation regimes. Values higher than 0.1 are in bold.
Rewiring edges results in the highest standard deviations by far with
$\sigma_\MMD>0.2$. Lower iterations of the algorithm also results in higher
standard deviations.}
  \label{tab:std_graph_kernel}
\end{table}


\section{Protein-Specific Descriptors}\label{sec:results_protein_descriptors}

The behaviour of \acrshort{mmd} values derived from the protein-specific descriptor
functions described in section \ref{sec:descriptors} are shown in Figure
\ref{fig:protein_specific_descriptors}. The dihedral angles descriptor is
extremely sensitive to any kind of Gaussian noise and even decreases in value
when the kernel is not parametrized properly when increasing the shearing. This
is overall a good sign, as it will flag proteins that are unrealistic. The
interatomic distance histogram also behaves well. However, it shows high
variance when twisting the protein, presumably because the intra-sample variance
in distances is a threshold hard to overcome. This phenomenon can also be
noticed in Figure \ref{fig:wlk} with the Weisfeiler-Lehman kernel and could
disappear when working on morphologically similar proteins as highlighted in
Section \ref{sec:results_graph_kernels}. Another attractive aspect of this class
of descriptors is that they are very cheap to compute -- see further results in
Section \ref{sec:results_runtime}.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_4.pdf}
  \caption[\acrshort{mmd} vs. perturbations using the two novel protein descriptors.]{\acrshort{mmd}
vs. perturbations (in \%) using the two novel protein descriptors shown in Section
\ref{sec:descriptors}. Various kernel configurations are shown here. Those
descriptors behave very well overall, with the dihedral angles descriptors being
particularly sensitive to Gaussian noise. The distance histogram exhibits
 higher inter-run variance when subject to twisting
pertubations.}
  \label{fig:protein_specific_descriptors}
\end{figure}

\section{\acrshort{mmd} from Learned Embeddings}

Because the only constraint of the input vectors to the kernels accepting
fixed-length vectors as inputs is that they are in $\R^d$, we can input
fixed-length vector embeddings into them, leveraging the immense progress made
in the past decade regarding embeddings of structured data, specifically protein
sequences as highlighted in Section \ref{sec:mmd}. The only perturbation such a
learned embedding would be sensitive to is changes in the protein sequence. As
such, we investigated how the \acrshort{mmd} values increase with increasing point mutation
probabilities with various kernels, as seen in Figure \ref{fig:esm_descriptor}.
We find that the kernel used does not have any bearing on the \acrshort{mmd} behavior as
long as $\sigma<1$ for the RBF kernel. Overall, we see that even in these low
mutation regimes, the \acrshort{esm} embedding is sensitive enough to detect such changes.
More sequence-specific perturbations are required to assess the
applicability of the \acrshort{esm} embedding as a sequence descriptor, such as
translocation of subsequences, additions, deletions, etc. However, the early
results of such a descriptor shown in this thesis are promising.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_5.pdf}
  \caption[\acrshort{mmd} using \acrshort{esm} embeddings.]{\acrshort{mmd} vs. mutation probability using the \acrshort{esm}
learned embeddings with various kernels. The correlation coefficients of the
resulting \acrshort{mmd}s with RBF kernels with $\sigma<1$ and the linear kernel are high,
therefore making them good configurations for sequences. This analysis should be
complemented by complementary sequence-specific perturbations -- see Section
\ref{sec:discussion_realistic_proteins} for details.}
  \label{fig:esm_descriptor}
\end{figure}


\section{Topological Descriptors and Kernels}\label{sec:results_topo_kernels}

Figure \ref{fig:tda_kernels} shows the behavior of both the multi-scale kernel
and persistence Fisher kernel. Overall, we can see that they are among the best
performing metrics in this thesis, with high correlations and low inter-run
variation. While they cannot detect changes in node labels in the current
setting, hence not being able to detect mutations, we will show in Section
\ref{sec:tda_limitations} how this could be alleviated. For computational
reasons, we did not compute variations of different kernel parameters, although
one could speed up the operation by precomputing the kernel matrices, and only
later multiplying the matrix by different constants (see Appendix A.5 of
\cite{obray2022evaluation}).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_6.pdf}
  \caption[\acrshort{mmd} using topological kernels.]{\acrshort{mmd} vs. Perturbation (in \%) using the multi-scale
kernel and persistence Fisher kernel. For the persistence Fisher kernel (PFK), both
the kernel bandwidth parameter and the Fisher bandwidth parameter are set to 1.
For the multi-scale kernel (MSK), the bandwidth parameter is also set to 1. Overall,
the correlation coefficients for those kernels are very high and show little
inter-run variance.}
  \label{fig:tda_kernels}
\end{figure}


\section{Inter-Run Variance}

In Figures \ref{fig:mmd_consistent_eps} through \ref{fig:tda_kernels}, we mostly
commented on the correlation coefficients between the (normalized) \acrshort{mmd} values
and the amount perturbation applied. While a high correlation is certainly a
good indicator of a particular \acrshort{mmd} configuration, one should also carefully
consider the (normalized) inter-run variance observed between independent
samples of proteins. We can see that in Figures \ref{fig:mmd_consistent_eps}
through \ref{fig:tda_kernels}, this variance changes quite a lot depending on
the \acrshort{mmd} configuration. It can be high for certain representations.
For instance in Figure \ref{fig:mmd_sensitivity_eps}, one can see that a higher
$\varepsilon$ value (e.g. 32\si{\angstrom}) show a higher inter-run variance
compared to a lower $\varepsilon$ value (e.g. 8\si{\angstrom}), when
it comes to the twisting perturbation (lowest row). Furthermore, one of the
descriptors devised for this thesis also exhibits high variance under twisting
perturbations: the pairwise distance histogram, Figure
\ref{fig:protein_specific_descriptors}. The consequence of a high normalized
inter-run variance can be just as nefarious as \acrshort{mmd} configurations with low
correlation coefficients because the range of absolute \acrshort{mmd} values can be too wide
to make an accurate assessment of the quality of generated samples.


\section{Runtime}\label{sec:results_runtime}

One of the desiderata of \acrshort{mmd} is a low computational complexity (see Section
\ref{sec:evalproblem}). As such, we report the various computation times for
each of the important elements of the pipelines outlined in this thesis. The
summary of all execution times can be found in Table \ref{tab:runtimes}.

First, we see that one computation stands out: computing the Vietoris-Rips
filtration of a point cloud. Together with a large memory footprint, the
Vietoris-Rips filtration is quite lengthy to compute, due to a runtime of
$\mathcal{O}(n^{3(k+2)})$, where $k$ is the number of dimensions (here, $k=3$)
and $n$ the number of points \citep{adams2018persistent}. Although some optimizations have been carried out
by \texttt{Ripser}, the software used to compute the Vietoris-Rips filtration by
\cite{Bauer2021Ripser},
it remains very expensive to compute and does not enjoy the benefits of hardware
acceleration through e.g. a GPU due to the difficulty of parallelizing such a
filtration.

Second, computing the distance histogram and dihedral angles histogram is the
fastest descriptors to compute, making them suitable to evaluate large
collections of generated proteins. Other graph descriptors are also reasonably
fast to compute, but they depend on the settings used to extract the graphs, and
generally, the runtime increases in the number of edges of the graph.

Third, the kernels used in this study all have reasonable runtimes, but we note
the particular efficiency of the RBF and linear kernel compared to the
persistence Fisher, multi-scale and Weisfeiler-Lehman kernel. The
implications of these runtimes will be further explored when making
recommendations to the practioner in Section
\ref{sec:discussion_recommendations}.


\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \textbf{Operation} &  \textbf{Execution Time} \\
    \midrule
    \textbf{Descriptor Functions} & \\
    \midrule
    Vietoris-Rips Filtration & 3332 s \\
    \acrshort{esm} Embedding & 163 s\\
    Degree Distribution Histogram (32\si{\angstrom}-graph) & 35 s\\
    Clustering Coefficient Histogram (32\si{\angstrom}-graph) & 175 s\\
    Laplacian Spectrum Histogram (32\si{\angstrom}-graph) & 35 s\\
    Distance Histogram & 28 s\\
    Dihedral Angles Histogram & 4 s\\
    \midrule
    \textbf{Kernels} & \\
    \midrule
    Weisfeiler-Lehman Kernel (4 iterations) & 32 s \\
    Persistence Fisher Kernel & 35 s \\
    Multi-Scale Kernel & 11 s \\
    RBF Kernel  & 1.7 ms \\
    Linear Kernel  & 7.7 ms \\
    \bottomrule
  \end{tabular}
  \caption[Runtime and computational complexity of the various elements of the
  pipeline.]{Runtime and computational complexity of the various elements of the
pipeline. These timings are obtained by executing the operation on 100 samples
spread across 10 CPU cores from an Intel Xeon Gold 6254 CPU clocked at 3.10GHz.}
  \label{tab:runtimes}
\end{table}


\section{Summary}

In this chapter, we present the results of the behavior of \acrshort{mmd} under various
configurations and subject to various perturbations. We first note that the
overall behavior of \acrshort{mmd} under various perturbations is stable when sufficient
care is put into the selection of the of the kernel parameters (Figure \ref{fig:mmd_consistent_eps}). As a side note,
we see also that overall the correlations for descriptors of
$\varepsilon$-graphs is better than for $k$-NN graphs (Figure \ref{fig:mmd_k_nn_graphs}). Second, we note that the
choice of graph representation impacts the behavior of \acrshort{mmd}'s behavior: the
lower the radius of the sphere used to extract the $\varepsilon$-graph, the more
sensitive the resulting \acrshort{mmd} configuration, i.e. the lower the perturbation
required for the \acrshort{mmd} to reach a certain threshold (Figure
\ref{fig:mmd_sensitivity_eps}).

We then present the results for more exotic configurations of \acrshort{mmd} which we
hypothesized might be uniquely suited for protein representations. First, we
investigate the applicability of graph kernels, specifically the
Weisfeiler-Lehman kernel, to graphs extracted from proteins, which revealed that
high levels of perturbations are required to detect changes in \acrshort{mmd} (Figure
\ref{fig:wlk}). We then presented the results of the efficient and biologically
relevant protein-specific descriptors, which seem to behave very well (Figure
\ref{fig:protein_specific_descriptors}). Using learned embeddings as sequence
descriptors such as the \acrshort{esm} embedding showed that the resulting \acrshort{mmd} is sensitive
to low rates of point mutation (Figure \ref{fig:esm_descriptor}). Using
topological descriptors and appropriate kernels also seems to work very well, despite
a high computational cost (Figure \ref{fig:tda_kernels}). Finally, we show the
results of the runtime of each significant operation of the workflow highlighted
in this thesis (Table \ref{tab:runtimes}).
