% TODO comment on standard deviation
\chapter{Results}\label{chap:results}

In this chapter, we will introduce the results of the experiments subjecting
protein representations to various relevant perturbation regimes highlighted in
chapter \ref{chap:methods}. We first discuss the results and implication of
classical MMD configurations, namely by showing how it behaves on protein
graph descriptors. We then move on to show results of the sensitivity of the MMD
values based on the underlying graph representation used to extract the various
graphs. Finally, we explore more exotic configurations of MMD that we
hypothesize might be more suitable for proteins. We round up this chapter with a
short section on the runtimes of the various elements of the computational
pipelines shown in this chapter.

\section{Overall MMD behaviour}

Surprisingly, we found that the behaviour of MMD was not as inconsistent for the
types of graphs extracted from proteins as was found on synthetic graphs by
\cite{o2021evaluation}. Figure \ref{fig:mmd_consistent_eps} exhibits a typical
trajectories of MMD values as different perturbation types are progressively
added to $\varepsilon$-graphs with $\varepsilon$ set to $8$\si{\angstrom}. We
can see that both the Spearman and Pearson correlation coefficients averaged
across runs are high, indicating a high correlation between the amount of
perturbation and the normalized MMD values. There is, however, an exception: the
MMD obtained from the degree histogram does not behave as expected, and is very
sensitive to the addition of edges and decreases in value as more and more edges
are added. We expect that this pathology is not very common in real models and
that this can be easily detected manually.

The overall influence of the kernel on MMD's behaviour subject to perturbations
such as Gaussian Noise, can be seen in Figure \ref{fig:mmd_effect_kernel}. We
can see that for low values of $\sigma$ (the hyperparameter of the Gaussian
kernel, see Section \ref{sec:kernels}) and for the linear kernel, MMD values
behave as desired. However, unpredictable patterns emerge when increasing
$\sigma$ and the kernel matrices can ``saturate'' quite easily. This can have
quite unpredictable behaviours: in the case of the degree histogram or the
clustering histogram, this results in an overly sensitive kernel, while the
clustering histogram remains oblivious to large amounts of perturbation.

$k$-NN graphs seem to behave similarly to $\varepsilon$-graphs, in that they are
able to detect perturbations - see Figure \ref{fig:mmd_k_nn_graphs} for an
example with 2-NN graphs. However, they are extremely sensitive to Gaussian
noise and to the addition of edges (see upper left and upper mid pane of Figure
\ref{fig:mmd_k_nn_graphs}). Presumably, this is because very small amounts of
noise ($<4$\si{\angstrom}) affect the direct local neighbourhood of each node
very significantly in such a way that the resulting $k$-NN graph is rapidly
disrupted, which is not necessarily the case with other perturbations. We note
that overall the $\varepsilon$-graphs behave more consistently than $k$-NN
graphs, with 13 out of 21 Pearson's correlation coefficients being higher in
$\varepsilon$-graphs compared to $k$-NN graphs and 11 out of 21 Spearman's
correlation coefficients being higher in 8\si{\angstrom}-graphs compared to 2-NN
graphs. Even when the $\varepsilon$-graphs did not compare favourably, it was by
a considerable lower margin than $k$-NN graphs in a similar setting. This effect
seems to be mitigated by increasing $k$, but this comes at the cost of a lower
Spearman correlation coefficient, which is even less desirable - see Figure
\ref{fig:k_vs_turbulence_gaussian_noise}. Changing kernel parameters did not
alleviate the pathologies (see Supplementary Figure
\ref{fig:mmd_effect_kernel_knn}) and had the same overall effect as observed in
Figure \ref{fig:mmd_effect_kernel}. We therefore proceed with further analysis
using $\varepsilon$-graphs.


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_1_1.pdf}
  \caption{MMD vs. perturbation (in \%) for various graph descriptors of the 8\si{\angstrom}-graphs under different perturbations regimes. The kernel
    used to obtain these graphs is the RBF Kernel with bandwidth 0.01.
    $\rho_{S}$: average Spearman correlation coefficient across runs.
    $\rho_{P}$: average Pearson correlation coefficient across runs.}
  \label{fig:mmd_consistent_eps}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_1_2.pdf}
  \caption{MMD vs. Gaussian noise perturbation (in \%) for various graph descriptors of the
    8\si{\angstrom}-graphs. The kernel here is shown on top of each subplots.}
  \label{fig:mmd_effect_kernel}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_1_3.pdf}
  \caption{MMD vs. Gaussian noise perturbation (in \%) for various graph descriptors of the
    2-nearest-neighbour graphs.}
  \label{fig:mmd_k_nn_graphs}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_1_4.pdf}
  \caption{MMD vs. Gaussian noise Perturbation (in \%) for various graph
    descriptors and values of $k$ used to extract the $k$-NN graph.}
  \label{fig:k_vs_turbulence_gaussian_noise}

\end{figure}

% MMD is quite stable, somewhat contradicts what obray is saying. Maybe due to
% the different nature of the graphs.

\section{Influence of the graph representation on the sensitivity to perturbations}\label{sec:results_sensitivity}

In the last section, we discussed the notion of sensitivity of a particular MMD
configuration to perturbations. We explore this idea further here. Specifically,
we investigate the impact of the $\varepsilon$ threshold on the resulting
sensitivity of the MMD configuration to various perturbations affecting the
underlying point cloud. Figure \ref{fig:mmd_sensitivity_eps} shows that, in
general, the lower the $\varepsilon$, the more sensitive the MMD to
perturbations. A low sensitivity to perturbation might be useful in the early
stages of modelling, when generated samples only need to \emph{coarsely} match
the reference samples. Conversely, a higher sensitivity to perturbations might
be useful when trying to \emph{finely} distinguish a set of anomalous samples
from the reference samples. Additionally, we can see that under the Gaussian
noise, twisting and tapering perturbation, larger values of $\varepsilon$
introduce larger variations in MMD values.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_2_1.pdf}
  \caption[MMD vs. point cloud perturbation for various descriptors.]{MMD vs. point cloud perturbation for various descriptors. In general,
when graphs are extracted with a lower $\varepsilon$ value, the MMD curve
increases more rapidly. The only exception to this trend is the laplacian
spectrum histogram descriptor under the tapering perturbation. In multiple
instances, we see that increasing the $\varepsilon$ value increases the MMD variance}
  \label{fig:mmd_sensitivity_eps}
\end{figure}

\section{Graph Kernels}\label{sec:results_graph_kernels}
% TODO compare WL with ESM (no modelling bias for WL on paper, might be a
% weakness since some mutations might not have a functional effect.)

The results of the behaviour of the Weisfeiler-Lehman kernel (see Sections
\ref{sec:kernels} and \ref{sec:methods_kernels}) can be seen in Figure
\ref{fig:wlk}. Contrary to the tendencies observed in Section
\ref{sec:results_sensitivity}, MMDs obtained using the Weisfeiler-Lehmann
kernels on 8\si{\angstrom} graphs are very insensitive to medium levels of perturbations,
and even completely oblivious to rewiring of graphs and generally insensitive to
graph perturbations unless highly perturbed (see upper row of Figure \ref{fig:wlk}). The most likely explanation
is that the diversity of Weisfeiler-Lehman hashes (i.e. neighbourhoods) observed within perturbed or
unperturbed samples is relatively high, which makes distinguishing between the
two a high threshold to overcome. This drawback could dissapear if one is working on
a more targeted set of morphologically similar proteins with a lower diversity
of Weisfeiler-Lehman hashes, but this is beyond the scope of the thesis.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_3.pdf}
  \caption[MMD vs. perturbations using the Weisfeiler-Lehman kernel.]{MMD vs.
    perturbations using the Weisfeiler-Lehman kernel using the
    8\si{\angstrom}-graphs as inputs.}
  \label{fig:wlk}
\end{figure}

\section{Protein-specific descriptors}\label{sec:results_protein_descriptors}

The behaviour of MMD values derived from the protein-specific descriptor
functions described in section \ref{sec:descriptors} are shown in Figure
\ref{fig:protein_specific_descriptors}. The dihedral angles descriptor is
extremely sensitive to any kind of Gaussian noise, and even decreases in value
when the kernel is not parametrized properly when increasing the shearing. This
is overall a good sign, as it will flag proteins that are unrealistic. The
interatomic distance histogram also behaves well. However, it shows high
variance when twisting the protein, presumably because the intra-sample variance
in distances is a threshold hard to overcome. This phenomenon can also be
noticed in Figure \ref{fig:wlk} with the Weisfeiler-Lehman kernel, and could
dissapear when working on morphologically similar proteins as highlighted in
Section \ref{sec:results_graph_kernels}. Another attractive aspect of this class
of descriptors is that they are very cheap to compute -- see further results in
Section \ref{sec:results_runtime}.

\begin{figure}
  \includegraphics[width=\textwidth]{./figures/results/res_4.pdf}
  \caption[MMD vs. perturbations using the two novel protein descriptors.]{MMD vs. perturbations using the two novel protein descriptors shown
    in Section \ref{sec:descriptors}. Various kernel configurations are shown
    here.}
  \label{fig:protein_specific_descriptors}
\end{figure}

\section{MMD From Learned Embeddings}

Because the only constraint of the input vectors to the kernels accepting
fixed-length vectors as inputs is that they are in $\R^d$, we can input
fixed-length vector embeddings into them, leveraging the immense progress made
in the past decade regarding embeddings of structured data, specifically protein
sequences as highlighted in Section \ref{sec:mmd}. The only perturbation such a
learned embedding would be sensitive to is changes in the protein sequence. As
such, we investigated how the MMD values increase with increasing point mutation
probabilities with various kernels, as seen in Figure \ref{fig:esm_descriptor}.
We find that the kernel used does not have any bearing on the MMD behaviour as
long as $\sigma<1$ for the RBF kernel. Overall, we see that even in these low
mutation regimes, the ESM embedding is sensitive enough to detect such changes.
More sequence-specific perturbations are required to assess the
applicability of the ESM embedding as a sequence descriptor, such as
translocation of subsequences, additions and deletions, etc. However, the early
results of such a descriptor shown in this thesis are promising

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_5.pdf}
  \caption[MMD vs. mutation probability using the ESM learned embeddings with
  various kernels ]{MMD vs. mutation probability using the ESM learned embeddings with various kernels.}
  \label{fig:esm_descriptor}
\end{figure}


\section{Topological Descriptors And Kernels}\label{sec:results_topo_kernels}

Figure \ref{fig:tda_kernels} shows the behaviour of both the multi-scale kernel
and persistence Fisher kernel. Overall, we can see that they are among the best
performing metrics in this thesis, with high correlations and low inter-run
variation. While they cannot detect changes in node labels in the current
setting, hence not being able to detect mutations, we will show in Section
\ref{sec:tda_limitations} how this could be alleviated. For computational
reasons, we did not compute variations of different kernel parameters, although
one could speed up the operation by precomputing the kernel matrices, and only
later multiplying the matrix by different constants (see Appendix A.5 of
\citep{o2021evaluation}).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figures/results/res_6.pdf}
  \caption[MMD vs. Perturbation (\%) using the multi-scale kernel and
  persistence Fisher kernel.]{MMD vs. Perturbation (\%) using the multi-scale
kernel and persistence Fisher kernel. For the persistence Fisher kernel, both
the kernel bandwidth parameter and the Fisher bandwidth parameter are set to 1.
For the multi-scale kernel, the bandwidth parameter is also set to 1.}
  \label{fig:tda_kernels}
\end{figure}


\section{Runtime}\label{sec:results_runtime}

One of the desiderata of MMD is a low computational complexity (see Section
\ref{sec:evalproblem}). As such, we report the various computation times for
each of the important elements of the pipelines outlined in this thesis. The
summary of all execution times can be found in Table \ref{tab:runtimes}.

First, we see that one computation stands out: computing the Vietoris-Rips
filtration of a point cloud. Together with a large memory footprint, the
Vietoris-Rips filtration is quite lengthy to compute, due to a runtime of
$\mathcal{O}(n^{3(k+2)})$, where $k$ is the number of dimensions (here, $k=3$)
and $n$ the number of points \citep{adams2018persistent}. Although some optimizations have been carried out
by \texttt{Ripser}, the software used to compute the Vietoris-Rips filtration,
it remains very expensive to compute and does not enjoy the benefits of hardware
acceleration through e.g. a GPU due to the difficulty of parallelizing such a
filtration \cite{Bauer2021Ripser}.

Second, computing the distance histogram and dihedral angles histogram are the
fastest descriptors to compute, making them suitable to evaluate large
collections of generated proteins. Other graph descriptors are also reasonably
fast to compute, but they depend on the settings used to extract the graphs, and
generally the runtime increases in the number of edges of the graph.

Third, the kernels used in this study all have reasonable runtimes, but we note
the particular efficiency of the RBF and linear kernel compared to the
persistence Fisher kernel, multi-scale and Weisfeiler-Lehmann kernel. The
implications of these runtimes will be further explored when making
recommendations to the practioner in Section
\ref{sec:discussion_recommendations}.


\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \textbf{Operation} &  \textbf{Execution Time} \\
    \midrule
    \textbf{Descriptor Functions} & \\
    \midrule
    Vietoris-Rips Filtration & 3332 s \\
    ESM Embedding & 163 s\\
    Degree Distribution Histogram (32\si{\angstrom}-graph) & 35 s\\
    Clustering Coefficient Histogram (32\si{\angstrom}-graph) & 175 s\\
    Laplacian Spectrum Histogram (32\si{\angstrom}-graph) & 35 s\\
    Distance Histogram & 28 s\\
    Dihedral Angles Histogram & 4 s\\
    \midrule
    \textbf{Kernels} & \\
    \midrule
    Weisfeiler-Lehman Kernel (4 iterations) & 32 s \\
    Persistence Fisher Kernel & 35 s \\
    Multi-Scale Kernel & 11 s \\
    RBF Kernel  & 1.7 ms \\
    Linear Kernel  & 7.7 ms \\
    \bottomrule
  \end{tabular}
  \caption{Runtime and computational complexity of the various elements of the
pipeline. These timings are obtained by executing the operation on 100 samples
spread across 10 CPU cores from an Intel Xeon Gold 6254 CPU clocked at 3.10GHz.}
  \label{tab:runtimes}
\end{table}



\section{Summary}

In this chapter, we present the results of the behaviour of MMD under various
configurations and subject to various perturbations. We first note that the
overall behaviour of MMD under various perturbations is stable when sufficient
care is put into the selection of the of the kernel parameters (Figure \ref{fig:mmd_consistent_eps}). As a side note,
we see also that overall the correlations for descriptors of
$\varepsilon$-graphs is better than for $k$-NN graphs (Figure \ref{fig:mmd_k_nn_graphs}). Second, we note that the
choice of graph representation impacts the behaviour of MMD's behaviour: the
lower the radius of the sphere used to extract the $\varepsilon$-graph, the more
sensitive the resulting MMD configuration, i.e. the lower the perturbation
required for the MMD to reach a certain threshold (Figure
\ref{fig:mmd_sensitivity_eps}).

We then present the results for more exotic configurations of MMD which we
hypothesized might be uniquely suited for protein representations. First, we
investigate the applicability of graph kernels, specifically the
Weisfeiler-Lehman kernel, to graphs extracted from proteins, which revealed that
high levels of perturbations are required to detect changes in MMD (Figure
\ref{fig:wlk}). We then presented the results of the efficient and biologically
relevant protein-specific descriptors, which seem to behave very well (Figure
\ref{fig:protein_specific_descriptors}). Using learned embeddings as sequence descriptors
such as the ESM embedding showed that the resulting MMD is sensitive to
low rates of point mutation (Figure \ref{fig:esm_descriptor}). Using
topological descriptors and appropriate kernels also seem to work well, despite
a high computational cost (Figure). Finally, we expand on runtime analysis and show the results of the runtime
of each significant operation (Table \ref{tab:runtimes}).
% TODO add topo summary
