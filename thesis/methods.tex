\chapter{Methods}\label{chap:methods}

The primary methodology employed in this thesis to assess the quality of metrics
used to evaluate generative protein models follows the fundamentally similar
approach used by \cite{o2021evaluation}, which we adapt to include the following
steps:

\begin{enumerate}
\item Take two i.i.d. samples of a database of proteins.
\item Progressively add some perturbation to one of the samples.
\item Measure the \acrshort{mmd} between the unperturbed and perturbed sample.
\end{enumerate}

We start by motivating the datasets that will be used to simulate a generative
protein model. We then describe and motivate the experimental setups -- i.e.
perturbations -- that we will employ to test the various configurations of \acrshort{mmd}.
Finally, we enumerate and justify which configurations of \acrshort{mmd} are tested,
including all the combinations of protein representations, descriptor functions,
and kernels. Finally, we describe and motivate the experimental setups -- i.e.
perturbations -- that we will employ to test the various metric configurations.


\section{Datasets}

Because generative models have not been applied to proteins yet, which is partly
due to the lack of suitable evaluation metrics, no generative model can be used
here. In this thesis, except otherwise stated, all results will be derived from
10 random samples from the \textit{homo sapiens} monomeric proteome downloaded
from the EBI AlphaFold2 database
\citep{varadi2022alphafold,tunyasuvunakool2021highly}, a repository comprising
predicted 3D structures of protein sequences obtained from AlphaFold2, the
current state-of-the-art method to predict protein structure from sequences
\citep{jumper2021highly}. This dataset was chosen because it contains
consistently formatted \texttt{pdb} files only containing information from
heavy atom directly contributing to the 3D structure of a single monomer, which
simplifies downstream processing.

\section{Perturbations}

While \cite{o2021evaluation} focused on \emph{graph perturbations} specifically,
we wanted to augment and refine the set of perturbations applied to the perturbed
sample of proteins to be more pertinent to proteins. Three
categories of perturbations can be distinguished:

\paragraph{Graph Perturbations} These perturbations mostly overlap with those
defined by \cite{o2021evaluation}, as they include (i) adding edges to a graph
(ii) removing edges from a graph (iii) rewiring, i.e. swapping, edges within a
graph.
\paragraph{Point cloud perturbations} These perturbations aim to add changes
to the underlying coordinates of each atom in the protein. Such
perturbations include injecting Gaussian noise (Equation
\ref{eq:gaussian_noise}), (ii) twisting, , (iii) shearing, and
(iv) tapering. We proceed to detail each of the equations governing the
perturbations below. An illustration of each of those perturbations can be found
in Figure \ref{fig:perturbation_illustration}

In the notations that follow, $x$, $y$ $z$ represent the unperturbed coordinates, and
$x'$, $y'$, and $z'$ represent the perturbed coordinates. The Gaussian noise added to a coordinate system is given by:
\begin{equation}
  \label{eq:gaussian_noise}
  \begin{bmatrix}
    x' \\
    y' \\
    z'
  \end{bmatrix} =
  \begin{bmatrix}
    x + \sigma \\
    y+ \sigma \\
    z+ \sigma
  \end{bmatrix}
\end{equation}
where $\sigma\sim\caln(0,\sigma)$ is set by the user. Twisting is achieved by
adding the following transformation to the coordinate system:

\begin{equation}
  \label{eq:twisting}
  \begin{bmatrix}
    x' \\
    y' \\
    z'
  \end{bmatrix} =
  \begin{bmatrix}
    x\cdot\cos(\alpha\cdot z) - y\cdot\sin(\alpha\cdot z) \\
    x\cdot\sin(\alpha\cdot z) + y\cdot\sin(\alpha\cdot z) \\
    z
  \end{bmatrix}
\end{equation}

where $\alpha\in\R$ is in $\rad\cdot\si{
\angstrom}^{-1}$ is set by the user. Shearing the coordinate system is achieved
by applying

\begin{equation}
  \label{eq:shearing}
  \begin{bmatrix}
    x' \\
    y' \\
    z'
  \end{bmatrix} =
  \begin{bmatrix}
    a \cdot z + x\\
    b \cdot z + y\\
    z
  \end{bmatrix}
\end{equation}

to the coordinate system, where $a, b\in\R$ are in \si{\angstrom} and set by the user. In this thesis, we set $a=b$. Similarly,
tapering is achieved by applying


\begin{equation}
  \label{eq:tapering}
  \begin{bmatrix}
    x' \\
    y' \\
    z'
  \end{bmatrix} =
  \begin{bmatrix}
    (0.5\cdot a^2\cdot z + b\cdot z + 1) \cdot x\\
    (0.5\cdot a^2\cdot z + b\cdot z + 1) \cdot y\\
    z
  \end{bmatrix}
\end{equation}

where $a, b\in\R$ are in \si{\angstrom} and set by the user. Similarly to
Equation \ref{eq:shearing}, we set $a=b$ in this thesis.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figures/protein_unperturbed.pdf}
  \includegraphics[width=\textwidth]{./figures/protein_perturbed.pdf}
  \caption[Illustration of the perturbations applied to proteins.]{Illustration
of the perturbations applied to proteins. The protein displayed here is the
Cilia- and flagella-associated protein 53 (PDB entry: Q96M91). Each $\alpha$
carbon is colored according to its position on the chain. On the unperturbed
protein, the lighter colors are located closer to the viewer.}
  \label{fig:perturbation_illustration}
\end{figure}

\paragraph{Mutation} These simply consist in swapping node labels, i.e.
changing an existing node label, with another. While graph perturbation
probabilities (e.g. of adding an edge) range from 0 to 1, here we mostly
concentrate on lower regimes of mutation, i.e. between 0 and 0.1, so see how
sensitive the \acrshort{mmd} to a few point mutations, which covers some of the
real-world protein engineering use cases \citep{poluri2016protein}.

For each perturbation, a range of degrees of perturbation is defined and 20
different evenly-spaced degrees of perturbation are examined and repeated 10
times with 10 pairs of i.i.d. proteins to estimate the sensitivity of the
particular \acrshort{mmd} configuration to the perturbation. The ranges used for each
parameter used in this thesis is shown in Table \ref{tab:perturbation_ranges}


\begin{table}
  \centering
  \begin{tabular}{>{\raggedright\arraybackslash}p{4.8cm}l}
    \toprule
    \textbf{Perturbation} & \textbf{Range} \\
    \midrule
    Twist & [0, 0.1] (rad/\si{\angstrom})\\
    Shear & [0, 5] (\si{\angstrom}) \\
    Taper & [0, 0.1] (\si{\angstrom}, \si{\angstrom}) \\
    Gaussian Noise & [0, 30] (\si{\angstrom}) \\
    Mutation & [0, 0.1]\\
    Graph Perturbations (remove, add, rewire edges) & [0, 1]\\
    \bottomrule
  \end{tabular}
  \caption{Perturbation ranges used in this thesis. Each interval was split into
    20 evenly-spaced degrees of perturbation.}
  \label{tab:perturbation_ranges}
\end{table}

\section{MMD Configurations}

We introduced \acrshort{mmd} in Section \ref{sec:mmd}, Equation \ref{eq:mmd}, and
noted that an important part of \acrshort{mmd} consists in choosing the right set of
descriptor function and kernel. Here, we list and motivate all graph extraction techniques,
descriptor functions employed, and kernels adopted in this experiment.

First, throughout our experiments, we will use the unbiased squared \acrshort{mmd} estimate,
which removes self-comparison terms in the kernel matrices
(\cite{gretton2012kernel}, Lemma 6), which can result in negative values. We
will normalize the resulting \acrshort{mmd} over the whole range of perturbation for each
curve as well to compare behaviors of different \acrshort{mmd} configurations not
operating on the same scale.

\begin{equation}
  \label{eq:normalization}
  \MMD_{\textnormal{Normalized}} = {\MMD - \min(\MMD_{\textnormal{Experiment}}) \over \max(\MMD_{\textnormal{Experiment}}) - \min(\MMD_{\textnormal{Experiment}})}
\end{equation}

Where $\MMD_{\textnormal{Experiment}}$ is the collection of \acrshort{mmd} values for a particular
experiment, i.e. a particular \acrshort{mmd} configuration tracked through the whole range
of a particular perturbation.

\subsection{Representations}
We will use several different representations in this thesis, which can be
grouped into three categories. The first is \emph{coordinates}. These are parsed
from each \texttt{.pdb} file. The second is \emph{graphs}. They include $k$-NN
and $\varepsilon$-graphs, introduced in Section \ref{sec:graphs}. A summary
table of the different types of graphs extracted from proteins in this thesis
can be found in Table \ref{tab:graph_extraction}. The third is the simple
protein \emph{sequence}. Each protein's sequence parsed from each \texttt{.pdb}
file as-is. Since we are only dealing with monomers, no additional processing is
required.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Graph type} & \textbf{Values} \\
    \midrule
    $k$-NN graphs & 2, 6, 8 \\
    $\varepsilon$-graphs & 8, 16, 32 \\
    \bottomrule
  \end{tabular}
  \caption{Ranges of parameters used to extract graphs from point clouds in this thesis.}
  \label{tab:graph_extraction}
\end{table}


\subsection{Descriptor functions}\label{sec:descriptors}

As discussed in Section \ref{sec:mmd}, some kernels require some alternative vectorized
representation to work. We will use the following protein descriptor functions
here.


\paragraph{Graph descriptors} They are defined in Section \ref{sec:mmd} and include
  the degree distribution histogram, the clustering coefficient histogram and
  the Laplacian spectrum histogram. These are all fixed-length vectors. Of note,
  we let the degree histogram in a range of 2699, due to the fact that adding
  edges between all nodes of the longest protein containing 2699 residues will
  result in the highest-degree node to be 2698.
\paragraph{Coordinates descriptors} In order to capture the information of the
3D structure of the protein beyond local neighborhood information,
a topological descriptor of each protein in the form of a persistence diagram is
extracted using a Vietoris-Rips filtration introduced in detail in Section
\ref{sec:tda}. To speed up computation, we sampled every other point to
dramatically reduce the running time and memory footprint without significantly
affecting the shape of the protein.
\paragraph{Protein-specific descriptors} In this thesis we introduce two novel
protein-specific descriptor functions resulting in fixed-length vector
representations. The first consists of a histogram of the pairwise distance of
each atom considered; the second consists in concatenating the two histograms of
the two dihedral angles $\phi$ and $\psi$ formed by each amino acid discussed in
Section \ref{sec:proteins}. The inspiration behind those two descriptors comes
from elements of the validation pipeline of novel Protein Data Bank structures
\citep{read2011new, gore2012implementing, gore2017validation}, where atoms too
close together are flagged and unusual dihedral angles are reported. Those
unusual dihedral angles are also called ``Ramachandran outliers'', after the
scientist who discovered a way to display the $\phi$ and $\psi$ in a 2-D
histogram, and recovered items related to the secondary structure of the protein
from such plots. \citep{ramachandran1063Stereochemistry}.

\paragraph{Sequence descriptors} In Section \ref{sec:mmd}, we discussed the
Evolutionary Scale Modelling family to construct descriptors of a fixed length
using a learned embedding. We use the 6-layer variant trained on
the UniRef50 snapshot from March 2018 which contains approximately 43 million
parameters, because it is able to process the full range of sequence lengths
that we have in our dataset (the longest sequence fragment is 2699 amino acids long).

Table \ref{tab:descriptor_function_setup} summarizes the parameters used to set
up the descriptor functions in this thesis.

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \textbf{Descriptor Name} & \textbf{Number of Bins} & \textbf{Range of Bins} \\
    \midrule
    Degree Histogram & 2699 & [0, 2699] \\
    Laplacian Spectrum Histogram & 100 & [0, 2] \\
    Distance Histogram & 1000 & [0, 1000] (\si{\angstrom}) \\
    Dihedral Angles Histogram & 100 & [$-\pi$, $+\pi$] (rad) \\
    \bottomrule
  \end{tabular}
  \caption[Descriptor function bin numbers and ranges of descriptor functions
  used in this thesis.]{Descriptor function bin numbers and ranges of descriptor functions
    used in this thesis. The ranges are given by
    [minimum value, maximum value] and (unit) when applicable.}
  \label{tab:descriptor_function_setup}
\end{table}

% TODO: add pgfplots figure of this workflow.

\subsection{Kernels}\label{sec:methods_kernels}
Once a suitable representation and descriptor function is selected, one requires
a kernel to evaluate the \acrshort{mmd} in the corresponding \acrshort{rkhs}. We
detail which kernels we are going to evaluate and why in this section. Kernels
used in this thesis can be grouped into three separate categories.

The first category has been discussed in the literature extensively since it was
used to evaluate generative graph neural network models, i.e. the
\emph{fixed-length vector kernels} like the Gaussian (RBF) kernel and the linear
kernel, both of which are defined in Section \ref{sec:kernels}\footnote{We will
  follow the speed-up trick outlined in Appendix A.5 by \cite{o2021evaluation}
  to reduce the computation time of each different bandwidth parameter}. Since the only
condition for each vector to be valid inputs for those kernels is that they are
in $\R^d$, the protein-specific vector representations outlined in Section
\ref{sec:descriptors} are valid inputs.
% Special implementation of wl-k

As alluded to in Section \ref{sec:kernels}, we introduce two new classes of
kernels for use in \acrshort{mmd} which so far have not been used to evaluate generative
models due to the unique aspects of proteins that need to be captured. This
brings us to the second category of kernels used in this thesis: \emph{graph kernels}.
Specifically, we are going to use the Weisfeiler-Lehman kernel discussed in
Section \ref{sec:kernels} because it captures \emph{local} patterns in the
neighbourhood of each node. In Appendix \ref{sec:sparse_wl}, we detail how we
achieved an 80\% improvement in the runtime of the Weisfeiler-Lehman kernel by
leveraging the sparsity of the graphs used in this thesis.

To estimate \emph{global} changes in the shape of the protein, we will also use
kernels using persistence diagrams as input, specifically using the Persistence
Fisher kernel defined in Section \ref{sec:kernels} \citep{le2018persistence}.


\section{Experimental Setup}

\subsection{Measuring the quality of \acrshort{mmd} configurations}

To objectively evaluate the various representation, descriptor, and kernel
combinations used in \acrshort{mmd}, we will use two correlation coefficients, namely the
Spearman's correlation coefficient and Pearson's correlation coefficient, each
given by:
\begin{equation}
  \label{eq:spearman} \rho_p(X,Y) = {\cov(\rank(X), \rank(Y)) \over
\sigma_{\rank(X)}\sigma_{\rank(Y)}},
\end{equation} and
\begin{equation}
  \label{eq:pearson} \rho_s(X,Y) = {\cov(X, Y)\over \sigma_X\sigma_Y},
\end{equation}
respectively. Here, $X$ is the vector containing the ordered set of values used
to perturb one of the protein sets and $Y$ the vector of ordered \acrshort{mmd} values
between the unperturbed and perturbed set for each perturbation level. In this
setting, a high Spearman correlation coefficient (Equation \ref{eq:spearman}) is
crucial to satisfy the first criterium of a performance metric: expressivity
(see Section \ref{sec:evalproblem}). This will guarantee that the metric
increases monotonically with the amount of perturbation. As
\cite{o2021evaluation} have shown, some configurations of \acrshort{mmd} on synthetic
datasets revealed that an increasing \acrshort{mmd} value with increasing perturbation is not
guaranteed. While \cite{thompson2022evaluation} highlighted that linearity is
not a requirement, the Pearson correlation coefficient (Equation
\ref{eq:pearson}) will allow us to further refine the selection of the metrics
behaving most predictably, and distill the most relevant configurations for a
given perturbation range.

\subsection{Software Library Design}


Due to the complexity and modularity of the methodology outlined above, it is
required to have a scalable library to execute all the evaluation experiments at
scale and efficiently. To accomplish this task, we developed a custom Python
library leveraging parallel processing of data to the greatest extent possible.
We were inspired by the standards of \texttt{scikit-learn} and implemented
multiple modules following the same design patterns to ensure that we could
build \texttt{Pipeline} objects with the necessary steps required to compute an
\acrshort{mmd}.

% Plot of the library structure.

\section{Summary}

In this chapter, we detailed the methodological setup employed in this thesis.
We first described the datasets we used, as well as the type of perturbations
applied to them. We then discussed the representations of the proteins used in
this study, as well as described the descriptor functions used for those
representations. Crucially, we motivated our choice for the collection of
kernels used for estimating the \acrshort{mmd}. Finally, we consider the different ways we
will use to describe our experimental setup to choose the best \acrshort{mmd} values.
