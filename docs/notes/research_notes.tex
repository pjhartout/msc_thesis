% Created 2022-04-06 Wed 10:23
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames,figures]{xcolor}
\usepackage[autostyle]{csquotes}
\usepackage[final]{pdfpages}
\usepackage{amsfonts, amssymb}            % Math symbols
\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{parskip}
\usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
\nocite{*}
\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
\newtheorem{definition}{Definition}[section]
\author{Philip Hartout}
\date{Monday January 24, 2022}
\title{Research notes on metrics for GNNs applied to biological problems}
\hypersetup{
 pdfauthor={Philip Hartout},
 pdftitle={Research notes on metrics for GNNs applied to biological problems},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.4.4)},
 pdflang={English}}
\begin{document}

\maketitle
\section{Fundamental concepts}
\label{sec:orgecf6092}
\subsection{Graph Laplacian}
\label{sec:org3a5fe64}
Given Adjacency matrix \(A\) of dimension \(n\times n\) and degree matrix \(D\) of
a given graph \(G\), the graph Laplacian \(L\) of \(G\) is given by:
\(L=D-A\).
\(p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iL^i\). \(p_w(L)\) is \(n\times n\).
\subsection{ChebNet}
\label{sec:org572c4a8}
\(p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iT_iL^i\)
where \(T_i\) is the degree-i Chebyshev polynomial of the first kind and
\(\widetilde{L}\) is the normalized Laplacian derived using the largest
eigenvalue of \(L\).

\(L\) is p.s.d., all eigenvalues of \(L\) are \(>0\). If \(\lambda_{\max}(L)>1\),
then \(L\)'s entries increase. \(\widetilde{L}\) normalizes the eigenvalues of \(L\)
and bounds them in \([-1,1]\). \(\widetilde{L}\) is defined as
\begin{equation*}
\widetilde{L} = {2L\over\lambda_{\max}(L) - I_n}
\end{equation*}
Embedding computation
\begin{enumerate}
\item \(h^{0}=x\)
\item For \(k=1,2,\dots,K\):
\begin{enumerate}
\item \(p^{(k)}=p_{w^{(k)}}(L)\)
\item \(g^{(k)}=p^{(k)}\times h^{k-1}\)
\item \(h^{(k)} = \sigma(g^{(k)})\)
\end{enumerate}
\end{enumerate}
where \(\sigma\) is some non-linearity.

\subsection{Modern GNNs}
\label{sec:org0dc6c6d}
When going back to \(p_w(L)=L\), focussing on one verted, we have:
   \begin{align*}
(Lx)_v &= L_v x\\
&= \sum_{u\in G}L_{vu}x_u\\
&= \sum_{u\in G}(D_{vu}-A_{vu})x_u\\
&= D_vx_v-\sum_{u\in\mathcal{N}(v)}x_u
   \end{align*}
This is a 1-hop localized convolution. This does two steps:
\begin{itemize}
\item Aggregating over immediate neighbour features \(x_u\).
\item Combining with the node's own feature \(x_v\)
\end{itemize}
Ensuring that the agg. is node order equivariant, the overall convolution
becomes node-order equivariant. This can be considered as messsage passing
between adjacent nodes. After each step, some nodes receive information from
their neighbour.

By iteratively repeating the 1-hop localized convolutions \(K\) times (i.e.
repeatedly passing messages), the receptive field of the conv. effectively
includes all nodes up to \(K\) hops away.

Some modern aggregation and combination functions:
\begin{itemize}
\item graph convolutional networks (GCN)
\item graph attention networks (GAT)
\item graph sample and aggregate (GraphSAGE)
\item graph isomorphism network (GIN)
\end{itemize}
\subsection{Global convolutions}
\label{sec:orgbfe1e8d}
\subsubsection{Spectral convolutions}
\label{sec:org22cbc6f}
Smoother graphs have a quantity \(R_(x)=\sum_{(i,j)\in E}(x_i-x_j)^2\) that
reflects that feature vectors \(x\) that assign similar values to adjacent
nodes in \(G\) would have smaller values of \(R_L(x)\). \(L\) is a real, symmetric
matrix which has eigenvalues \(\lambda_1\leq\dots\leq\lambda_n\). Eigenvectors
can be taken to be orthonormal.

The set of eigenvalues of \(L\) are successively less smooth. They are called the spectrum of \(L\).

The spectral decomp. of \(L\) as \(L=U\Lambda U^T\) where \(\Lambda\) is the
diagonal matrix of sorted eigenvalues, and \(U\) denotes the matrix of the
eigenvectors. sorted by increasing eigenvalues. Orthonormality between
eigenvectors gives us \(U^T U = I\). Since each \(u\in\mathbb{R}^n\), any \(x\)
can be represented as a linear combination of these eigenvectors, i.e.:
\begin{equation*}
x = \sum_{i=1}^{n}\widetilde{x_i}u_i=U\widetilde{x}.
\end{equation*}
where \(\widetilde{x}\) are the coefficients. Again, the orthonormality of the eigenvalues allows us to state \(x=U\widetilde{x} \iff U^Tx=\widetilde{x}\).
We can then compute global convolutions, defining:
    \begin{equation*}
h^{(k)} =
\begin{bmatrix}
h^{(k)}_1\\
\vdots\\
h^{(k)}_n\\
\end{bmatrix}
    \end{equation*}
We start with the original features \(h^{(0)}=x\), then
\begin{enumerate}
\item \(\hat{h}^{(k-1)} = U_m^Th^{(k-1)}\)
\item \(\hat{g}^{(k)}=\hat{w}^{(k)}\odot\hat{h}^{(k-1)}\)
\item \(h^{(k)}=\sigma(g^{(k)})\)
\end{enumerate}

\section{Graph Neural Networks}
\label{sec:org00b1878}
\subsection{Reviews}
\label{sec:org5c3ab65}
\subsubsection{Graph neural networks:}
\label{sec:org755f823}
    A review of methods and applications
Zhou et al mention several GNN approaches in use today
Generative models popular today:
\paragraph{Sequential graph generation process}
\label{sec:org1568572}
\begin{itemize}
\item GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
\item Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
\item GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
\end{itemize}
\paragraph{Non-sequential graph generation process}
\label{sec:orgab8d0ea}
\begin{itemize}
\item MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
\item Ma et al 2018 - constrained VAE for semantic validity of generated graph
\item GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules Example work showing EMD kernel:
\item Graph Normalizing Flows
\end{itemize}
This one has a fairly comprehensive website: \url{https://sites.google.com/view/graph-normalizing-flows/}
Full architecture

\begin{figure}[htbp]
\centering
\includegraphics[width=2.0in]{./images/full_arch_gnf.png}
\caption{\label{fig:Full architecture of the graph noramlizing flow DNN}figure name}
\end{figure}
\begin{itemize}
\item Graphite isotropic gaussian for VAE + iterative refinement for decoding
\end{itemize}
\subsection{Three most popular according to O'Bray 2021:}
\label{sec:org6aa90c1}
\begin{itemize}
\item GraphRNN, GRAN, Graph Score Matching.

\item Graph Recurrent Attention Networks, also uses graph spectra for MMD.
\begin{quote}
In previous work, You et al. [37] computed degree distributions, clustering
coefficient distributions, and the number of occurrence of all orbits with 4
nodes, and then used the maximum mean discrepancy (MMD) over these graph
statistics, relying on Gaussian kernels with the first Wasserstein distance,
i.e., earth mover’s distance (EMD), in the MMD.In practice, we found computing
this MMD with the Gaussian EMD kernel to be very slow for moderately large
graphs. Therefore, we use the total variation (TV) distance, which greatly
speeds up the evaluation and is still consistent with EMD. In addition to the
node degree, clustering coefficient and orbit counts (used by [36]), we also
compare the spectra of the graphs by computing the eigenvalues of the
normalized graph Laplacian (quantized to approximate a probability density).
This spectral comparison provides a view of the global graph properties,
whereas the previous metrics focus on local graph statistics.
\end{quote}

\item Graph Score Matching
On MMD, they say the following:
\end{itemize}
\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{./images/MMD_settings_graph_score_matching_paper.png}
\caption{\label{fig:MMD settings for evaluation of the graph score matching model}MMD optimization strategy}
\end{figure}

\section{Generative modelling metrics}
\label{sec:org1fa0965}
\subsection{Objective:}
\label{sec:org2a1924d}
\subsubsection{Generative graph dist close to the input graph dist}
\label{sec:org5610b01}
\subsubsection{(pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)}
\label{sec:orgc404d77}
\subsection{On images}
\label{sec:org6c4735a}
\subsubsection{Frechet Inception Distance}
\label{sec:org9f6993e}
The idea here is to use deeper representational layers of an ANN and used the squared Wasserstein metric to compare two multinomial Gaussians.
Introduced 2017
\subsubsection{LPIPS \href{https://richzhang.github.io/PerceptualSimilarity/}{Project page}}
\label{sec:orgf96923a}
Introduced 2017
\subsubsection{Why comparing graphs is hard:}
\label{sec:org179ea25}
\begin{itemize}
\item Metrics need to deal with spatial invariances such as cycles.
\item Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.
\item Other publications:
\end{itemize}
\subsection{Desiderata for good metrics:}
\label{sec:orgc10050b}
\begin{enumerate}
\item Robust to noise
\item Expressive, if they don't arise from the same dist, then metric should detect this.
\item Computationally efficient.
\end{enumerate}
\section{MMD - current accepted method to evaluate generative GNNs}
\label{sec:org16ea500}
\begin{itemize}
\item The MMD formula goes as follows:
\end{itemize}
\(\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)\)
\begin{itemize}
\item use it for hypothesis/two-sample testing.
\item In practice, we evaluate \(d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))\) for a distribution \(\mathcal{G}\). Given
multiple distributions \(G_1, G_2, \hdots\), the values of \(d_{MMD}\) can be used
to rank models, where smaller values are assumed to indicate a larger
agreement with the original distribution \(\mathcal{G}*\).
\item Commonly used kernels: first Wasserstein distance, total variation distance,
radial basis function.
\item Commonly used descriptor functions: degree distribution histogram, clustering
coefficient, Laplacian spectrum histogram.
\item Recommended kernels: RBF, Laplacian kernel, linear kernel (expressivity \& robustness need to be analyzed)
\end{itemize}
\subsection{Potential pitfalls of descriptors}
\label{sec:org60ed959}
\begin{itemize}
\item Degree distributions are ok seemingly
\item Clustering does not distinguish fully connected vs disconnected cliques
\item Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.
\item Parameters and descriptors are set a priori in the best case
\item Model performance is highly dependent on parameters and descriptor functions.
\end{itemize}
\section{Research objectives}
\label{sec:orgc945fa5}
There are multiple objectives here:
\begin{enumerate}
\item Find optimal kernel/hyperparameter combination based on controlled experiments on a given dataset to evaluate a good MMD configuration.
\begin{itemize}
\item For this we will need \url{https://www.alphafold.ebi.ac.uk/download}, because it's clean. Also filter single chain proteins to extract graphs in the first place.
\item This can be built as a first step to get the pipeline going.
\end{itemize}

\item Show which parameters influence evaluation and how?
\begin{itemize}
\item Conduct perturbation experiments on graphs
\end{itemize}

\item Find novel domain-agnostic evaluation \& domain-specific evaluation metrics
\begin{enumerate}
\item Domain-agnostic evaluation measures
\begin{itemize}
\item Correlation with graph-edit distance
\item Correlation with perturbation
\item Topology/persistence based approaches could be useful for modelling features like binding pockets, etc?
\end{itemize}

\item Domain-specific evaluation measures
\begin{itemize}
\item Alignment
\item Energy?
\end{itemize}
\end{enumerate}
\end{enumerate}
\subsection{From Tim: gather literature sources. Intro structure}
\label{sec:orge635ddf}
\subsubsection{Evaluation of generative models (different domains)}
\label{sec:org5995e50}
\subsubsection{Evaluation of generative models for graphs}
\label{sec:org19348c0}
\paragraph{Check how it was done before, why combo of parameters/kernels were used.}
\label{sec:orga36f565}
\subsubsection{Evaluation of proteins (…/molecules/drugs) (What makes a valid protein?)}
\label{sec:org46e435b}
\subsubsection{Evaluation of generative models for proteins}
\label{sec:org3bf7347}
\section{Module-wise breakdown of the plan}
\label{sec:orge5378d3}
\begin{itemize}
\item Graph extraction
\item Descriptor functions
\item kernels, MMD
\item Domain agnostic
\item Domain specific
\item Other metrics
\item TDA descriptors
\item Labeled edge graph
\item NSPDK
\item Other metrics
\item Extract graph from real datasets
\end{itemize}

\section{References}
\label{sec:orgd18938c}
\printbibliography[heading=none]
\end{document}
