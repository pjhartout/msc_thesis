#+BIND: org-export-use-babel nil
#+TITLE: Research notes on metrics for GNNs applied to biological problems
#+AUTHOR: Philip Hartout
#+EMAIL: <philip.hartout@protonmail.com>
#+DATE: Monday January 24, 2022
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:[a4paper,12pt,twoside]
#+LaTeX_HEADER:\usepackage[usenames,dvipsnames,figures]{xcolor}
#+LaTeX_HEADER:\usepackage[autostyle]{csquotes}
#+LaTeX_HEADER:\usepackage[final]{pdfpages}
#+LaTeX_HEADER:\usepackage{amsfonts, amssymb}            % Math symbols
#+LaTeX_HEADER:\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
#+LATEX_HEADER_EXTRA:\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
#+LATEX_HEADER_EXTRA:\newtheorem{definition}{Definition}[section]
#+MACRO: NEWLINE @@latex:\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session python_emacs_session :cache :results value
#+PROPERTY: header-args :exports none :tangle "~/Documents/Git/msc_thesis/thesis/refs.bib"
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+LATEX_COMPILER: pdflatexorg-mode restarted

#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
#+LATEX_HEADER: \nocite{*}
#+OPTIONS: <:nil c:nil todo:nil H:5

* Graph Neural Networks
** Reviews
*** Graph neural networks:
    A review of methods and applications
#+begin_src bibtex
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
#+end_src
Zhou et al mention several GNN approaches in use today
Generative models popular today:
**** Sequential graph generation process
 + GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
 #+begin_src bibtex
@inproceedings{you2018graphrnn,
  title={{GraphRNN: Generating realistic graphs with deep auto-regressive models}},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  booktitle={International conference on machine learning},
  pages={5708--5717},
  year={2018},
  organization={PMLR}
}

 #+end_src
 + Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
 #+begin_src bibtex
@article{li2018learning,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

 #+end_src
 + GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
 #+begin_src bibtex
@article{shi2020graphaf,
  title={{GraphAF: a flow-based autoregressive model for molecular graph generation}},
  author={Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  journal={arXiv preprint arXiv:2001.09382},
  year={2020}
}
 #+end_src
**** Non-sequential graph generation process
 + MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
#+begin_src bibtex
@article{de2018molgan,
  title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}
#+end_src
 + Ma et al 2018 - constrained VAE for semantic validity of generated graph
   #+begin_src bibtex
@article{ma2018constrained,
  title={Constrained generation of semantically valid graphs via regularizing variational autoencoders},
  author={Ma, Tengfei and Chen, Jie and Xiao, Cao},
  journal={arXiv preprint arXiv:1809.02630},
  year={2018}
}

   #+end_src
 + GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules
   #+begin_src bibtex
@article{you2018graph,
  title={Graph convolutional policy network for goal-directed molecular graph generation},
  author={You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1806.02473},
  year={2018}
}
   #+end_src
 + Graph Normalizing Flows
   #+begin_src bibtex
@article{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={arXiv preprint arXiv:1905.13177},
  year={2019}
}
   #+end_src
This one has a fairly comprehensive website: https://sites.google.com/view/graph-normalizing-flows/
Full architecture

#+NAME: fig:Full architecture of the graph noramlizing flow DNN
#+CAPTION: figure name
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200
[[./images/full_arch_gnf.png]]
 + Graphite isotropic gaussian for VAE + iterative refinement for decoding
   #+begin_src bibtex
@inproceedings{grover2019graphite,
  title={Graphite: Iterative generative modeling of graphs},
  author={Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2434--2444},
  year={2019},
  organization={PMLR}
}
   #+end_src

** Three most popular according to O'Bray 2021:
+ GraphRNN, GRAN, Graph Score Matching.

+ Graph Recurrent Attention Networks
  #+begin_src bibtex
@article{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S},
  journal={arXiv preprint arXiv:1910.00760},
  year={2019}
}
  #+end_src

  #+begin_quote
  In previous work, You et al. [37] computed degree distributions, clustering
  coefficient distributions, and the number of occurrence of all orbits with 4
  nodes, and then used the maximum mean discrepancy (MMD) over these graph
  statistics, relying on Gaussian kernels with the first Wasserstein distance,
  i.e., earth moverâ€™s distance (EMD), in the MMD.In practice, we found computing
  this MMD with the Gaussian EMD kernel to be very slow for moderately large
  graphs. Therefore, we use the total variation (TV) distance, which greatly
  speeds up the evaluation and is still consistent with EMD. In addition to the
  node degree, clustering coefficient and orbit counts (used by [36]), we also
  compare the spectra of the graphs by computing the eigenvalues of the
  normalized graph Laplacian (quantized to approximate a probability density).
  This spectral comparison provides a view of the global graph properties,
  whereas the previous metrics focus on local graph statistics.
  #+end_quote

+ Graph Score Matching
  #+begin_src bibtex
@inproceedings{niu2020permutation,
  title={Permutation invariant graph generation via score-based generative modeling},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}
  #+end_src

  On MMD, they say the following:
#+NAME: fig:MMD settings for evaluation of the graph score matching model
#+CAPTION: MMD optimization strategy
#+ATTR_ORG: :width 1000
#+ATTR_LATEX: :width \linewidth
#+ATTR_HTML: :width 500
[[./images/MMD_settings_graph_score_matching_paper.png]]

* Generative modelling metrics
** Objective:
*** Generative graph dist close to the input graph dist
*** (pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)
** On images
*** Frechet Inception Distance
The idea here is to use deeper representational layers of an ANN and used the squared Wasserstein metric to compare two multinomial Gaussians.
Introduced 2017
#+begin_src bibtex
@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
#+end_src
*** LPIPS [[https://richzhang.github.io/PerceptualSimilarity/][Project page]]
Introduced 2017
#+begin_src bibtex
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
#+end_src
*** Why comparing graphs is hard:
  + Metrics need to deal with spatial invariances such as cycles.
  + Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.
  + Other publications:
  #+begin_src bibtex
@article{theis2015note,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}
  #+end_src

** Desiderata for good metrics:
 1. Robust to noise
 2. Expressive, if they don't arise from the same dist, then metric should detect this.
 3. Computationally efficient.
* MMD - current accepted method to evaluate generative GNNs
+ The MMD formula goes as follows:
$\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)$
+ use it for hypothesis/two-sample testing.
+ In practice, we evaluate $d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))$ for a distribution $\mathcal{G}$. Given
  multiple distributions $G_1, G_2, \hdots$, the values of $d_{MMD}$ can be used
  to rank models, where smaller values are assumed to indicate a larger
  agreement with the original distribution $\mathcal{G}*$.
+ Commonly used kernels: first Wasserstein distance, total variation distance,
  radial basis function.
+ Commonly used descriptor functions: degree distribution histogram, clustering
  coefficient, Laplacian spectrum histogram.
+ Recommended kernels: RBF, Laplacian kernel, linear kernel (expressivity & robustness need to be analyzed)
** Potential pitfalls of descriptors
+ Degree distributions are ok seemingly
+ Clustering does not distinguish fully connected vs disconnected cliques
+ Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.
+ Parameters and descriptors are set a priori in the best case
+ Model performance is highly dependent on parameters and descriptor functions.

* Research objectives
There are multiple objectives here:
1. Find optimal kernel/hyperparameter combination based on controlled experiments on a given dataset to evaluate a good MMD configuration.
   + For this we will need https://www.alphafold.ebi.ac.uk/download, because it's clean. Also filter single chain proteins to extract graphs in the first place.
   + This can be built as a first step to get the pipeline going.

2. Show which parameters influence evaluation and how?
   + Conduct perturbation experiments on graphs

3. Find novel domain-agnostic evaluation & domain-specific evaluation metrics
   1. Domain-agnostic evaluation measures
      + Correlation with graph-edit distance
      + Correlation with perturbation
      + Topology/persistence based approaches could be useful for modelling features like binding pockets, etc?

   2. Domain-specific evaluation measures
      + Alignment
      + Energy?


* Plan
Make hyperparameter finding module
** DONE Extract graphs from alphafold DB
** Determine perturbations
** Optimize MMD to find best combo for proportional increase according to proportional perturbation


* References
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
  #+LaTeX: \printbibliography
