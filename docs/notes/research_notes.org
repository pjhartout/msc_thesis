#+BIND: org-export-use-babel nil
#+TITLE: Research notes on metrics for GNNs applied to biological problems
#+AUTHOR: Philip Hartout
#+EMAIL: <philip.hartout@protonmail.com>
#+DATE: Monday January 24, 2022
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:[a4paper,12pt,twoside]
#+LaTeX_HEADER:\usepackage[usenames,dvipsnames,figures]{xcolor}
#+LaTeX_HEADER:\usepackage[autostyle]{csquotes}
#+LaTeX_HEADER:\usepackage[final]{pdfpages}
#+LaTeX_HEADER:\usepackage{amsfonts, amssymb}            % Math symbols
#+LaTeX_HEADER:\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
#+LATEX_HEADER_EXTRA:\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
#+LATEX_HEADER_EXTRA:\newtheorem{definition}{Definition}[section]
#+MACRO: NEWLINE @@latex:\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session python_emacs_session :cache :results value
#+PROPERTY: header-args :exports none :tangle "~/Documents/Git/msc_thesis/thesis/refs.bib"
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+LATEX_COMPILER: pdflatexorg-mode restarted
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
#+LATEX_HEADER: \nocite{*}
#+OPTIONS: <:nil c:nil todo:nil H:5
#+EXCLUDE_TAGS: noexport
* Fundamental concepts
** Graph Laplacian
   Given Adjacency matrix $A$ of dimension $n\times n$ and degree matrix $D$ of
   a given graph $G$, the graph Laplacian $L$ of $G$ is given by:
   $L=D-A$.
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iL^i$. $p_w(L)$ is $n\times n$.
** ChebNet
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iT_iL^i$
   where $T_i$ is the degree-i Chebyshev polynomial of the first kind and
   $\widetilde{L}$ is the normalized Laplacian derived using the largest
   eigenvalue of $L$.

   $L$ is p.s.d., all eigenvalues of $L$ are $>0$. If $\lambda_{\max}(L)>1$,
   then $L$'s entries increase. $\widetilde{L}$ normalizes the eigenvalues of $L$
   and bounds them in $[-1,1]$. $\widetilde{L}$ is defined as
\begin{equation*}
\widetilde{L} = {2L\over\lambda_{\max}(L) - I_n}
\end{equation*}
Embedding computation
1. $h^{0}=x$
2. For $k=1,2,\dots,K$:
   1. $p^{(k)}=p_{w^{(k)}}(L)$
   2. $g^{(k)}=p^{(k)}\times h^{k-1}$
   3. $h^{(k)} = \sigma(g^{(k)})$
where $\sigma$ is some non-linearity.

** Modern GNNs
   When going back to $p_w(L)=L$, focussing on one verted, we have:
   \begin{align*}
(Lx)_v &= L_v x\\
&= \sum_{u\in G}L_{vu}x_u\\
&= \sum_{u\in G}(D_{vu}-A_{vu})x_u\\
&= D_vx_v-\sum_{u\in\mathcal{N}(v)}x_u
   \end{align*}
   This is a 1-hop localized convolution. This does two steps:
   + Aggregating over immediate neighbour features $x_u$.
   + Combining with the node's own feature $x_v$
   Ensuring that the agg. is node order equivariant, the overall convolution
   becomes node-order equivariant. This can be considered as messsage passing
   between adjacent nodes. After each step, some nodes receive information from
   their neighbour.

   By iteratively repeating the 1-hop localized convolutions $K$ times (i.e.
   repeatedly passing messages), the receptive field of the conv. effectively
   includes all nodes up to $K$ hops away.

   Some modern aggregation and combination functions:
   + graph convolutional networks (GCN)
   + graph attention networks (GAT)
   + graph sample and aggregate (GraphSAGE)
   + graph isomorphism network (GIN)
** Global convolutions
*** Spectral convolutions
    Smoother graphs have a quantity $R_(x)=\sum_{(i,j)\in E}(x_i-x_j)^2$ that
    reflects that feature vectors $x$ that assign similar values to adjacent
    nodes in $G$ would have smaller values of $R_L(x)$. $L$ is a real, symmetric
    matrix which has eigenvalues $\lambda_1\leq\dots\leq\lambda_n$. Eigenvectors
    can be taken to be orthonormal.

    The set of eigenvalues of $L$ are successively less smooth. They are called the spectrum of $L$.

    The spectral decomp. of $L$ as $L=U\Lambda U^T$ where $\Lambda$ is the
    diagonal matrix of sorted eigenvalues, and $U$ denotes the matrix of the
    eigenvectors. sorted by increasing eigenvalues. Orthonormality between
    eigenvectors gives us $U^T U = I$. Since each $u\in\mathbb{R}^n$, any $x$
    can be represented as a linear combination of these eigenvectors, i.e.:
    \begin{equation*}
    x = \sum_{i=1}^{n}\widetilde{x_i}u_i=U\widetilde{x}.
    \end{equation*}
    where $\widetilde{x}$ are the coefficients. Again, the orthonormality of the eigenvalues allows us to state $x=U\widetilde{x} \iff U^Tx=\widetilde{x}$.
    We can then compute global convolutions, defining:
    \begin{equation*}
h^{(k)} =
\begin{bmatrix}
h^{(k)}_1\\
\vdots\\
h^{(k)}_n\\
\end{bmatrix}
    \end{equation*}
    We start with the original features $h^{(0)}=x$, then
    1. $\hat{h}^{(k-1)} = U_m^Th^{(k-1)}$
    2. $\hat{g}^{(k)}=\hat{w}^{(k)}\odot\hat{h}^{(k-1)}$
    3. $h^{(k)}=\sigma(g^{(k)})$

* Graph Neural Networks
** Reviews
*** Graph neural networks:
    A review of methods and applications
#+begin_src bibtex
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
#+end_src
Zhou et al mention several GNN approaches in use today
Generative models popular today:
**** Sequential graph generation process
 + GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
 #+begin_src bibtex
@inproceedings{you2018graphrnn,
  title={{GraphRNN: Generating realistic graphs with deep auto-regressive models}},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  booktitle={International conference on machine learning},
  pages={5708--5717},
  year={2018},
  organization={PMLR}
}

 #+end_src
 + Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
 #+begin_src bibtex
@article{li2018learning,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

 #+end_src
 + GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
 #+begin_src bibtex
@article{shi2020graphaf,
  title={{GraphAF: a flow-based autoregressive model for molecular graph generation}},
  author={Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  journal={arXiv preprint arXiv:2001.09382},
  year={2020}
}
 #+end_src
**** Non-sequential graph generation process
 + MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
#+begin_src bibtex
@article{de2018molgan,
  title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}
#+end_src
 + Ma et al 2018 - constrained VAE for semantic validity of generated graph
   #+begin_src bibtex
@article{ma2018constrained,
  title={Constrained generation of semantically valid graphs via regularizing variational autoencoders},
  author={Ma, Tengfei and Chen, Jie and Xiao, Cao},
  journal={arXiv preprint arXiv:1809.02630},
  year={2018}
}

   #+end_src
 + GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules Example work showing EMD kernel:
   #+begin_src bibtex
@article{you2018graph,
  title={Graph convolutional policy network for goal-directed molecular graph generation},
  author={You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1806.02473},
  year={2018}
}
   #+end_src
 + Graph Normalizing Flows
   #+begin_src bibtex
@article{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={arXiv preprint arXiv:1905.13177},
  year={2019}
}
   #+end_src
This one has a fairly comprehensive website: https://sites.google.com/view/graph-normalizing-flows/
Full architecture

#+NAME: fig:Full architecture of the graph noramlizing flow DNN
#+CAPTION: figure name
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200
[[./images/full_arch_gnf.png]]
 + Graphite isotropic gaussian for VAE + iterative refinement for decoding
   #+begin_src bibtex
@inproceedings{grover2019graphite,
  title={Graphite: Iterative generative modeling of graphs},
  author={Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2434--2444},
  year={2019},
  organization={PMLR}
}
   #+end_src

** Three most popular according to O'Bray 2021:
   #+begin_src bibtex
@article{o2021evaluation,
  title={Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions},
  author={O'Bray, Leslie and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
  journal={arXiv preprint arXiv:2106.01098},
  year={2021}
}
   #+end_src
+ GraphRNN, GRAN, Graph Score Matching.

+ Graph Recurrent Attention Networks, also uses graph spectra for MMD.
  #+begin_src bibtex
@article{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S},
  journal={arXiv preprint arXiv:1910.00760},
  year={2019}
}
  #+end_src

  #+begin_quote
  In previous work, You et al. [37] computed degree distributions, clustering
  coefficient distributions, and the number of occurrence of all orbits with 4
  nodes, and then used the maximum mean discrepancy (MMD) over these graph
  statistics, relying on Gaussian kernels with the first Wasserstein distance,
  i.e., earth mover’s distance (EMD), in the MMD.In practice, we found computing
  this MMD with the Gaussian EMD kernel to be very slow for moderately large
  graphs. Therefore, we use the total variation (TV) distance, which greatly
  speeds up the evaluation and is still consistent with EMD. In addition to the
  node degree, clustering coefficient and orbit counts (used by [36]), we also
  compare the spectra of the graphs by computing the eigenvalues of the
  normalized graph Laplacian (quantized to approximate a probability density).
  This spectral comparison provides a view of the global graph properties,
  whereas the previous metrics focus on local graph statistics.
  #+end_quote

+ Graph Score Matching
  #+begin_src bibtex
@inproceedings{niu2020permutation,
  title={Permutation invariant graph generation via score-based generative modeling},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}
  #+end_src

  On MMD, they say the following:
#+NAME: fig:MMD settings for evaluation of the graph score matching model
#+CAPTION: MMD optimization strategy
#+ATTR_ORG: :width 1000
#+ATTR_LATEX: :width \linewidth
#+ATTR_HTML: :width 500
[[./images/MMD_settings_graph_score_matching_paper.png]]

* Generative modelling metrics
** Objective:
*** Generative graph dist close to the input graph dist
*** (pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)
** On images
*** Frechet Inception Distance
The idea here is to use deeper representational layers of an ANN and used the squared Wasserstein metric to compare two multinomial Gaussians.
Introduced 2017
#+begin_src bibtex
@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
#+end_src
*** LPIPS [[https://richzhang.github.io/PerceptualSimilarity/][Project page]]
Introduced 2017
#+begin_src bibtex
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
#+end_src
*** Why comparing graphs is hard:
  + Metrics need to deal with spatial invariances such as cycles.
  + Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.
  + Other publications:
  #+begin_src bibtex
@article{theis2015note,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}
  #+end_src

** Desiderata for good metrics:
 1. Robust to noise
 2. Expressive, if they don't arise from the same dist, then metric should detect this.
 3. Computationally efficient.
* MMD - current accepted method to evaluate generative GNNs
+ The MMD formula goes as follows:
$\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)$
+ use it for hypothesis/two-sample testing.
+ In practice, we evaluate $d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))$ for a distribution $\mathcal{G}$. Given
  multiple distributions $G_1, G_2, \hdots$, the values of $d_{MMD}$ can be used
  to rank models, where smaller values are assumed to indicate a larger
  agreement with the original distribution $\mathcal{G}*$.
+ Commonly used kernels: first Wasserstein distance, total variation distance,
  radial basis function.
+ Commonly used descriptor functions: degree distribution histogram, clustering
  coefficient, Laplacian spectrum histogram.
+ Recommended kernels: RBF, Laplacian kernel, linear kernel (expressivity & robustness need to be analyzed)
** Potential pitfalls of descriptors
+ Degree distributions are ok seemingly
+ Clustering does not distinguish fully connected vs disconnected cliques
+ Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.
+ Parameters and descriptors are set a priori in the best case
+ Model performance is highly dependent on parameters and descriptor functions.
* Research objectives
There are multiple objectives here:
1. Find optimal kernel/hyperparameter combination based on controlled experiments on a given dataset to evaluate a good MMD configuration.
   + For this we will need https://www.alphafold.ebi.ac.uk/download, because it's clean. Also filter single chain proteins to extract graphs in the first place.
   + This can be built as a first step to get the pipeline going.

2. Show which parameters influence evaluation and how?
   + Conduct perturbation experiments on graphs

3. Find novel domain-agnostic evaluation & domain-specific evaluation metrics
   1. Domain-agnostic evaluation measures
      + Correlation with graph-edit distance
      + Correlation with perturbation
      + Topology/persistence based approaches could be useful for modelling features like binding pockets, etc?

   2. Domain-specific evaluation measures
      + Alignment
      + Energy?
** From Tim: gather literature sources. Intro structure
*** Evaluation of generative models (different domains)
*** Evaluation of generative models for graphs
**** Check how it was done before, why combo of parameters/kernels were used.
*** Evaluation of proteins (…/molecules/drugs) (What makes a valid protein?)
*** Evaluation of generative models for proteins
* Module-wise breakdown of the plan
+ Graph extraction
+ Descriptor functions
+ kernels, MMD
+ Domain agnostic
+ Domain specific
+ Other metrics
+ TDA descriptors
+ Labeled edge graph
+ NSPDK
+ Other metrics
+ Extract graph from real datasets

* Annotations :noexport:
Online approach to k-NN graph construction
#+begin_src bibtex
@article{zhao2021approximate,
  title={Approximate k-NN graph construction: a generic online approach},
  author={Zhao, Wan-Lei and Wang, Hui and Ngo, Chong-Wah},
  journal={IEEE Transactions on Multimedia},
  year={2021},
  publisher={IEEE}
}
#+end_src

epsilon nearest neighbor graphs dissertation with history and construction methods.
#+begin_src bibtex
@phdthesis{anastasiu2016algorithms,
  title={Algorithms for Constructing Exact Nearest Neighbor Graphs},
  author={Anastasiu, David C},
  year={2016},
  school={University of Minnesota}
}
#+end_src

Giotto-TDA library
#+begin_src bibtex
@article{tauzin2021giotto,
  title={giotto-tda:: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration.},
  author={Tauzin, Guillaume and Lupo, Umberto and Tunstall, Lewis and P{\'e}rez, Julian Burella and Caorsi, Matteo and Medina-Mardones, Anibal M and Dassatti, Alberto and Hess, Kathryn},
  journal={J. Mach. Learn. Res.},
  volume={22},
  pages={39--1},
  year={2021}
}
#+end_src

Heat kernel on persistence diagrams
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={A stable multi-scale kernel for topological machine learning},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4741--4748},
  year={2015}
}
#+end_src

original mmd papers
#+begin_src bibtex
@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
  publisher={JMLR. org}
}
#+end_src

Specific MMD for biological data:
#+begin_src bibtex
@article{borgwardt2006integrating,
  title={Integrating structured biological data by kernel maximum mean discrepancy},
  author={Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J and Kriegel, Hans-Peter and Sch{\"o}lkopf, Bernhard and Smola, Alex J},
  journal={Bioinformatics},
  volume={22},
  number={14},
  pages={e49--e57},
  year={2006},
  publisher={Oxford University Press}
}
#+end_src

** Additional metrics for GGNNs
Introduction several metrics based on the features extracted by an untrained
random GNN, showing more expressive metrics of GNN performance. "In this work,
we mitigate these issues by searching for scalar, domain-agnostic, and scalable
metrics for evaluating and ranking GGMs. To this end, we study existing GGM
metrics and neural-network-based metrics emerging from generative models of
images that use embeddings extracted from a task-specific network."

+ States "all metrics are frequently displayed together to approximate generation quality and evaluate GGMs" -> problematic for ranking.
+ MMD does not incorporate node/edge features, only graph structure.
+ Use image domain metrics using random and pretrained GNNs.
+ (Q1) What are the strengths and limitations of each metric?
+ (Q2) Is pretraining a GNN necessary to accurately evaluate GGMs with image domain metrics? -> No
+ (Q3) Is there a strong scalar and domain-agnostic metric for evaluating and ranking GGMs?
+ Expressivity: ability of graphs to measure diversity of generated graphs
  + They simulate expressivity by running Affinity Propagation on graphs, by simulating mode collapse and mode dropping.
  + "To simulate mode collapse, we progressively replace each datapoint with its cluster centre. The degree of perturbation $t$ represents the ratio of clusters that have been collapsed in this manner."
  + To simulate mode dropping, we progressively remove clusters from $\mathbb{S}_g$. To keep $|\mathbb{S}_g|$ constant, we randomly select samples from the remaining clusters to duplicate. In this experiment, the degree of perturbation $t$ is the ratio of clusters that have been deleted from $\mathbb{S}_g$.

+ GIN work uses ZINC dataset to show that the metric captures changes in node and edge labels.
+ Rank correlation is used (Spearman, Pearson biased for linearity)

#+begin_src bibtex
@article{thompson2022evaluation,
  title={On Evaluation Metrics for Graph Generative Models},
  author={Thompson, Rylee and Knyazev, Boris and Ghalebi, Elahe and Kim, Jungtaek and Taylor, Graham W},
  journal={arXiv preprint arXiv:2201.09871},
  year={2022}
}
#+end_src

Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
Includes node and edge features but does not handle continuous features in evaluation.
#+begin_src bibtex
@inproceedings{costa2010fast,
  title={Fast neighborhood subgraph pairwise distance kernel},
  author={Costa, Fabrizio and De Grave, Kurt},
  booktitle={ICML},
  year={2010}
}
#+end_src

Examples using the Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
#+begin_src bibtex
@article{podda2021graphgen,
  title={GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation},
  author={Podda, Marco and Bacciu, Davide},
  journal={arXiv preprint arXiv:2107.08396},
  year={2021}
}
#+end_src
#+begin_src bibtex
@article{kawai2019scalable,
  title={Scalable Generative Models for Graphs with Graph Attention Mechanism},
  author={Kawai, Wataru and Mukuta, Yusuke and Harada, Tatsuya},
  journal={arXiv preprint arXiv:1906.01861},
  year={2019}
}
#+end_src
#+begin_src bibtex
@inproceedings{goyal2020graphgen,
  title={GraphGen: a scalable approach to domain-agnostic labeled graph generation},
  author={Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1253--1263},
  year={2020}
}
#+end_src

Inverse protein folding problem, maybe good to look at to see what makes a good protein.
#+begin_src bibtex
@article{ingraham2019generative,
  title={Generative models for graph-based protein design},
  author={Ingraham, John and Garg, Vikas K and Barzilay, Regina and Jaakkola, Tommi},
  year={2019}
}
#+end_src

Papers discussing Delaunay graphs (dual graph of Voronoi diagram), obtained through Delaunay triangulation. Useful for extracting hierarchical structures. Graph edges can also be added on the basis of the Delaunay triangulation. Delaunay triangles correspond to joining points that share a face in the 3D Voronoi diagram of the protein structures. For distance-based edges, a Long Interaction Network (LIN) parameter controls the minimum required separation in the amino acid sequence for edge creation. This can be useful in reducing the number of noisy edges under distance-based edge creation schemes. Edge featurisation for atom-level graphs is provided by annotations of bond type and ring status.
#+begin_src bibtex
@article{taylor2006graph,
  title={Graph theoretic properties of networks formed by the Delaunay tessellation of protein structures},
  author={Taylor, Todd J and Vaisman, Iosif I},
  journal={Physical Review E},
  volume={73},
  number={4},
  pages={041925},
  year={2006},
  publisher={APS}
}
#+end_src

Not a great review, but still useful source of references and gives overview of the field.
#+begin_src bibtex
@article{fasoulis2021graph,
  title={Graph representation learning for structural proteomics},
  author={Fasoulis, Romanos and Paliouras, Georgios and Kavraki, Lydia E},
  journal={Emerging Topics in Life Sciences},
  volume={5},
  number={6},
  pages={789--802},
  year={2021},
  publisher={Portland Press Ltd.}
}
#+end_src

Increased the number of metrics by performing MMD directly with node and edge feature distributions.
#+begin_src bibtex
@inproceedings{goyal2020graphgen,
  title={GraphGen: a scalable approach to domain-agnostic labeled graph generation},
  author={Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1253--1263},
  year={2020}
}
#+end_src

Randomly initialized CNN to evaluate generative models
#+begin_src bibtex
@article{xu2018empirical,
  title={An empirical study on evaluation metrics of generative adversarial networks},
  author={Xu, Qiantong and Huang, Gao and Yuan, Yang and Guo, Chuan and Sun, Yu and Wu, Felix and Weinberger, Kilian},
  journal={arXiv preprint arXiv:1806.07755},
  year={2018}
}
@inproceedings{naeem2020reliable,
  title={Reliable fidelity and diversity metrics for generative models},
  author={Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  booktitle={International Conference on Machine Learning},
  pages={7176--7185},
  year={2020},
  organization={PMLR}
}
#+end_src

GAN work on metric disturbances:
#+begin_src bibtex
@article{xu2018empirical,
  title={An empirical study on evaluation metrics of generative adversarial networks},
  author={Xu, Qiantong and Huang, Gao and Yuan, Yang and Guo, Chuan and Sun, Yu and Wu, Felix and Weinberger, Kilian},
  journal={arXiv preprint arXiv:1806.07755},
  year={2018}
}
#+end_src


** Failure modes in molecule generation and optimization
   Blunt way to eliminate molecules:
+ penalize high logP (high bioaccumulation, highly lipophillic) .
+ GuacaMol, check also MOSES
+ Frechet Chemnet distance
+ Difficult to assess whether or not the model capture intrinsic patterns of the data or just copies it. (/copy problem/, e.g. AddCarbon model)
+ likelihood on test set should be reported
+ scores not optimizing for what practicioner wants.
+ models generate molecules numerically superior but not useful, refinement of scoring function to account for unexpected behaviour.
  + This phenomenon of optimization of a score in unexpected ways, has been observed in other applications [39]. In one illustrative setting,
    the aim was to develop a body capable of locomotion, but the optimization procedure instead discovered the simpler solution of a tall
    body falling over, which also satisfied the specified scoring function.
+ Things difficult to quantify: bioactivites, constraints "respected" by medicinal chemists
+ Set up an experiment with optimization score and control scores, to get
  insight into how a generative model optimizes the score and whether it is
  sensitive to mentioned biases (data splits) in the bioactivity model. they see
  that the optimization score increases rapidly compared to data control score.

#+begin_src bibtex
@article{renz2019failure,
  title={On failure modes in molecule generation and optimization},
  author={Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={Drug Discovery Today: Technologies},
  volume={32},
  pages={55--63},
  year={2019},
  publisher={Elsevier}
}
#+end_src

Protein embeddings for binding residues prediction for multiple ligand classes.
#+begin_src bibtex
@article{littmann2021protein,
  title={Protein embeddings and deep learning predict binding residues for various ligand classes},
  author={Littmann, Maria and Heinzinger, Michael and Dallago, Christian and Weissenow, Konstantin and Rost, Burkhard},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--15},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

Graph Kernels for comparative analysis of protein active sites (NADH, ATP binding site). Use random walk, shortest path and fingerprint kernel.
#+begin_src bibtex
@inproceedings{fober2009graph,
  title={Graph-kernels for the comparative analysis of protein active sites},
  author={Fober, Thomas and Mernberger, Marco and Moritz, Ralph and H{\"u}llermeier, Eyke},
  booktitle={German conference on bioinformatics 2009},
  year={2009},
  organization={Gesellschaft f{\"u}r Informatik eV}
}
#+end_src

Persistence Fisher Kernel
#+begin_src bibtex
@article{le2018persistence,
  title={Persistence fisher kernel: A riemannian manifold kernel for persistence diagrams},
  author={Le, Tam and Yamada, Makoto},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
#+end_src

Sliced Wasserstein kernel (approximates the wasserstein distance in kernel space)
#+begin_src bibtex
@inproceedings{carriere2017sliced,
  title={Sliced Wasserstein kernel for persistence diagrams},
  author={Carriere, Mathieu and Cuturi, Marco and Oudot, Steve},
  booktitle={International conference on machine learning},
  pages={664--673},
  year={2017},
  organization={PMLR}
}
#+end_src

Proof that the Wasserstein distance is not negative definite and that
transforming it to a similarity measure is not possible. (See Appendix A.)
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={A stable multi-scale kernel for topological machine learning},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4741--4748},
  year={2015}
}
#+end_src

** Check criteria to be admitted to pdb
**** https://www.wwpdb.org/validation/2017/XrayValidationReportHelp

****  This is all "relative (percentile rank) compared to the rest of the database"


Survey of graph kernels
#+begin_src bibtex
@article{borgwardt2020graph,
  title={Graph kernels: State-of-the-art and future challenges},
  author={Borgwardt, Karsten and Ghisu, Elisabetta and Llinares-L{\'o}pez, Felipe and O'Bray, Leslie and Rieck, Bastian},
  journal={arXiv preprint arXiv:2011.03854},
  year={2020}
}
#+end_src

Kernel using Wasserstein distance
#+begin_src bibtex
@article{oh2019kernel,
    title={Kernel wasserstein distance},
    author={Oh, Jung Hun and Pouryahya, Maryam and Iyer, Aditi and Apte, Aditya P and Tannenbaum, Allen and Deasy, Joseph O},
    journal={arXiv preprint arXiv:1905.09314},
    year={2019}
  }
#+end_src

* References
#+LaTeX: \printbibliography[heading=none]
