#+BIND: org-export-use-babel nil
#+TITLE: Research notes on metrics for GNNs applied to biological problems
#+AUTHOR: Philip Hartout
#+EMAIL: <philip.hartout@protonmail.com>
#+DATE: Monday January 24, 2022
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:[a4paper,12pt,twoside]
#+LaTeX_HEADER:\usepackage[usenames,dvipsnames,figures]{xcolor}
#+LaTeX_HEADER:\usepackage[autostyle]{csquotes}
#+LaTeX_HEADER:\usepackage[final]{pdfpages}
#+LaTeX_HEADER:\usepackage{amsfonts, amssymb}            % Math symbols
#+LaTeX_HEADER:\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
#+LATEX_HEADER_EXTRA:\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
#+LATEX_HEADER_EXTRA:\newtheorem{definition}{Definition}[section]
#+MACRO: NEWLINE @@latex:\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session python_emacs_session :cache :results value
#+PROPERTY: header-args :exports none :tangle "~/Documents/Git/msc_thesis/thesis/refs.bib"
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+LATEX_COMPILER: pdflatexorg-mode restarted
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
#+LATEX_HEADER: \nocite{*}
#+OPTIONS: <:nil c:nil todo:nil H:5
#+EXCLUDE_TAGS: noexport
* Fundamental concepts
** Graph Laplacian
   Given Adjacency matrix $A$ of dimension $n\times n$ and degree matrix $D$ of
   a given graph $G$, the graph Laplacian $L$ of $G$ is given by:
   $L=D-A$.
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iL^i$. $p_w(L)$ is $n\times n$.
** ChebNet
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iT_iL^i$
   where $T_i$ is the degree-i Chebyshev polynomial of the first kind and
   $\widetilde{L}$ is the normalized Laplacian derived using the largest
   eigenvalue of $L$.

   $L$ is p.s.d., all eigenvalues of $L$ are $>0$. If $\lambda_{\max}(L)>1$,
   then $L$'s entries increase. $\widetilde{L}$ normalizes the eigenvalues of $L$
   and bounds them in $[-1,1]$. $\widetilde{L}$ is defined as
\begin{equation*}
\widetilde{L} = {2L\over\lambda_{\max}(L) - I_n}
\end{equation*}
Embedding computation
1. $h^{0}=x$
2. For $k=1,2,\dots,K$:
   1. $p^{(k)}=p_{w^{(k)}}(L)$
   2. $g^{(k)}=p^{(k)}\times h^{k-1}$
   3. $h^{(k)} = \sigma(g^{(k)})$
where $\sigma$ is some non-linearity.

** Modern GNNs
   When going back to $p_w(L)=L$, focussing on one verted, we have:
   \begin{align*}
(Lx)_v &= L_v x\\
&= \sum_{u\in G}L_{vu}x_u\\
&= \sum_{u\in G}(D_{vu}-A_{vu})x_u\\
&= D_vx_v-\sum_{u\in\mathcal{N}(v)}x_u
   \end{align*}
   This is a 1-hop localized convolution. This does two steps:
   + Aggregating over immediate neighbour features $x_u$.
   + Combining with the node's own feature $x_v$
   Ensuring that the agg. is node order equivariant, the overall convolution
   becomes node-order equivariant. This can be considered as messsage passing
   between adjacent nodes. After each step, some nodes receive information from
   their neighbour.

   By iteratively repeating the 1-hop localized convolutions $K$ times (i.e.
   repeatedly passing messages), the receptive field of the conv. effectively
   includes all nodes up to $K$ hops away.

   Some modern aggregation and combination functions:
   + graph convolutional networks (GCN)
   + graph attention networks (GAT)
   + graph sample and aggregate (GraphSAGE)
   + graph isomorphism network (GIN)
** Global convolutions
*** Spectral convolutions
    Smoother graphs have a quantity $R_(x)=\sum_{(i,j)\in E}(x_i-x_j)^2$ that
    reflects that feature vectors $x$ that assign similar values to adjacent
    nodes in $G$ would have smaller values of $R_L(x)$. $L$ is a real, symmetric
    matrix which has eigenvalues $\lambda_1\leq\dots\leq\lambda_n$. Eigenvectors
    can be taken to be orthonormal.

    The set of eigenvalues of $L$ are successively less smooth. They are called the spectrum of $L$.

    The spectral decomp. of $L$ as $L=U\Lambda U^T$ where $\Lambda$ is the
    diagonal matrix of sorted eigenvalues, and $U$ denotes the matrix of the
    eigenvectors. sorted by increasing eigenvalues. Orthonormality between
    eigenvectors gives us $U^T U = I$. Since each $u\in\mathbb{R}^n$, any $x$
    can be represented as a linear combination of these eigenvectors, i.e.:
    \begin{equation*}
    x = \sum_{i=1}^{n}\widetilde{x_i}u_i=U\widetilde{x}.
    \end{equation*}
    where $\widetilde{x}$ are the coefficients. Again, the orthonormality of the eigenvalues allows us to state $x=U\widetilde{x} \iff U^Tx=\widetilde{x}$.
    We can then compute global convolutions, defining:
    \begin{equation*}
h^{(k)} =
\begin{bmatrix}
h^{(k)}_1\\
\vdots\\
h^{(k)}_n\\
\end{bmatrix}
    \end{equation*}
    We start with the original features $h^{(0)}=x$, then
    1. $\hat{h}^{(k-1)} = U_m^Th^{(k-1)}$
    2. $\hat{g}^{(k)}=\hat{w}^{(k)}\odot\hat{h}^{(k-1)}$
    3. $h^{(k)}=\sigma(g^{(k)})$

* Graph Neural Networks
** Reviews
*** Graph neural networks:
    A review of methods and applications
#+begin_src bibtex
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
#+end_src
Zhou et al mention several GNN approaches in use today
Generative models popular today:
**** Sequential graph generation process
 + GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
 #+begin_src bibtex
@inproceedings{you2018graphrnn,
  title={{GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models}},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2018},
}

 #+end_src
 + Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
 #+begin_src bibtex
@article{li2018learning,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

 #+end_src
 + GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
 #+begin_src bibtex
@inproceedings{shi2019graphaf,
  title={GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation},
  author={Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

 #+end_src
**** Non-sequential graph generation process
 + MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
#+begin_src bibtexe
@article{de2018molgan,
  title={{MolGAN: An implicit generative model for small molecular graphs}},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={Proceedings of the International Conference on Machine Learning Workshop on Theoretical Foundations
  and Applications of Deep Generative Models},
  year={2018}
}
#+end_src
 + Ma et al 2018 - constrained VAE for semantic validity of generated graph
   #+begin_src bibtex
@inproceedings{ma2018constrained,
  title={Constrained generation of semantically valid graphs via regularizing variational autoencoders},
  author={Ma, Tengfei and Chen, Jie and Xiao, Cao},
  booktitle={Proceedings of the 31st Conference on Advances in Neural Information Processing Systems},
  year={2018}
}
   #+end_src
 + GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules Example work showing EMD kernel:
   #+begin_src bibtex
@inproceedings{you2018graph,
  title={Graph convolutional policy network for goal-directed molecular graph generation},
  author={You, Jiaxuan and Liu, Bowen and Ying, Zhitao and Pande, Vijay and Leskovec, Jure},
  journal={Proceedings of the 31st Conference on Advances in Neural Information Processing Systems},
  year={2018}
}
   #+end_src
 + Graph Normalizing Flows
   #+begin_src bibtex
@inproceedings{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={Proceedings of the 32nd Conference on Advances in Neural Information Processing Systems},
  year={2019}
}
   #+end_src
This one has a fairly comprehensive website: https://sites.google.com/view/graph-normalizing-flows/
Full architecture

#+NAME: fig:Full architecture of the graph noramlizing flow DNN
#+CAPTION: figure name
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200
[[./images/full_arch_gnf.png]]
 + Graphite isotropic gaussian for VAE + iterative refinement for decoding
   #+begin_src bibtex
@inproceedings{grover2019graphite,
  title={Graphite: Iterative generative modeling of graphs},
  author={Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={2434--2444},
  year={2019},
  organization={PMLR}
}
   #+end_src

** Three most popular according to O'Bray 2021:
   #+begin_src bibtex
@inproceedings{obray2022evaluation,
	title        = {Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions},
	author       = {Leslie O'Bray and Max Horn and Bastian Rieck and Karsten Borgwardt},
	year         = 2022,
	booktitle    = {Proceedings of the International Conference on Learning Representations},
}
   #+end_src
+ GraphRNN, GRAN, Graph Score Matching.

+ Graph Recurrent Attention Networks, also uses graph spectra for MMD. GRAN.
  #+begin_src bibtex
@inproceedings{liao2019efficient,
  title={{Efficient Graph Generation with Graph Recurrent Attention Networks}},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={4255--4265},
  year={2019}
}

  #+end_src

  #+begin_quote
  In previous work, You et al. [37] computed degree distributions, clustering
  coefficient distributions, and the number of occurrence of all orbits with 4
  nodes, and then used the maximum mean discrepancy (MMD) over these graph
  statistics, relying on Gaussian kernels with the first Wasserstein distance,
  i.e., earth mover’s distance (EMD), in the MMD.In practice, we found computing
  this MMD with the Gaussian EMD kernel to be very slow for moderately large
  graphs. Therefore, we use the total variation (TV) distance, which greatly
  speeds up the evaluation and is still consistent with EMD. In addition to the
  node degree, clustering coefficient and orbit counts (used by [36]), we also
  compare the spectra of the graphs by computing the eigenvalues of the
  normalized graph Laplacian (quantized to approximate a probability density).
  This spectral comparison provides a view of the global graph properties,
  whereas the previous metrics focus on local graph statistics.
  #+end_quote

+ Graph Score Matching
  #+begin_src bibtex
@inproceedings{niu2020permutation,
  title={{Permutation Invariant Graph Generation via Score-Based Generative Modeling}},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={Proceedings of the  International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}
  #+end_src

  On MMD, they say the following:
#+NAME: fig:MMD settings for evaluation of the graph score matching model
#+CAPTION: MMD optimization strategy
#+ATTR_ORG: :width 1000
#+ATTR_LATEX: :width \linewidth
#+ATTR_HTML: :width 500
[[./images/MMD_settings_graph_score_matching_paper.png]]

* Generative modelling metrics
** Objective:
*** Generative graph dist close to the input graph dist
*** (pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)
** On images
*** Frechet Inception Distance
The idea here is to use deeper representational layers of an ANN and used the squared Wasserstein metric to compare two multinomial Gaussians.
Introduced 2017
#+begin_src bibtex
@inproceedings{heusel2017gans,
  title={{GANs trained by a two time-scale update rule converge to a local nash equilibrium}},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6629--6640},
  year={2017}
}

#+end_src
*** LPIPS [[https://richzhang.github.io/PerceptualSimilarity/][Project page]]
Introduced 2017
#+begin_src bibtex
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
#+end_src
*** Why comparing graphs is hard:
  + Metrics need to deal with spatial invariances such as cycles.
  + Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.
  + Other publications:
  #+begin_src bibtex
@inproceedings{theis2016note,
  title={A note on the evaluation of generative models},
  author={Theis, L and van den Oord, A and Bethge, M},
  booktitle={Proceedings of the International Conference on Learning Representations},
  pages={1--10},
  year={2016}
}
  #+end_src

** Desiderata for good metrics:
 1. Robust to noise
 2. Expressive, if they don't arise from the same dist, then metric should detect this.
 3. Computationally efficient.
* MMD - current accepted method to evaluate generative GNNs
+ The MMD formula goes as follows:
$\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)$
+ use it for hypothesis/two-sample testing.
+ In practice, we evaluate $d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))$ for a distribution $\mathcal{G}$. Given
  multiple distributions $G_1, G_2, \hdots$, the values of $d_{MMD}$ can be used
  to rank models, where smaller values are assumed to indicate a larger
  agreement with the original distribution $\mathcal{G}*$.
+ Commonly used kernels: first Wasserstein distance, total variation distance,
  radial basis function.
+ Commonly used descriptor functions: degree distribution histogram, clustering
  coefficient, Laplacian spectrum histogram.
+ Recommended kernels: RBF, Laplacian kernel, linear kernel (expressivity & robustness need to be analyzed)
** Potential pitfalls of descriptors
+ Degree distributions are ok seemingly
+ Clustering does not distinguish fully connected vs disconnected cliques
+ Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.
+ Parameters and descriptors are set a priori in the best case
+ Model performance is highly dependent on parameters and descriptor functions.
* Research objectives
There are multiple objectives here:
1. Find optimal kernel/hyperparameter combination based on controlled experiments on a given dataset to evaluate a good MMD configuration.
   + For this we will need https://www.alphafold.ebi.ac.uk/download, because it's clean. Also filter single chain proteins to extract graphs in the first place.
   + This can be built as a first step to get the pipeline going.

2. Show which parameters influence evaluation and how?
   + Conduct perturbation experiments on graphs

3. Find novel domain-agnostic evaluation & domain-specific evaluation metrics
   1. Domain-agnostic evaluation measures
      + Correlation with graph-edit distance
      + Correlation with perturbation
      + Topology/persistence based approaches could be useful for modelling features like binding pockets, etc?

   2. Domain-specific evaluation measures
      + Alignment
      + Energy?
** From Tim: gather literature sources. Intro structure
*** Evaluation of generative models (different domains)
*** Evaluation of generative models for graphs
**** Check how it was done before, why combo of parameters/kernels were used.
*** Evaluation of proteins (…/molecules/drugs) (What makes a valid protein?)
*** Evaluation of generative models for proteins
* Module-wise breakdown of the plan
+ Graph extraction
+ Descriptor functions
+ kernels, MMD
+ Domain agnostic
+ Domain specific
+ Other metrics
+ TDA descriptors
+ Labeled edge graph
+ NSPDK
+ Other metrics
+ Extract graph from real datasets

* Annotations :noexport:

Protein engineering techniques.
#+begin_src bibtex
@article{poluri2017protein,
  title={{Protein Engineering Techniques: Gateways to Synthetic Protein Universe}},
  author={Poluri, Krishna Mohan and Gulati, Khushboo},
  journal={{SpringerBriefs in Forensic and Medical Bioinformatics}},
  year={2017},
  publisher={Singapore: Springer}
}
#+end_src

  Blog explaining weisfeiler-lehmann procedure (with illustrations)
  #+begin_src bibtex
@misc{mengin2019,
  author       = {Elie Mengin},
  year         = {2019},
  title        = {{Weisfeiler-Lehman Graph Kernel for Binary Function Analysis}},
  howpublished = {\url{https://blog.quarkslab.com/weisfeiler-lehman-graph-kernel-for-binary-function-analysis.html}},
  organization = {Quarkslab},
}
  #+end_src

Transformer blog
#+begin_src bibtex
@misc{alammar2018,
  author       = {Jay Alammar},
  year         = {2018},
  title        = {{The Illustrated Transformer}},
howpublished = {\url{https://jalammar.github.io/illustrated-transformer/}},
}
#+end_src

Proof that graph laplacian spectrum eigenvalues are bounded in
  #+begin_src bibtex
@book{chung1997spectral,
  title={{Spectral Graph Theory}},
  author = {Chung, Fan R. K.},
  year={1997},
  publisher={American Mathematical Society}
}

#+end_src

Spectrum determines graphs?
#+begin_src bibtex
@article{van2003graphs,
  title={Which graphs are determined by their spectrum?},
  author={Van Dam, Edwin R and Haemers, Willem H},
  journal={Linear Algebra and its Applications},
  volume={373},
  pages={241--272},
  year={2003},
  publisher={Elsevier}
}
#+end_src
Trees are cospectral
#+begin_src bibtex
@article{schwenk1973almost,
  title={{Almost All Trees Are Cospectral}},
  author={Schwenk, Allen J},
  journal={New Directions in the Theory of Graphs},
  pages={275--307},
  year={1973},
  publisher={Academic Press}
}
#+end_src

AlphaFold Model
#+begin_src bibtex
@article{jumper2021highly,
  title={{Highly accurate protein structure prediction with AlphaFold}},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

AlphaFold EBI Database
#+begin_src bibtex
@article{varadi2022alphafold,
  title={{AlphaFold Protein Structure Database: Massively expanding the structural coverage of protein-sequence space with high-accuracy models}},
  author={Varadi, Mihaly and Anyango, Stephen and Deshpande, Mandar and Nair, Sreenath and Natassia, Cindy and Yordanova, Galabina and Yuan, David and Stroe, Oana and Wood, Gemma and Laydon, Agata and others},
  journal={{Nucleic Acids Research}},
  volume={50},
  number={D1},
  pages={D439--D444},
  year={2022},
  publisher={{Oxford University Press}}
}
#+end_src

AlphaFold Human proteome
#+begin_src bibtex
@article{tunyasuvunakool2021highly,
  title={Highly accurate protein structure prediction for the human proteome},
  author={Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and {\v{Z}}{\'\i}dek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={590--596},
  year={2021},
  publisher={{Nature Publishing Group}}
}
#+end_src

ESM FAIR models
#+begin_src bibtex
@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={{Proceedings of the National Academy of Sciences}},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={{National Acad Sciences}}
}
#+end_src

Transformer original paper
#+begin_src bibtex
@inproceedings{vaswani2017attention,
  title={{Attention is all you need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={{Proceedings of the 30th IEEE conference on computer vision and pattern recognition workshops}},
  year={2017}
}
#+end_src


#+begin_src bibtex
@article{weisfeiler1968reduction,
  title={{The reduction of a graph to canonical form and the algebra which appears therein}},
  author={Weisfeiler, Boris and Lehman, Andrei},
  journal={{NTI, Series}},
  volume={2},
  number={9},
  pages={12--16},
  year={1968}
}
#+end_src

Papers setting \Theta to euclidean space to turn PDs into points in some probability distribution.
#+begin_src bibtex
@inproceedings{anirudh2016riemannian,
  title={{A Riemannian Framework for Statistical Analysis of Topological Persistence Diagrams}},
  author={Anirudh, Rushil and Venkataraman, Vinay and Natesan Ramamurthy, Karthikeyan and Turaga, Pavan},
  booktitle={{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops}},
  pages={68--76},
  year={2016}
}
#+end_src


Clustering coefficients & cliques
#+begin_src bibtex
@article{watts1998collective,
  title={{Collective dynamics of ‘small-world’ networks}},
  author={Watts, Duncan J and Strogatz, Steven H},
  journal={Nature},
  volume={393},
  number={6684},
  pages={440--442},
  year={1998},
  publisher={{Nature Publishing Group}}
}
#+end_src


Online approach to k-NN graph construction
#+begin_src bibtex
@article{zhao2021approximate,
  title={{Approximate k-NN Graph Construction: A Generic Online Approach}},
  author={Zhao, Wan-Lei and Wang, Hui and Ngo, Chong-Wah},
  journal={IEEE Transactions on Multimedia},
  volume={24},
  pages={1909--1921},
  year={2021},
  publisher={IEEE}
}

#+end_src

epsilon nearest neighbor graphs dissertation with history and construction methods.
#+begin_src bibtex
@phdthesis{anastasiu2016algorithms,
  title={{Algorithms for Constructing Exact Nearest Neighbor Graphs}},
  author={Anastasiu, David C},
  year={2016},
  school={University of Minnesota}
}
#+end_src

Giotto-TDA library
#+begin_src bibtex
@article{tauzin2021giotto,
  title={{giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration}},
  author={Tauzin, Guillaume and Lupo, Umberto and Tunstall, Lewis and Perez, Julian Burella and Caorsi, Matteo and Medina-Mardones, Anibal M and Dassatti, Alberto and Hess, Kathryn},
  journal={Journal Of Machine Learning Research},
  volume={22},
  year={2021}
}

#+end_src

Heat kernel on persistence diagrams
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={{A Stable Multi-Scale Kernel for Topological Machine Learning}},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4741--4748},
  year={2015}
}
#+end_src

original mmd papers
#+begin_src bibtex
@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
}
#+end_src

Specific MMD for biological data:
#+begin_src bibtex
@article{borgwardt2006integrating,
  title={{Integrating structured biological data by Kernel Maximum Mean Discrepancy}},
  author={Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J and Kriegel, Hans-Peter and Sch{\"o}lkopf, Bernhard and Smola, Alex J},
  journal={Bioinformatics},
  volume={22},
  number={14},
  pages={e49--e57},
  year={2006},
  publisher={Oxford University Press}
}
#+end_src

** Additional metrics for GGNNs
Introduction several metrics based on the features extracted by an untrained
random GNN, showing more expressive metrics of GNN performance. "In this work,
we mitigate these issues by searching for scalar, domain-agnostic, and scalable
metrics for evaluating and ranking GGMs. To this end, we study existing GGM
metrics and neural-network-based metrics emerging from generative models of
images that use embeddings extracted from a task-specific network."

+ States "all metrics are frequently displayed together to approximate generation quality and evaluate GGMs" -> problematic for ranking.
+ MMD does not incorporate node/edge features, only graph structure.
+ Use image domain metrics using random and pretrained GNNs.
+ (Q1) What are the strengths and limitations of each metric?
+ (Q2) Is pretraining a GNN necessary to accurately evaluate GGMs with image domain metrics? -> No
+ (Q3) Is there a strong scalar and domain-agnostic metric for evaluating and ranking GGMs?
+ Expressivity: ability of graphs to measure diversity of generated graphs
  + They simulate expressivity by running Affinity Propagation on graphs, by simulating mode collapse and mode dropping.
  + "To simulate mode collapse, we progressively replace each datapoint with its cluster centre. The degree of perturbation $t$ represents the ratio of clusters that have been collapsed in this manner."
  + To simulate mode dropping, we progressively remove clusters from $\mathbb{S}_g$. To keep $|\mathbb{S}_g|$ constant, we randomly select samples from the remaining clusters to duplicate. In this experiment, the degree of perturbation $t$ is the ratio of clusters that have been deleted from $\mathbb{S}_g$.

+ GIN work uses ZINC dataset to show that the metric captures changes in node and edge labels.
+ Rank correlation is used (Spearman, Pearson biased for linearity)

#+begin_src bibtex
@inproceedings{thompson2022evaluation,
  title={{On Evaluation Metrics for Graph Generative Models}},
  author={Thompson, Rylee and Knyazev, Boris and Ghalebi, Elahe and Kim, Jungtaek and Taylor, Graham W},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2022},
}
#+end_src

Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
Includes node and edge features but does not handle continuous features in evaluation.
#+begin_src bibtex
@inproceedings{costa2010fast,
  title={{Fast Neighborhood Subgraph Pairwise Distance Kernel}},
  author={Costa, Fabrizio and De Grave, Kurt},
  booktitle={Proceedings of the 26th International Conference on Machine Learning},
  pages={255--262},
  year={2010},
  organization={Omnipress; Madison, WI, USA}
}
#+end_src

Examples using the Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
#+begin_src bibtex
@inproceedings{podda2021graphgen,
  title={{Graphgen-redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation}},
  author={Podda, Marco and Bacciu, Davide},
  booktitle={Proceedings of the International Joint Conference on Neural Networks},
  year={2021},
}
#+end_src

#+begin_src bibtex
@article{kawai2019scalable,
  title={Scalable Generative Models for Graphs with Graph Attention Mechanism},
  author={Kawai, Wataru and Mukuta, Yusuke and Harada, Tatsuya},
  journal={arXiv preprint arXiv:1906.01861},
  year={2019}
}
#+end_src

Increased the number of metrics by performing MMD directly with node and edge feature distributions.
#+begin_src bibtex
@inproceedings{goyal2020graphgen,
  title={{GraphGen: a scalable approach to domain-agnostic labeled graph generation}},
  author={Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1253--1263},
  year={2020}
}
#+end_src

Inverse protein folding problem, maybe good to look at to see what makes a good protein.

Papers discussing Delaunay graphs (dual graph of Voronoi diagram), obtained through Delaunay triangulation. Useful for extracting hierarchical structures. Graph edges can also be added on the basis of the Delaunay triangulation. Delaunay triangles correspond to joining points that share a face in the 3D Voronoi diagram of the protein structures. For distance-based edges, a Long Interaction Network (LIN) parameter controls the minimum required separation in the amino acid sequence for edge creation. This can be useful in reducing the number of noisy edges under distance-based edge creation schemes. Edge featurisation for atom-level graphs is provided by annotations of bond type and ring status.
#+begin_src bibtex
@article{taylor2006graph,
  title={{Graph theoretic properties of networks formed by the Delaunay tessellation of protein structures}},
  author={Taylor, Todd J and Vaisman, Iosif I},
  journal={Physical Review E},
  volume={73},
  number={4},
  pages={041925},
  year={2006},
  publisher={APS}
}
#+end_src

Not a great review, but still useful source of references and gives overview of the field.
#+begin_src bibtex
@article{fasoulis2021graph,
  title={{Graph representation learning for structural proteomics}},
  author={Fasoulis, Romanos and Paliouras, Georgios and Kavraki, Lydia E},
  journal={Emerging Topics in Life Sciences},
  volume={5},
  number={6},
  pages={789--802},
  year={2021},
  publisher={Portland Press Ltd.}
}
#+end_src


Randomly initialized CNN to evaluate generative models
#+begin_src bibtex
@inproceedings{naeem2020reliable,
  title={{Reliable Fidelity and Diversity Metrics for Generative Models}},
  author={Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2020},
}
#+end_src

GAN work on metric disturbances:
#+begin_src bibtex
@article{xu2018empirical,
  title={An empirical study on evaluation metrics of generative adversarial networks},
  author={Xu, Qiantong and Huang, Gao and Yuan, Yang and Guo, Chuan and Sun, Yu and Wu, Felix and Weinberger, Kilian},
  journal={arXiv preprint arXiv:1806.07755},
  year={2018}
}
#+end_src


** Failure modes in molecule generation and optimization
   Blunt way to eliminate molecules:
+ penalize high logP (high bioaccumulation, highly lipophillic) .
+ GuacaMol, check also MOSES
+ Frechet Chemnet distance
+ Difficult to assess whether or not the model capture intrinsic patterns of the data or just copies it. (/copy problem/, e.g. AddCarbon model)
+ likelihood on test set should be reported
+ scores not optimizing for what practicioner wants.
+ models generate molecules numerically superior but not useful, refinement of scoring function to account for unexpected behaviour.
  + This phenomenon of optimization of a score in unexpected ways, has been observed in other applications [39]. In one illustrative setting,
    the aim was to develop a body capable of locomotion, but the optimization procedure instead discovered the simpler solution of a tall
    body falling over, which also satisfied the specified scoring function.
+ Things difficult to quantify: bioactivites, constraints "respected" by medicinal chemists
+ Set up an experiment with optimization score and control scores, to get
  insight into how a generative model optimizes the score and whether it is
  sensitive to mentioned biases (data splits) in the bioactivity model. they see
  that the optimization score increases rapidly compared to data control score.

#+begin_src bibtex
@article{renz2019failure,
  title={On failure modes in molecule generation and optimization},
  author={Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={Drug Discovery Today: Technologies},
  volume={32},
  pages={55--63},
  year={2019},
  publisher={Elsevier}
}
#+end_src

Protein embeddings for binding residues prediction for multiple ligand classes.
#+begin_src bibtex
@article{littmann2021protein,
  title={Protein embeddings and deep learning predict binding residues for various ligand classes},
  author={Littmann, Maria and Heinzinger, Michael and Dallago, Christian and Weissenow, Konstantin and Rost, Burkhard},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--15},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

Graph Kernels for comparative analysis of protein active sites (NADH, ATP binding site). Use random walk, shortest path and fingerprint kernel.
#+begin_src bibtex
@inproceedings{fober2009graph,
  title={Graph-kernels for the comparative analysis of protein active sites},
  author={Fober, Thomas and Mernberger, Marco and Moritz, Ralph and H{\"u}llermeier, Eyke},
  booktitle={German conference on bioinformatics 2009},
  year={2009},
  organization={Gesellschaft f{\"u}r Informatik eV}
}
#+end_src

Persistence Fisher Kernel
#+begin_src bibtex
@inproceedings{le2018persistence,
  title={{Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams}},
  author={Le, Tam and Yamada, Makoto},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={10028--10039},
  year={2018}
}

#+end_src

Sliced Wasserstein kernel (approximates the wasserstein distance in kernel space)
#+begin_src bibtex
@inproceedings{carriere2017sliced,
  title={{Sliced Wasserstein kernel for persistence diagrams}},
  author={Carri{\`e}re, Mathieu and Cuturi, Marco and Oudot, Steve},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  pages={664--673},
  year={2017}
}
#+end_src

Proof that the Wasserstein distance is not negative definite and that
transforming it to a similarity measure is not possible. (See Appendix A.)
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={{A Stable Multi-Scale Kernel for Topological Machine Learning}},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015},
  pages={4741--4748},
  year={2015},
  organization={IEEE Computer Society}
}
#+end_src

Gudhi references
#+begin_src bibtex
@inproceedings{maria2014gudhi,
  title={{The Gudhi Library: Simplicial Complexes and Persistent Homology}},
  author={Maria, Cl{\'e}ment and Boissonnat, Jean-Daniel and Glisse, Marc and Yvinec, Mariette},
  booktitle={The 4th International Congress on Mathematical Software (ICMS)},
  year={2014}
}
#+end_src

Persistence weighted Gaussian kernel (slower than Fisher-based kernel)
#+begin_src bibtex
@inproceedings{kusano2016persistence,
  title={{Persistence weighted Gaussian kernel for topological data analysis}},
  author={Kusano, Genki and Fukumizu, Kenji and Hiraoka, Yasuaki},
  booktitle={Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48},
  pages={2004--2013},
  year={2016}
}

#+end_src


Survey of graph kernels
#+begin_src bibtex
@article{borgwardt2020graph,
  title={Graph Kernels: State-of-the-Art and Future Challenges},
  author={Borgwardt, Karsten and Ghisu, Elisabetta and Llinares-L{\'o}pez, Felipe and O’Bray, Leslie and Rieck, Bastian Alexander},
  journal={Foundations and Trends in Machine Learning},
  volume={13},
  number={5-6},
  pages={531--712},
  year={2020},
  publisher={Now Publishers}
}

#+end_src

Kernel using Wasserstein distance
#+begin_src bibtex
@article{oh2019kernel,
    title={{Kernel Wasserstein Distance}},
    author={Oh, Jung Hun and Pouryahya, Maryam and Iyer, Aditi and Apte, Aditya P and Tannenbaum, Allen and Deasy, Joseph O},
    journal={arXiv preprint arXiv:1905.09314},
    year={2019}
  }
#+end_src


Some good work on persistent homology on proteins:
#+begin_src bibtex
@article{xia2014persistent,
  title={{Persistent homology analysis of protein structure, flexibility, and folding}},
  author={Xia, Kelin and Wei, Guo-Wei},
  journal={Proceedings of the International Journal for Numerical Methods in Biomedical Engineering},
  year={2014},
  publisher={Wiley Online Library}
}
#+end_src

Persistent homology used to analyse protein folding states
#+begin_src bibtex
@article{ichinomiya2020protein,
  title={{Protein-Folding Analysis Using Features Obtained by Persistent Homology}},
  author={Ichinomiya, Takashi and Obayashi, Ippei and Hiraoka, Yasuaki},
  journal={Biophysical Journal},
  volume={118},
  number={12},
  pages={2926--2937},
  year={2020},
  publisher={Elsevier}
}
#+end_src

Reviews different types of PH and applies it to analyze DNA data.
#+begin_src bibtex
@article{meng2020weighted,
  title={Weighted persistent homology for biomolecular data analysis},
  author={Meng, Zhenyu and Anand, D Vijay and Lu, Yunpeng and Wu, Jie and Xia, Kelin},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}
#+end_src

Check criteria to be admitted to pdb
\href{https://www.wwpdb.org/validation/2017/XrayValidationReportHelp}{relative}
This is all "relative (percentile rank) compared to the rest of the database"

Validation papers:
#+begin_src bibtex
@article{read2011new,
  title={{A New Generation of Crystallographic Validation Tools for the Protein Data Bank}},
  author={Read, Randy J and Adams, Paul D and Arendall, W Bryan and Brunger, Axel T and Emsley, Paul and Joosten, Robbie P and Kleywegt, Gerard J and Krissinel, Eugene B and L{\"u}tteke, Thomas and Otwinowski, Zbyszek and others},
  journal={Structure},
  volume={10},
  number={19},
  pages={1395--1412},
  year={2011}
}

@article{gore2017validation,
  title={{Validation of structures in the Protein Data Bank}},
  author={Gore, Swanand and Garc{\'\i}a, Eduardo Sanz and Hendrickx, Pieter MS and Gutmanas, Aleksandras and Westbrook, John D and Yang, Huanwang and Feng, Zukang and Baskaran, Kumaran and Berrisford, John M and Hudson, Brian P and others},
  journal={Structure},
  volume={25},
  number={12},
  pages={1916--1927},
  year={2017},
  publisher={Elsevier}
}
@article{gore2012implementing,
  title={{Implementing an X-ray validation pipeline for the Protein Data Bank}},
  author={Gore, Swanand and Velankar, Sameer and Kleywegt, Gerard J},
  journal={Acta Crystallographica Section D: Biological Crystallography},
  volume={68},
  number={4},
  pages={478--483},
  year={2012},
  publisher={International Union of Crystallography}
}

#+end_src

#+begin_src bibtex
@article{ramachandran1063Stereochemistry,
title = {{Stereochemistry of Polypeptide Chain Configurations}},
journal = {Journal of Molecular Biology},
author = {G.N. Ramachandran and C. Ramakrishnan and V. Sasisekharan},
volume = {7},
number = {1},
pages = {95-99},
year = {1963},
issn = {0022-2836},
doi = {https://doi.org/10.1016/S0022-2836(63)80023-6},
}
#+end_src

Good review of kernels, applications and other theoretical considerations
#+begin_src bibtex
@article{ghojogh2021reproducing,
  title={{Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr{\"o}m Method, and Use of Kernels in Machine Learning: Tutorial and Survey}},
  author={Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  journal={arXiv e-prints},
  pages={arXiv--2106},
  year={2021}
}

#+end_src

Example of ramachandran plot
#+begin_src bibtex
@article{nayak2018identification,
  title={{Identification of potential inhibitors for mycobacterial uridine diphosphogalactofuranose-galactopyranose mutase enzyme: A novel drug target through in silico approach}},
  author={Nayak, Tapaswini and Jena, Lingaraja and Waghmare, Pranita and Harinath, Bhaskar C and others},
  journal={International Journal of Mycobacteriology},
  volume={7},
  number={1},
  pages={61},
  year={2018},
  publisher={Medknow Publications}
}
#+end_src

* [#B] Matrix of experiments to perform

| Graph extraction | Perturbations | Descriptor   | Kernels            | Organism | Sample size |
| --               | twist         | tda          | persistence fisher | human    |         100 |
| --               | twist         | ramachandran | linear             | human    |         100 |
| --               | mutation      | esm          | linear             | human    |         100 |
| eps              | rewire        | degree       | linear             | human    |         100 |
| eps              | rewire        | degree       | gaussian           | human    |         100 |
| eps              | twist         | degree       | linear             | human    |         100 |
| eps              | twist         | degree       | gaussian           | human    |         100 |
| k-nn             | rewire        | degree       | linear             | human    |         100 |
| k-nn             | rewire        | degree       | gaussian           | human    |         100 |
| k-nn             | twist         | degree       | linear             | human    |         100 |
| k-nn             | twist         | degree       | gaussian           | human    |         100 |
| eps              | twist         | --           | w-l                | human    |         100 |
| eps              | shear         | --           | w-l                | human    |         100 |
| eps              | rewire        | --           | w-l                | human    |         100 |
| k-nn             | twist         | --           | w-l                | human    |         100 |
| k-nn             | shear         | --           | w-l                | human    |         100 |
| k-nn             | rewire        | --           | w-l                | human    |         100 |
| --               | twist         | tda          | persistence fisher | ecoli    |         100 |
| --               | twist         | ramachandran | linear             | ecoli    |         100 |
| --               | mutation      | esm          | linear             | ecoli    |         100 |
| eps              | rewire        | degree       | linear             | ecoli    |         100 |
| eps              | rewire        | degree       | gaussian           | ecoli    |         100 |
| eps              | twist         | degree       | linear             | ecoli    |         100 |
| eps              | twist         | degree       | gaussian           | ecoli    |         100 |
| k-nn             | rewire        | degree       | linear             | ecoli    |         100 |
| k-nn             | rewire        | degree       | gaussian           | ecoli    |         100 |
| k-nn             | twist         | degree       | linear             | ecoli    |         100 |
| k-nn             | twist         | degree       | gaussian           | ecoli    |         100 |
| eps              | twist         | --           | w-l                | ecoli    |         100 |
| eps              | shear         | --           | w-l                | ecoli    |         100 |
| eps              | rewire        | --           | w-l                | ecoli    |         100 |
| k-nn             | twist         | --           | w-l                | ecoli    |         100 |
| k-nn             | shear         | --           | w-l                | ecoli    |         100 |
| k-nn             | rewire        | --           | w-l                | ecoli    |         100 |

Filter by relevant sections

List of references from TDA internship
#+begin_src bibtex
@article{toniolo2018patterns,
  title={Patterns of cerebellar gray matter atrophy across Alzheimer’s disease progression},
  author={Toniolo, Sofia and Serra, Laura and Olivito, Giusy and Marra, Camillo and Bozzali, Marco and Cercignani, Mara},
  journal={Frontiers in Cellular Neuroscience},
  volume={12},
  pages={430},
  year={2018},
  publisher={Frontiers}
}
@article{amezquita2020shape,
  title={The shape of things to come: Topological data analysis and biology, from molecules to organisms},
  author={Am{\'e}zquita, Erik J and Quigley, Michelle Y and Ophelders, Tim and Munch, Elizabeth and Chitwood, Daniel H},
  journal={Developmental Dynamics},
  year={2020},
  publisher={Wiley Online Library}
}
@article{world2017global,
  title={Global action plan on the public health response to dementia 2017--2025},
  author={WHO},
  year={2017},
  publisher={World Health Organization}
}
@article{sleeman2019escalating,
  title={The escalating global burden of serious health-related suffering: projections to 2060 by world regions, age groups, and health conditions},
  author={Sleeman, Katherine E and de Brito, Maja and Etkind, Simon and Nkhoma, Kennedy and Guo, Ping and Higginson, Irene J and Gomes, Barbara and Harding, Richard},
  journal={The Lancet Global Health},
  volume={7},
  number={7},
  pages={e883--e892},
  year={2019},
  publisher={Elsevier}
}
@article{yiannopoulou2020current,
  title={Current and Future Treatments in Alzheimer Disease: An Update},
  author={Yiannopoulou, Konstantina G and Papageorgiou, Sokratis G},
  journal={Journal of Central Nervous System Disease},
  volume={12},
  pages={1179573520907397},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{mckhann2011diagnosis,
  title={The diagnosis of dementia due to Alzheimer's disease: recommendations from the National Institute on Aging-Alzheimer's Association workgroups on diagnostic guidelines for Alzheimer's disease},
  author={McKhann, Guy M and Knopman, David S and Chertkow, Howard and Hyman, Bradley T and Jack Jr, Clifford R and Kawas, Claudia H and Klunk, William E and Koroshetz, Walter J and Manly, Jennifer J and Mayeux, Richard and others},
  journal={Alzheimer's \& dementia},
  volume={7},
  number={3},
  pages={263--269},
  year={2011},
  publisher={Wiley Online Library}
}
@article{lehmann2016biomarkers,
  title={Biomarkers of Alzheimer’s disease: The present and the future},
  author={Lehmann, Sylvain and Teunissen, Charlotte Elisabeth},
  journal={Frontiers in Neurology},
  volume={7},
  pages={158},
  year={2016},
  publisher={Frontiers}
}
@article{smits2012early,
  title={Early onset Alzheimer's disease is associated with a distinct neuropsychological profile},
  author={Smits, Lieke L and Pijnenburg, Yolande AL and Koedam, Esther LGE and van der Vlies, Annelies E and Reuling, Ilona EW and Koene, Teddy and Teunissen, Charlotte E and Scheltens, Philip and van der Flier, Wiesje M},
  journal={Journal of Alzheimer's Disease},
  volume=30,
  number=1,
  pages={101--108},
  year=2012,
  publisher={IOS Press}
}
@article{hur2020innate,
  title={The innate immunity protein IFITM3 modulates $\gamma$-secretase in Alzheimer’s disease},
  author={Hur, Ji-Yeun and Frost, Georgia R and Wu, Xianzhong and Crump, Christina and Pan, Si Jia and Wong, Eitan and Barros, Marilia and Li, Thomas and Nie, Pengju and Zhai, Yujia and others},
  journal={Nature},
  pages={1--6},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{tharp2013origins,
  title={Origins of amyloid-$\beta$},
  author={Tharp, William G and Sarkar, Indra Neil},
  journal={BMC genomics},
  volume={14},
  number={1},
  pages={1--15},
  year={2013},
  publisher={BioMed Central}
}
@article{fulop2018can,
  title={Can an infection hypothesis explain the beta amyloid hypothesis of Alzheimer’s disease?},
  author={Fulop, Tamas and Witkowski, Jacek M and Bourgade, Karine and Khalil, Abdelouahed and Zerif, Echarki and Larbi, Anis and Hirokawa, Katsuiku and Pawelec, Graham and Bocti, Christian and Lacombe, Guy and others},
  journal={Frontiers in aging neuroscience},
  volume={10},
  pages={224},
  year={2018},
  publisher={Frontiers}
}
@article{frisoni2010clinical,
  title={The clinical use of structural MRI in Alzheimer disease},
  author={Frisoni, Giovanni B and Fox, Nick C and Jack, Clifford R and Scheltens, Philip and Thompson, Paul M},
  journal={Nature Reviews Neurology},
  volume={6},
  number={2},
  pages={67--77},
  year={2010},
  publisher={Nature Publishing Group}
}
@article{dawkins2014insights,
  title={Insights into the physiological function of the $\beta$-amyloid precursor protein: beyond Alzheimer's disease},
  author={Dawkins, Edgar and Small, David H},
  journal={Journal of neurochemistry},
  volume={129},
  number={5},
  pages={756--769},
  year={2014},
  publisher={Wiley Online Library}
}
@article{da2016insights,
  title={Insights on the pathophysiology of Alzheimer's disease: The crosstalk between amyloid pathology, neuroinflammation and the peripheral immune system},
  author={D{\'a} Mesquita, Sandro and Ferreira, Ana Catarina and Sousa, Jo{\~a}o Carlos and Correia-Neves, Margarida and Sousa, Nuno and Marques, Fernanda},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={68},
  pages={547--562},
  year={2016},
  publisher={Elsevier}
}
@article{poulakis2018heterogeneous,
  title={Heterogeneous patterns of brain atrophy in Alzheimer's disease},
  author={Poulakis, Konstantinos and Pereira, Joana B and Mecocci, Patrizia and Vellas, Bruno and Tsolaki, Magda and K{\l}oszewska, Iwona and Soininen, Hilkka and Lovestone, Simon and Simmons, Andrew and Wahlund, Lars-Olof and others},
  journal={Neurobiology of aging},
  volume={65},
  pages={98--108},
  year={2018},
  publisher={Elsevier}
}
@book{poincare1895analysis,
  title={{Analysis Situs}},
  author={Poincar{\'e}, Henri},
  year={1895},
  publisher={Gauthier-Villars}
}
@article{dey1999computational,
  title={{Computational Topology}},
  author={Dey, Tamal K and Edelsbrunner, Herbert and Guha, Sumanta},
  journal={Contemporary Mathematics},
  volume=223,
  pages={109--144},
  year=1999,
  publisher={American Mathematical Society}
}
@book{james1999history,
  title={{History of Topology}},
  author={James, Ioan Mackenzie},
  year={1999},
  publisher={Elsevier}
}
@article{freedman2009algebraic,
  title={{Algebraic Topology for Computer Vision}},
  author={Freedman, Daniel and Chen, Chao},
  journal={Computer Vision},
  pages={239--268},
  year={2009}
}
@book{edelsbrunner2010computational,
  title={Computational Topology: An Introduction},
  author={Edelsbrunner, Herbert and Harer, John},
  year={2010},
  publisher={American Mathematical Society}
}
@article{adams2017persistence,
  title={{Persistence images: A stable vector representation of persistent homology}},
  author={Adams, Henry and Emerson, Tegan and Kirby, Michael and Neville, Rachel and Peterson, Chris and Shipman, Patrick and Chepushtanova, Sofya and Hanson, Eric and Motta, Francis and Ziegelmeier, Lori},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={218--252},
  year={2017},
  publisher={JMLR. org}
}
@article{mileyko2011probability,
  title={Probability measures on the space of persistence diagrams},
  author={Mileyko, Yuriy and Mukherjee, Sayan and Harer, John},
  journal={Inverse Problems},
  volume={27},
  number={12},
  pages={124007},
  year={2011},
  publisher={IOP Publishing}
}
@article{ghrist2008barcodes,
  title={{Barcodes: The persistent topology of data}},
  author={Ghrist, Robert},
  journal={Bulletin of the American Mathematical Society},
  volume={45},
  number={1},
  pages={61--75},
  year={2008}
}
@incollection{bubenik2020persistence,
  title={The persistence landscape and some of its properties},
  author={Bubenik, Peter},
  booktitle={Topological Data Analysis},
  pages={97--117},
  year={2020},
  publisher={Springer}
}
@article{bubenik2015statistical,
  title={Statistical topological data analysis using persistence landscapes},
  author={Bubenik, Peter},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={77--102},
  year={2015},
  publisher={JMLR. org}
}
@article{bubenik2017persistence,
  title={A persistence landscapes toolbox for topological statistics},
  author={Bubenik, Peter and D{\l}otko, Pawe{\l}},
  journal={Journal of Symbolic Computation},
  volume={78},
  pages={91--114},
  year={2017},
  publisher={Elsevier}
}
@article{berwald2018computing,
  title={Computing Wasserstein distance for persistence diagrams on a quantum computer},
  author={Berwald, Jesse J and Gottlieb, Joel M and Munch, Elizabeth},
  journal={arXiv preprint arXiv:1809.06433},
  year={2018}
}
@article{goedert2006century,
  title={A century of Alzheimer's disease},
  author={Goedert, Michel and Spillantini, Maria Grazia},
  journal={science},
  volume={314},
  number={5800},
  pages={777--781},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@article{jack2008alzheimer,
  title={The Alzheimer's disease neuroimaging initiative (ADNI): MRI methods},
  author={Jack Jr, Clifford R and Bernstein, Matt A and Fox, Nick C and Thompson, Paul and Alexander, Gene and Harvey, Danielle and Borowski, Bret and Britson, Paula J and L. Whitwell, Jennifer and Ward, Chadwick and others},
  journal={Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={27},
  number={4},
  pages={685--691},
  year={2008},
  publisher={Wiley Online Library}
}
@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Franpo\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}
@article{wen2020convolutional,
  title={Convolutional Neural Networks for Classification of Alzheimer's Disease: Overview and Reproducible Evaluation},
  author={Wen, Junhao and Thibeau-Sutre, Elina and Diaz-Melo, Mauricio and Samper-Gonz{\'a}lez, Jorge and Routier, Alexandre and Bottani, Simona and Dormont, Didier and Durrleman, Stanley and Burgos, Ninon and Colliot, Olivier and others},
  journal={Medical Image Analysis},
  pages={101694},
  year={2020},
  publisher={Elsevier}
}
@article{tijms2020pathophysiological,
  title={Pathophysiological subtypes of Alzheimer's disease based on cerebrospinal fluid proteomics.},
  author={Tijms, Betty M and Gobom, Johan and Reus, Lianne and Jansen, Iris and Hong, Shengjun and Dobricic, Valerija and Kilpert, Fabian and ten Kate, Mara and Barkhof, Frederik and Tsolaki, Magda and others},
  journal={medRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory Press}
}
@article{poulakis2018heterogeneous,
  title={Heterogeneous patterns of brain atrophy in Alzheimer's disease},
  author={Poulakis, Konstantinos and Pereira, Joana B and Mecocci, Patrizia and Vellas, Bruno and Tsolaki, Magda and K{\l}oszewska, Iwona and Soininen, Hilkka and Lovestone, Simon and Simmons, Andrew and Wahlund, Lars-Olof and others},
  journal={Neurobiology of aging},
  volume={65},
  pages={98--108},
  year={2018},
  publisher={Elsevier}
}
@article{min2018survey,
  title={A survey of clustering with deep learning: From the perspective of network architecture},
  author={Min, Erxue and Guo, Xifeng and Liu, Qiang and Zhang, Gen and Cui, Jianjing and Long, Jun},
  journal={IEEE Access},
  volume={6},
  pages={39501--39514},
  year={2018},
  publisher={IEEE}
}
@inproceedings{hofer2017deep,
  title={{Deep Learning with Topological Signatures}},
  author={Hofer, Christoph and Kwitt, Roland and Niethammer, Marc and Uhl, Andreas},
  booktitle={Proceedings of the 30th Conference on Advances in Neural Information Processing Systems},
  pages={1634--1644},
  year={2017}
}
@article{scarmeas2004cognitive,
  title={Cognitive reserve: implications for diagnosis and prevention of Alzheimer’s disease},
  author={Scarmeas, Nikolaos and Stern, Yaakov},
  journal={Current neurology and neuroscience reports},
  volume={4},
  number={5},
  pages={374--380},
  year={2004},
  publisher={Springer}
}
@article{van2017neuroimaging,
  title={A neuroimaging approach to capture cognitive reserve: application to Alzheimer's disease},
  author={van Loenhoud, Anna C and Wink, Alle Meije and Groot, Colin and Verfaillie, Sander CJ and Twisk, Jos and Barkhof, Frederik and van Berckel, Bart and Scheltens, Philip and van der Flier, Wiesje M and Ossenkoppele, Rik},
  journal={Human brain mapping},
  volume={38},
  number={9},
  pages={4703--4715},
  year={2017},
  publisher={Wiley Online Library}
}
@article{gauthier2006mild,
  title={Mild cognitive impairment},
  author={Gauthier, Serge and Reisberg, Barry and Zaudig, Michael and Petersen, Ronald C and Ritchie, Karen and Broich, Karl and Belleville, Sylvie and Brodaty, Henry and Bennett, David and Chertkow, Howard and others},
  journal={The lancet},
  volume={367},
  number={9518},
  pages={1262--1270},
  year={2006},
  publisher={Elsevier}
}
@article{bruningk2020image,
  title={Image analysis for Alzheimer's disease prediction: Embracing pathological hallmarks for model architecture design},
  author={Br{\"u}ningk, Sarah C and Hensel, Felix and Jutzeler, Catherine R and Rieck, Bastian},
  journal={arXiv preprint arXiv:2011.06531},
  year={2020}
}
@article{liu2018anatomical,
  title={Anatomical landmark based deep feature representation for MR images in brain disease diagnosis},
  author={Liu, Mingxia and Zhang, Jun and Nie, Dong and Yap, Pew-Thian and Shen, Dinggang},
  journal={IEEE journal of biomedical and health informatics},
  volume={22},
  number={5},
  pages={1476--1485},
  year={2018},
  publisher={IEEE}
}
@article{mrabah2019deep,
  title={Deep clustering with a dynamic autoencoder},
  author={Mrabah, Nairouz and Khan, Naimul Mefraz and Ksantini, Riadh and Lachiri, Z},
  journal={CoRR},
  year={2019}
}
@article{collins19943d,
  title={3D Model-based segmentation of individual brain structures from magnetic resonance imaging data},
  author={Collins, D Louis},
  year={1994},
  publisher={McGill University}
}
@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year=1947,
  publisher={JSTOR}
}
@article{halliday1998regional,
  title={Regional specificity of brain atrophy in Huntington's disease},
  author={Halliday, GM and McRitchie, DA and Macdonald, V and Double, KL and Trent, RJ and McCusker, E},
  journal={Experimental neurology},
  volume={154},
  number={2},
  pages={663--672},
  year={1998},
  publisher={Elsevier}
}
@article{kuhl1982cerebral,
  title={Cerebral metabolism and atrophy in Huntington's disease determined by 18FDG and computed tomographic scan},
  author={Kuhl, David E and Phelps, Michael E and Markham, Charles H and Metter, E Jeffrey and Riege, Walter H and Winter, James},
  journal={Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society},
  volume={12},
  number={5},
  pages={425--434},
  year={1982},
  publisher={Wiley Online Library}
}
@article{kassubek2004topography,
  title={Topography of cerebral atrophy in early Huntington’s disease: a voxel based morphometric MRI study},
  author={Kassubek, J and Juengling, FD and Kioschies, T and Henkel, K and Karitzky, J and Kramer, B and Ecker, D and Andrich, J and Saft, C and Kraus, P and others},
  journal={Journal of Neurology, Neurosurgery \& Psychiatry},
  volume={75},
  number={2},
  pages={213--220},
  year={2004},
  publisher={BMJ Publishing Group Ltd}
}
@article{pini2016brain,
  title={Brain atrophy in Alzheimer’s disease and aging},
  author={Pini, Lorenzo and Pievani, Michela and Bocchetta, Martina and Altomare, Daniele and Bosco, Paolo and Cavedo, Enrica and Galluzzi, Samantha and Marizzoni, Moira and Frisoni, Giovanni B},
  journal={Ageing research reviews},
  volume={30},
  pages={25--48},
  year={2016},
  publisher={Elsevier}
}
@article{skraba2020wasserstein,
  title={Wasserstein Stability for Persistence Diagrams},
  author={Skraba, Primoz and Turner, Katharine},
  journal={arXiv preprint arXiv:2006.16824},
  year={2020}
}
@article{cohen2007stability,
  title={Stability of persistence diagrams},
  author={Cohen-Steiner, David and Edelsbrunner, Herbert and Harer, John},
  journal={Discrete \& computational geometry},
  volume={37},
  number={1},
  pages={103--120},
  year={2007},
  publisher={Springer}
}
@article{shapiro1965analysis,
  title={An analysis of variance test for normality (complete samples)},
  author={Shapiro, Samuel Sanford and Wilk, Martin B},
  journal={Biometrika},
  volume={52},
  number={3/4},
  pages={591--611},
  year={1965},
  publisher={JSTOR}
}
@article{vanherpe2016framework,
  title={Framework for efficient synthesis of spatially embedded morphologies},
  author={Vanherpe, Liesbeth and Kanari, Lida and Atenekeng, Guy and Palacios, Juan and Shillcock, Julian},
  journal={Physical Review E},
  volume=94,
  number=2,
  pages=023315,
  year=2016,
  publisher={APS}
}
#+end_src

topology & geometrical analysis links
#+begin_src bibtex
@article{huguet2022time,
  title={Time-inhomogeneous diffusion geometry and topology},
  author={Huguet, Guillaume and Tong, Alexander and Rieck, Bastian and Huang, Jessie and Kuchroo, Manik and Hirn, Matthew and Wolf, Guy and Krishnaswamy, Smita},
  journal={arXiv preprint arXiv:2203.14860},
  year={2022}
}
#+end_src

original paper from vietoris on VR complex filtrations
#+begin_src bibtex
@article{vietoris1927hoheren,
  title={{{\"U}ber den h{\"o}heren Zusammenhang kompakter R{\"a}ume und eine Klasse von zusammenhangstreuen Abbildungen}},
  author={Vietoris, Leopold},
  journal={Mathematische Annalen},
  volume={97},
  number={1},
  pages={454--472},
  year={1927},
  publisher={Springer}
}
#+end_src

Runtime complexity of VR filtration
#+begin_src bibtex
@article{adams2018persistent,
  title={{The Persistent Homology of Cyclic Graphs}},
  author={Adams, Henry and Coldren, Ethan and Willmot, Sean},
  journal={arXiv preprint arXiv:1812.03374},
  year={2018}
}
#+end_src

de novo protein design, tim's paper
#+begin_src bibtex
@article{kucera2022conditional,
  title={Conditional generative modeling for de novo protein design with hierarchical functions},
  author={Kucera, Tim and Togninalli, Matteo and Meng-Papaxanthos, Laetitia},
  journal={Bioinformatics},
  volume={38},
  number={13},
  pages={3454--3461},
  year={2022},
  publisher={Oxford University Press}
}

#+end_src

Sequence kernels best for proteins
#+begin_src bibtex
@inproceedings{leslie2002spectrum,
  title={{The Spectrum Kernel: A String Kernel for SVM Protein Classification}},
  author={Leslie, C and Eskin, E and Noble, WS},
  booktitle={Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
  pages={564--575},
  year={2002}
}

#+end_src

Ripser VR optimizations
#+begin_src bibtex
@article{Bauer2021Ripser,
	Author = {Ulrich Bauer},
	Title = {{Ripser: efficient computation of Vietoris-Rips persistence barcodes}},
	Journal = {Journal of Applied and Computational Topology},
	Year = {2021},
	Publisher = {Springer},
	DOI = {10.1007/s41468-021-00071-5}
}
#+end_src

original paper by goodfellow on GANs
#+begin_src bibtex
@inproceedings{goodfellow2014generative,
  title={{Generative Adversarial Nets}},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Proceedings of the 27th Conference on Advances in Neural Information Processing Systems},
  year={2014},
}
#+end_src

original VAE paper

#+begin_src bibtex
@inproceedings{kingma2013auto,
  title={{Auto-Encoding Variational Bayes}},
  author={Kingma, Diederik P and Welling, Max},
	booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2013},
}
#+end_src

SOTA GAN
#+begin_src bibtex
@article{sauer2022stylegan,
  title={{StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets}},
  author={Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
  journal={arXiv preprint arXiv:2202.00273},
  year={2022}
}
#+end_src

Review of techniques used to do generative modelling
#+begin_src bibtex
@article{bond2021deep,
  title={{Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models}},
  author={Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris George},
  journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  year={2021},
  publisher={IEEE Computer Society}
}

#+end_src

Inception v3 (on arxiv) paper used as inputs to GANs. Network primarily used for
object detection

#+begin_src bibtex
@inproceedings{szegedy2016rethinking,
  title={{Rethinking the Inception Architecture for Computer Vision}},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2016}
}

#+end_src

Graph embedding method review

#+begin_src bibtex
@article{xu2021understanding,
  title={{Understanding Graph Embedding Methods and Their Applications}},
  author={Xu, Mengjia},
  journal={SIAM Review},
  volume={63},
  number={4},
  pages={825--853},
  year={2021},
  publisher={SIAM}
}
#+end_src

#+begin_src bibtex
@article{preuer2018frechet,
  title={{Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery}},
  author={Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={Journal of Chemical Information and Modeling},
  volume={58},
  number={9},
  pages={1736--1741},
  year={2018},
  publisher={ACS Publications}
}
#+end_src


GNNs are able to extract meaningful representations without any training:
#+begin_src bibtex

@inproceedings{kipf2016semi,
  title={{Semi-Supervised Classification with Graph Convolutional Networks}},
  author={Kipf, Thomas N and Welling, Max},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year={2016}
}

@inproceedings{morris2019weisfeiler,
  title={{Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks}},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={Proceedings of the 33rd AAAI Conference on Artificial Intelligence},
  pages={4602--4609},
  year={2019}
}



@inproceedings{xu2018powerful,
  title={{How Powerful are Graph Neural Networks?}},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  booktitle={Proceedings of the  International Conference on Learning Representations},
  year={2018}
}
#+end_src

Empirical studies of graph neural networks are keys xu2018empirical and o2021evaluation

Original Weisfeiler-Lehman paper.

#+begin_src bibtex
@article{shervashidze2011weisfeiler,
  title={{Weisfeiler-Lehman Graph Kernels}},
  author={Shervashidze, Nino and Schweitzer, Pascal and van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2539--2561},
  year={2011}
}
#+end_src

Research questions:
+ How does MMD behave proteome-wide w/o perturbations?
  * Proteome wide resampling w/o pertubations for e-coli + human
  * WL kernel
  * TDA
  * ESM
  * Subset for specific proteins (TM proteins)
  * RQ: What is the MMD between two random sets of proteins using the Weisfeiler-Lehman kernel on the 8Angrstrom-graph extracted from the human and E. Coli proteome?
+ Graph kernel performance:
  * Vary epsilon
  * degree histogram additional
  * add variance from data in runs
+ TDA performance:
  * Add data variance
  * Add comparison with other kernels
  * Separate experiment with hyperparameters
  * Compare to the biological descriptors
+ ESM:
  * Mutation
  * Other kernels like WL kernel

  Directory structure

  /data/systematic/representations/organism/perturbation/twist/amount/*.pkl (protein objects)

  representations = constant/
  representations = variable/


CATH citation (protein database superfamilies)
#+begin_src bibtex
@article{orengo1997cath,
  title={{CATH--a hierarchic classification of protein domain structures}},
  author={Orengo, Christine A and Michie, Alex D and Jones, Susan and Jones, David T and Swindells, Mark B and Thornton, Janet M},
  journal={Structure},
  volume={5},
  number={8},
  pages={1093--1109},
  year={1997},
  publisher={Elsevier}
}
#+end_src

SCOP citation (structural features)
#+begin_src bibtex
@article{murzin1995scop,
  title={{SCOP: a structural classification of proteins database for the investigation of sequences and structures}},
  author={Murzin, Alexey G and Brenner, Steven E and Hubbard, Tim and Chothia, Cyrus},
  journal={Journal of molecular biology},
  volume={247},
  number={4},
  pages={536--540},
  year={1995},
  publisher={Elsevier}
}
#+end_src

Crystals and atom-type filtrations
#+begin_src bibtex
@article{jiang2021topological,
  title={{Topological representations of crystalline compounds for the machine-learning prediction of materials properties}},
  author={Jiang, Yi and Chen, Dong and Chen, Xin and Tangyi, Li and Guo-Wei, Wei and Pan, Feng},
  journal={NPJ Computational Materials},
  volume={7},
  number={1},
  year={2021},
  publisher={Nature Publishing Group}
}

#+end_src

Discusses failure modes (collapse, drop).
#+begin_src bibtex
@article{salimans2016improved,
  title={{Improved Techniques for Training GANs}},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Proceedings of the 29th Conference on Advances in Neural Information Processing Systems},
  year={2016}
}
#+end_src

Wasserstein-GANs
#+begin_src bibtex
@inproceedings{arjovsky2017wasserstein,
  title={{Wasserstein GAN}},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2017},
}
#+end_src

#+begin_src bibtex
@article{lopez2020enhancing,
  title={Enhancing scientific discoveries in molecular biology with deep generative models},
  author={Lopez, Romain and Gayoso, Adam and Yosef, Nir},
  journal={Molecular Systems Biology},
  volume={16},
  number={9},
  pages={e9198},
  year={2020}
}
#+end_src

If x represents an amino acid sequence, then the probability that x occurs in nature can be better estimated by a generative model that accounts for interactions between residues
#+begin_src bibtex
@article{riesselman2018deep,
  title={Deep generative models of genetic variation capture the effects of mutations},
  author={Riesselman, Adam J and Ingraham, John B and Marks, Debora S},
  journal={Nature Methods},
  volume={15},
  number={10},
  pages={816--822},
  year={2018},
  publisher={Nature Publishing Group}
}
#+end_src

reviews of deep generative modelling for protein design

#+begin_src bibtex
@article{strokach2022deep,
  title={{Deep generative modeling for protein design}},
  author={Strokach, Alexey and Kim, Philip M},
  journal={Current Opinion in Structural Biology},
  volume={72},
  pages={226--236},
  year={2022},
  publisher={Elsevier}
}
#+end_src

#+begin_src bibtex
@article{jendrusch2021alphadesign,
  title={{AlphaDesign: A de novo protein design framework based on AlphaFold}},
  author={Jendrusch, Michael and Korbel, Jan O and Sadiq, S Kashif},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}
#+end_src

#+begin_src bibtex
@article{strokach2020fast,
  title={{Fast and Flexible Protein Design Using Deep Graph Neural Networks}},
  author={Strokach, Alexey and Becerra, David and Corbi-Verge, Carles and Perez-Riba, Albert and Kim, Philip M},
  journal={Cell Systems},
  volume={11},
  number={4},
  pages={402--411},
  year={2020},
  publisher={Elsevier}
}
#+end_src

#+begin_src bibtex
@article{madani2021deep,
  title={Deep neural language modeling enables functional protein generation across families},
  author={Madani, Ali and Krause, Ben and Greene, Eric R and Subramanian, Subu and Mohr, Benjamin P and Holton, James M and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z and Socher, Richard and others},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}
#+end_src

#+begin_src bibtex
@inproceedings{meier2021language,
  title={Language models enable zero-shot prediction of the effects of mutations on protein function},
  author={Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex},
  booktitle={Proceedings of the 34th Conference on Advances in Neural Information Processing Systems},
  year={2021}
}
#+end_src

Low-N
#+begin_src bibtex
@article{biswas2021low,
  title={{Low-$N$ protein engineering with data-efficient deep learning}},
  author={Biswas, Surojit and Khimulya, Grigory and Alley, Ethan C and Esvelt, Kevin M and Church, George M},
  journal={Nature Methods},
  volume={18},
  number={4},
  pages={389--396},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

#+begin_src bibtex
@article{madani2020progen,
  title={{ProGen: Language Modeling for Protein Generation}},
  author={Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R and Huang, Po-Ssu and Socher, Richard},
  journal={arXiv preprint arXiv:2004.03497},
  year={2020}
}
#+end_src

#+begin_src bibtex
@article{maddhuri2021protein,
  title={Protein contact map refinement for improving structure prediction using generative adversarial networks},
  author={Maddhuri, Venkata Subramaniya and Sai, Raghavendra and Terashi, Genki and Jain, Aashish and Kagaya, Yuki and Kihara, Daisuke},
  journal={Bioinformatics},
  volume={37},
  number={19},
  pages={3168--3174},
  year={2021},
  publisher={Oxford University Press}
}
#+end_src

#+begin_src bibtex
@inproceedings{ingraham2019generative,
  title={{Generative Models for Graph-Based Protein Design}},
  author={Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
  journal={Proceedings of the 32nd Conference on Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{anand2018generative,
  title={Generative modeling for protein structures},
  author={Anand, Namrata and Huang, Possu},
  journal={Proceedings of the 31st Conference on Advances in neural information processing systems},
  year={2018}
}

#+end_src

#+begin_src bibtex
@article{brown2020language,
  title={{Large Language Models are Zero-Shot Reasoners}},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}
#+end_src

#+begin_src bibtex
@article{ramesh2022hierarchical,
  title={{Hierarchical Text-Conditional Image Generation with CLIP Latents}},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:1803.03324},
  year={2022}
}
@article{saharia2022hierarchical,
  title={{Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding}},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}
#+end_src

#+begin_src bibtex
@article{guo2020systematic,
  author    = {Xiaojie Guo and Liang Zhao},
  title     = {{A Systematic Survey on Deep Generative Models for Graph Generation}},
  journal   = {arXiv preprint arXiv:2007.06686},
  year      = {2020},
}
#+end_src

#+begin_src bibtex
@inproceedings{weinstein2021structured,
  title={A structured observation distribution for generative biological sequence prediction and forecasting},
  author={Weinstein, Eli N and Marks, Debora},
  booktitle={International Conference on Machine Learning},
  year={2021},
}
#+end_src

#+begin_src bibtex
@article{wang2021directed,
  title={Directed Evolution: Methodologies and Applications},
  author={Wang, Yajie and Xue, Pu and Cao, Mingfeng and Yu, Tianhao and Lane, Stephan T and Zhao, Huimin},
  journal={Chemical Reviews},
  volume={121},
  number={20},
  pages={12384--12444},
  year={2021},
  publisher={ACS Publications}
}
#+end_src

#+begin_src bibtex
@article{repecka2021expanding,
  title={Expanding functional protein sequence spaces using generative adversarial networks},
  author={Repecka, Donatas and Jauniskis, Vykintas and Karpus, Laurynas and Rembeza, Elzbieta and Rokaitis, Irmantas and Zrimec, Jan and Poviloniene, Simona and Laurynenas, Audrius and Viknander, Sandra and Abuajwa, Wissam and others},
  journal={Nature Machine Intelligence},
  volume={3},
  number={4},
  pages={324--333},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

This + AlphaFold paper shows importance of
#+begin_src bibtex
@article{hornak2006comparison,
  title={{Comparison of multiple Amber force fields and development of improved protein backbone parameters}},
  author={Hornak, Viktor and Abel, Robert and Okur, Asim and Strockbine, Bentley and Roitberg, Adrian and Simmerling, Carlos},
  journal={Proteins: Structure, Function, and Bioinformatics},
  volume={65},
  number={3},
  pages={712--725},
  year={2006},
  publisher={Wiley Online Library}
}
#+end_src


#+begin_src bibtex
@inproceedings{binkowksi2018demystifying,
  title={{Demystifying MMD GANs}},
  author={Bi{\'n}kowski, Miko{\l}aj and Sutherland, Danica J and Arbel, Michael and Gretton, Arthur},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2018},
}
#+end_src

#+begin_src bibtex
@article{betzalel2022study,
  title={{A Study on the Evaluation of Generative Models}},
  author={Betzalel, Eyal and Penso, Coby and Navon, Aviv and Fetaya, Ethan},
  journal={arXiv preprint arXiv:2206.10935},
  year={2022}
}
#+end_src


#+begin_src bibtex
@article{wood2005protein,
  title={{Protein Secondary Structure Prediction with Dihedral Angles}},
  author={Wood, Matthew J and Hirst, Jonathan D},
  journal={PROTEINS: Structure, Function, and Bioinformatics},
  volume={59},
  number={3},
  pages={476--481},
  year={2005},
  publisher={Wiley Online Library}
}
#+end_src

#+begin_src bibtex
@article{o20102,
  title={{2.4 The Functions of Proteins Are Determined by Their Three-Dimensional Structures, in The Essentials of Cell Biology}},
  author={O'Connor, CM and Adams, JU},
  journal={Cambridge, MA: NPG Education},
  year={2010}
}
#+end_src

intractable likelihood functions
#+begin_src bibtex
@article{yildirim2015parameter,
  title={{Parameter Estimation in Hidden Markov Models with Intractable Likelihoods Using Sequential Monte Carlo}},
  author={Y{\i}ld{\i}r{\i}m, Sinan and Singh, Sumeetpal S and Dean, Thomas and Jasra, Ajay},
  journal={Journal of Computational and Graphical Statistics},
  volume={24},
  number={3},
  pages={846--865},
  year={2015},
  publisher={Taylor \& Francis}
}
@article{drovandi2011likelihood,
  title={{Likelihood-free Bayesian estimation of multivariate quantile distributions}},
  author={Drovandi, Christopher C and Pettitt, Anthony N},
  journal={Computational Statistics \& Data Analysis},
  volume={55},
  number={9},
  pages={2541--2556},
  year={2011},
  publisher={Elsevier}
}
@article{rayner2002numerical,
  title={Numerical maximum likelihood estimation for the g-and-k and generalized g-and-h distributions},
  author={Rayner, Glen D and MacGillivray, Helen L},
  journal={Statistics and Computing},
  volume={12},
  number={1},
  pages={57--75},
  year={2002},
  publisher={Springer}
}
#+end_src

HMMs
#+begin_src bibtex
@article{baum1968growth,
  title={Growth transformations for functions on manifolds},
  author={Baum, Leonard E and Sell, George},
  journal={Pacific Journal of Mathematics},
  volume={27},
  number={2},
  pages={211--227},
  year={1968},
  publisher={Mathematical Sciences Publishers}
}
@article{baum1970maximization,
  title={{A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains}},
  author={Baum, Leonard E and Petrie, Ted and Soules, George and Weiss, Norman},
  journal={The annals of mathematical statistics},
  volume={41},
  number={1},
  pages={164--171},
  year={1970},
  publisher={JSTOR}
}

#+end_src
** Notes for thesis
*** Discuss mode collapse and mode dropping
*** Discuss descriptors with bin_ranges not able to capture extreme cases
*** We did not consider more advance mutation types (transpositions, deletions, additions, or reverse)
*** meta-metrics: take into account standard deviations
*** DONE epsilon value relationship to sensitivity of metric
*** de-novo design, promote structural heterogeneity, different than point mutation because this can be done in a lab
*** Discuss mode collapse and mode dropping
*** Discuss descriptors with bin_ranges not able to capture extreme cases
*** We did not consider more advance mutation types (transpositions, deletions, additions, or reverse)
*** meta-metrics: take into account standard deviations
*** are graphs good reps? what's the best graph rep?
*** epsilon value relationship to sensitivity of metric
*** de-novo design, promote structural heterogeneity, different than point mutation because this can be done in a lab
*** model space of possible proteins/feasible proteins, evolution has only explored a subset, because of incremenets
*** find "jumps" in design landscape
*** find new variants of proteins
*** key is conditional generative models
*** generative models are only way to get better proteins, because discriminative models are constrained to range of values it is trained on.
*** out-of-distribution design of samples.
*** are graphs good reps for proteins? what are the best params to use?
*** AlphaFold reps have error rates that are approx. to the actual experimental data (different errors).
*** artificial monomeric proteins in real data
*** Make list of statements based on data analysis and make plots/tables accordingly
*** Metric more or less relevant for modelling stage = coarse and picks up extreme cases in the beginning and more refined later on.


** Cluster essentials
scontrol show jobid -dd [id]
squeue --start -p cpu

** References
#+LaTeX: \printbibliography[heading=none]
