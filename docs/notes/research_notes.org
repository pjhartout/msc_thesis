#+BIND: org-export-use-babel nil
#+TITLE: Research notes on metrics for GNNs applied to biological problems
#+AUTHOR: Philip Hartout
#+EMAIL: <philip.hartout@protonmail.com>
#+DATE: Monday January 24, 2022
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:[a4paper,12pt,twoside]
#+LaTeX_HEADER:\usepackage[usenames,dvipsnames,figures]{xcolor}
#+LaTeX_HEADER:\usepackage[autostyle]{csquotes}
#+LaTeX_HEADER:\usepackage[final]{pdfpages}
#+LaTeX_HEADER:\usepackage{amsfonts, amssymb}            % Math symbols
#+LaTeX_HEADER:\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
#+LATEX_HEADER_EXTRA:\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
#+LATEX_HEADER_EXTRA:\newtheorem{definition}{Definition}[section]
#+MACRO: NEWLINE @@latex:\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session python_emacs_session :cache :results value
#+PROPERTY: header-args :exports none :tangle "~/Documents/Git/msc_thesis/thesis/refs.bib"
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+LATEX_COMPILER: pdflatexorg-mode restarted
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
#+LATEX_HEADER: \nocite{*}
#+OPTIONS: <:nil c:nil todo:nil H:5
#+EXCLUDE_TAGS: noexport
* Fundamental concepts
** Graph Laplacian
   Given Adjacency matrix $A$ of dimension $n\times n$ and degree matrix $D$ of
   a given graph $G$, the graph Laplacian $L$ of $G$ is given by:
   $L=D-A$.
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iL^i$. $p_w(L)$ is $n\times n$.
** ChebNet
   $p_w(L)=w_0I_n+w_1L+w_2L+\dots+w_dL^d=\sum_{i=0}^{d}w_iT_iL^i$
   where $T_i$ is the degree-i Chebyshev polynomial of the first kind and
   $\widetilde{L}$ is the normalized Laplacian derived using the largest
   eigenvalue of $L$.

   $L$ is p.s.d., all eigenvalues of $L$ are $>0$. If $\lambda_{\max}(L)>1$,
   then $L$'s entries increase. $\widetilde{L}$ normalizes the eigenvalues of $L$
   and bounds them in $[-1,1]$. $\widetilde{L}$ is defined as
\begin{equation*}
\widetilde{L} = {2L\over\lambda_{\max}(L) - I_n}
\end{equation*}
Embedding computation
1. $h^{0}=x$
2. For $k=1,2,\dots,K$:
   1. $p^{(k)}=p_{w^{(k)}}(L)$
   2. $g^{(k)}=p^{(k)}\times h^{k-1}$
   3. $h^{(k)} = \sigma(g^{(k)})$
where $\sigma$ is some non-linearity.

** Modern GNNs
   When going back to $p_w(L)=L$, focussing on one verted, we have:
   \begin{align*}
(Lx)_v &= L_v x\\
&= \sum_{u\in G}L_{vu}x_u\\
&= \sum_{u\in G}(D_{vu}-A_{vu})x_u\\
&= D_vx_v-\sum_{u\in\mathcal{N}(v)}x_u
   \end{align*}
   This is a 1-hop localized convolution. This does two steps:
   + Aggregating over immediate neighbour features $x_u$.
   + Combining with the node's own feature $x_v$
   Ensuring that the agg. is node order equivariant, the overall convolution
   becomes node-order equivariant. This can be considered as messsage passing
   between adjacent nodes. After each step, some nodes receive information from
   their neighbour.

   By iteratively repeating the 1-hop localized convolutions $K$ times (i.e.
   repeatedly passing messages), the receptive field of the conv. effectively
   includes all nodes up to $K$ hops away.

   Some modern aggregation and combination functions:
   + graph convolutional networks (GCN)
   + graph attention networks (GAT)
   + graph sample and aggregate (GraphSAGE)
   + graph isomorphism network (GIN)
** Global convolutions
*** Spectral convolutions
    Smoother graphs have a quantity $R_(x)=\sum_{(i,j)\in E}(x_i-x_j)^2$ that
    reflects that feature vectors $x$ that assign similar values to adjacent
    nodes in $G$ would have smaller values of $R_L(x)$. $L$ is a real, symmetric
    matrix which has eigenvalues $\lambda_1\leq\dots\leq\lambda_n$. Eigenvectors
    can be taken to be orthonormal.

    The set of eigenvalues of $L$ are successively less smooth. They are called the spectrum of $L$.

    The spectral decomp. of $L$ as $L=U\Lambda U^T$ where $\Lambda$ is the
    diagonal matrix of sorted eigenvalues, and $U$ denotes the matrix of the
    eigenvectors. sorted by increasing eigenvalues. Orthonormality between
    eigenvectors gives us $U^T U = I$. Since each $u\in\mathbb{R}^n$, any $x$
    can be represented as a linear combination of these eigenvectors, i.e.:
    \begin{equation*}
    x = \sum_{i=1}^{n}\widetilde{x_i}u_i=U\widetilde{x}.
    \end{equation*}
    where $\widetilde{x}$ are the coefficients. Again, the orthonormality of the eigenvalues allows us to state $x=U\widetilde{x} \iff U^Tx=\widetilde{x}$.
    We can then compute global convolutions, defining:
    \begin{equation*}
h^{(k)} =
\begin{bmatrix}
h^{(k)}_1\\
\vdots\\
h^{(k)}_n\\
\end{bmatrix}
    \end{equation*}
    We start with the original features $h^{(0)}=x$, then
    1. $\hat{h}^{(k-1)} = U_m^Th^{(k-1)}$
    2. $\hat{g}^{(k)}=\hat{w}^{(k)}\odot\hat{h}^{(k-1)}$
    3. $h^{(k)}=\sigma(g^{(k)})$

* Graph Neural Networks
** Reviews
*** Graph neural networks:
    A review of methods and applications
#+begin_src bibtex
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
#+end_src
Zhou et al mention several GNN approaches in use today
Generative models popular today:
**** Sequential graph generation process
 + GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
 #+begin_src bibtex
@inproceedings{you2018graphrnn,
  title={{GraphRNN: Generating realistic graphs with deep auto-regressive models}},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  booktitle={International conference on machine learning},
  pages={5708--5717},
  year={2018},
  organization={PMLR}
}

 #+end_src
 + Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
 #+begin_src bibtex
@article{li2018learning,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

 #+end_src
 + GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
 #+begin_src bibtex
@article{shi2020graphaf,
  title={{GraphAF: a flow-based autoregressive model for molecular graph generation}},
  author={Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  journal={arXiv preprint arXiv:2001.09382},
  year={2020}
}
 #+end_src
**** Non-sequential graph generation process
 + MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
#+begin_src bibtex
@article{de2018molgan,
  title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}
#+end_src
 + Ma et al 2018 - constrained VAE for semantic validity of generated graph
   #+begin_src bibtex
@article{ma2018constrained,
  title={Constrained generation of semantically valid graphs via regularizing variational autoencoders},
  author={Ma, Tengfei and Chen, Jie and Xiao, Cao},
  journal={arXiv preprint arXiv:1809.02630},
  year={2018}
}

   #+end_src
 + GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules Example work showing EMD kernel:
   #+begin_src bibtex
@article{you2018graph,
  title={Graph convolutional policy network for goal-directed molecular graph generation},
  author={You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1806.02473},
  year={2018}
}
   #+end_src
 + Graph Normalizing Flows
   #+begin_src bibtex
@article{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={arXiv preprint arXiv:1905.13177},
  year={2019}
}
   #+end_src
This one has a fairly comprehensive website: https://sites.google.com/view/graph-normalizing-flows/
Full architecture

#+NAME: fig:Full architecture of the graph noramlizing flow DNN
#+CAPTION: figure name
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200
[[./images/full_arch_gnf.png]]
 + Graphite isotropic gaussian for VAE + iterative refinement for decoding
   #+begin_src bibtex
@inproceedings{grover2019graphite,
  title={Graphite: Iterative generative modeling of graphs},
  author={Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2434--2444},
  year={2019},
  organization={PMLR}
}
   #+end_src

** Three most popular according to O'Bray 2021:
   #+begin_src bibtex
@article{o2021evaluation,
  title={Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions},
  author={O'Bray, Leslie and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
  journal={arXiv preprint arXiv:2106.01098},
  year={2021}
}
   #+end_src
+ GraphRNN, GRAN, Graph Score Matching.

+ Graph Recurrent Attention Networks, also uses graph spectra for MMD. GRAN.
  #+begin_src bibtex
@article{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S},
  journal={arXiv preprint arXiv:1910.00760},
  year={2019}
}
  #+end_src

  #+begin_quote
  In previous work, You et al. [37] computed degree distributions, clustering
  coefficient distributions, and the number of occurrence of all orbits with 4
  nodes, and then used the maximum mean discrepancy (MMD) over these graph
  statistics, relying on Gaussian kernels with the first Wasserstein distance,
  i.e., earth mover’s distance (EMD), in the MMD.In practice, we found computing
  this MMD with the Gaussian EMD kernel to be very slow for moderately large
  graphs. Therefore, we use the total variation (TV) distance, which greatly
  speeds up the evaluation and is still consistent with EMD. In addition to the
  node degree, clustering coefficient and orbit counts (used by [36]), we also
  compare the spectra of the graphs by computing the eigenvalues of the
  normalized graph Laplacian (quantized to approximate a probability density).
  This spectral comparison provides a view of the global graph properties,
  whereas the previous metrics focus on local graph statistics.
  #+end_quote

+ Graph Score Matching
  #+begin_src bibtex
@inproceedings{niu2020permutation,
  title={Permutation invariant graph generation via score-based generative modeling},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}
  #+end_src

  On MMD, they say the following:
#+NAME: fig:MMD settings for evaluation of the graph score matching model
#+CAPTION: MMD optimization strategy
#+ATTR_ORG: :width 1000
#+ATTR_LATEX: :width \linewidth
#+ATTR_HTML: :width 500
[[./images/MMD_settings_graph_score_matching_paper.png]]

* Generative modelling metrics
** Objective:
*** Generative graph dist close to the input graph dist
*** (pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)
** On images
*** Frechet Inception Distance
The idea here is to use deeper representational layers of an ANN and used the squared Wasserstein metric to compare two multinomial Gaussians.
Introduced 2017
#+begin_src bibtex
@article{heusel2017gans
,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
#+end_src
*** LPIPS [[https://richzhang.github.io/PerceptualSimilarity/][Project page]]
Introduced 2017
#+begin_src bibtex
@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}
#+end_src
*** Why comparing graphs is hard:
  + Metrics need to deal with spatial invariances such as cycles.
  + Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.
  + Other publications:
  #+begin_src bibtex
@article{theis2015note,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}
  #+end_src

** Desiderata for good metrics:
 1. Robust to noise
 2. Expressive, if they don't arise from the same dist, then metric should detect this.
 3. Computationally efficient.
* MMD - current accepted method to evaluate generative GNNs
+ The MMD formula goes as follows:
$\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)$
+ use it for hypothesis/two-sample testing.
+ In practice, we evaluate $d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))$ for a distribution $\mathcal{G}$. Given
  multiple distributions $G_1, G_2, \hdots$, the values of $d_{MMD}$ can be used
  to rank models, where smaller values are assumed to indicate a larger
  agreement with the original distribution $\mathcal{G}*$.
+ Commonly used kernels: first Wasserstein distance, total variation distance,
  radial basis function.
+ Commonly used descriptor functions: degree distribution histogram, clustering
  coefficient, Laplacian spectrum histogram.
+ Recommended kernels: RBF, Laplacian kernel, linear kernel (expressivity & robustness need to be analyzed)
** Potential pitfalls of descriptors
+ Degree distributions are ok seemingly
+ Clustering does not distinguish fully connected vs disconnected cliques
+ Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.
+ Parameters and descriptors are set a priori in the best case
+ Model performance is highly dependent on parameters and descriptor functions.
* Research objectives
There are multiple objectives here:
1. Find optimal kernel/hyperparameter combination based on controlled experiments on a given dataset to evaluate a good MMD configuration.
   + For this we will need https://www.alphafold.ebi.ac.uk/download, because it's clean. Also filter single chain proteins to extract graphs in the first place.
   + This can be built as a first step to get the pipeline going.

2. Show which parameters influence evaluation and how?
   + Conduct perturbation experiments on graphs

3. Find novel domain-agnostic evaluation & domain-specific evaluation metrics
   1. Domain-agnostic evaluation measures
      + Correlation with graph-edit distance
      + Correlation with perturbation
      + Topology/persistence based approaches could be useful for modelling features like binding pockets, etc?

   2. Domain-specific evaluation measures
      + Alignment
      + Energy?
** From Tim: gather literature sources. Intro structure
*** Evaluation of generative models (different domains)
*** Evaluation of generative models for graphs
**** Check how it was done before, why combo of parameters/kernels were used.
*** Evaluation of proteins (…/molecules/drugs) (What makes a valid protein?)
*** Evaluation of generative models for proteins
* Module-wise breakdown of the plan
+ Graph extraction
+ Descriptor functions
+ kernels, MMD
+ Domain agnostic
+ Domain specific
+ Other metrics
+ TDA descriptors
+ Labeled edge graph
+ NSPDK
+ Other metrics
+ Extract graph from real datasets

* Annotations :noexport:
Online approach to k-NN graph construction
#+begin_src bibtex
@article{zhao2021approximate,
  title={Approximate k-NN graph construction: a generic online approach},
  author={Zhao, Wan-Lei and Wang, Hui and Ngo, Chong-Wah},
  journal={IEEE Transactions on Multimedia},
  year={2021},
  publisher={IEEE}
}
#+end_src

epsilon nearest neighbor graphs dissertation with history and construction methods.
#+begin_src bibtex
@phdthesis{anastasiu2016algorithms,
  title={Algorithms for Constructing Exact Nearest Neighbor Graphs},
  author={Anastasiu, David C},
  year={2016},
  school={University of Minnesota}
}
#+end_src

Giotto-TDA library
#+begin_src bibtex
@article{tauzin2021giotto,
  title={giotto-tda:: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration.},
  author={Tauzin, Guillaume and Lupo, Umberto and Tunstall, Lewis and P{\'e}rez, Julian Burella and Caorsi, Matteo and Medina-Mardones, Anibal M and Dassatti, Alberto and Hess, Kathryn},
  journal={J. Mach. Learn. Res.},
  volume={22},
  pages={39--1},
  year={2021}
}
#+end_src

Heat kernel on persistence diagrams
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={A stable multi-scale kernel for topological machine learning},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4741--4748},
  year={2015}
}
#+end_src

original mmd papers
#+begin_src bibtex
@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
  publisher={JMLR. org}
}
#+end_src

Specific MMD for biological data:
#+begin_src bibtex
@article{borgwardt2006integrating,
  title={Integrating structured biological data by kernel maximum mean discrepancy},
  author={Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J and Kriegel, Hans-Peter and Sch{\"o}lkopf, Bernhard and Smola, Alex J},
  journal={Bioinformatics},
  volume={22},
  number={14},
  pages={e49--e57},
  year={2006},
  publisher={Oxford University Press}
}
#+end_src

** Additional metrics for GGNNs
Introduction several metrics based on the features extracted by an untrained
random GNN, showing more expressive metrics of GNN performance. "In this work,
we mitigate these issues by searching for scalar, domain-agnostic, and scalable
metrics for evaluating and ranking GGMs. To this end, we study existing GGM
metrics and neural-network-based metrics emerging from generative models of
images that use embeddings extracted from a task-specific network."

+ States "all metrics are frequently displayed together to approximate generation quality and evaluate GGMs" -> problematic for ranking.
+ MMD does not incorporate node/edge features, only graph structure.
+ Use image domain metrics using random and pretrained GNNs.
+ (Q1) What are the strengths and limitations of each metric?
+ (Q2) Is pretraining a GNN necessary to accurately evaluate GGMs with image domain metrics? -> No
+ (Q3) Is there a strong scalar and domain-agnostic metric for evaluating and ranking GGMs?
+ Expressivity: ability of graphs to measure diversity of generated graphs
  + They simulate expressivity by running Affinity Propagation on graphs, by simulating mode collapse and mode dropping.
  + "To simulate mode collapse, we progressively replace each datapoint with its cluster centre. The degree of perturbation $t$ represents the ratio of clusters that have been collapsed in this manner."
  + To simulate mode dropping, we progressively remove clusters from $\mathbb{S}_g$. To keep $|\mathbb{S}_g|$ constant, we randomly select samples from the remaining clusters to duplicate. In this experiment, the degree of perturbation $t$ is the ratio of clusters that have been deleted from $\mathbb{S}_g$.

+ GIN work uses ZINC dataset to show that the metric captures changes in node and edge labels.
+ Rank correlation is used (Spearman, Pearson biased for linearity)

#+begin_src bibtex
@article{thompson2022evaluation,
  title={On Evaluation Metrics for Graph Generative Models},
  author={Thompson, Rylee and Knyazev, Boris and Ghalebi, Elahe and Kim, Jungtaek and Taylor, Graham W},
  journal={arXiv preprint arXiv:2201.09871},
  year={2022}
}
#+end_src

Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
Includes node and edge features but does not handle continuous features in evaluation.
#+begin_src bibtex
@inproceedings{costa2010fast,
  title={Fast neighborhood subgraph pairwise distance kernel},
  author={Costa, Fabrizio and De Grave, Kurt},
  booktitle={ICML},
  year={2010}
}
#+end_src

Examples using the Neighborhood Subgraph Pairwise Distance graph kernel (NSPDK)
#+begin_src bibtex
@article{podda2021graphgen,
  title={GraphGen-Redux: a Fast and Lightweight Recurrent Model for labeled Graph Generation},
  author={Podda, Marco and Bacciu, Davide},
  journal={arXiv preprint arXiv:2107.08396},
  year={2021}
}
#+end_src
#+begin_src bibtex
@article{kawai2019scalable,
  title={Scalable Generative Models for Graphs with Graph Attention Mechanism},
  author={Kawai, Wataru and Mukuta, Yusuke and Harada, Tatsuya},
  journal={arXiv preprint arXiv:1906.01861},
  year={2019}
}
#+end_src
#+begin_src bibtex
@inproceedings{goyal2020graphgen,
  title={GraphGen: a scalable approach to domain-agnostic labeled graph generation},
  author={Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1253--1263},
  year={2020}
}
#+end_src

Inverse protein folding problem, maybe good to look at to see what makes a good protein.
#+begin_src bibtex
@article{ingraham2019generative,
  title={Generative models for graph-based protein design},
  author={Ingraham, John and Garg, Vikas K and Barzilay, Regina and Jaakkola, Tommi},
  year={2019}
}
#+end_src

Papers discussing Delaunay graphs (dual graph of Voronoi diagram), obtained through Delaunay triangulation. Useful for extracting hierarchical structures. Graph edges can also be added on the basis of the Delaunay triangulation. Delaunay triangles correspond to joining points that share a face in the 3D Voronoi diagram of the protein structures. For distance-based edges, a Long Interaction Network (LIN) parameter controls the minimum required separation in the amino acid sequence for edge creation. This can be useful in reducing the number of noisy edges under distance-based edge creation schemes. Edge featurisation for atom-level graphs is provided by annotations of bond type and ring status.
#+begin_src bibtex
@article{taylor2006graph,
  title={Graph theoretic properties of networks formed by the Delaunay tessellation of protein structures},
  author={Taylor, Todd J and Vaisman, Iosif I},
  journal={Physical Review E},
  volume={73},
  number={4},
  pages={041925},
  year={2006},
  publisher={APS}
}
#+end_src

Not a great review, but still useful source of references and gives overview of the field.
#+begin_src bibtex
@article{fasoulis2021graph,
  title={Graph representation learning for structural proteomics},
  author={Fasoulis, Romanos and Paliouras, Georgios and Kavraki, Lydia E},
  journal={Emerging Topics in Life Sciences},
  volume={5},
  number={6},
  pages={789--802},
  year={2021},
  publisher={Portland Press Ltd.}
}
#+end_src

Increased the number of metrics by performing MMD directly with node and edge feature distributions.
#+begin_src bibtex
@inproceedings{goyal2020graphgen,
  title={GraphGen: a scalable approach to domain-agnostic labeled graph generation},
  author={Goyal, Nikhil and Jain, Harsh Vardhan and Ranu, Sayan},
  booktitle={Proceedings of The Web Conference 2020},
  pages={1253--1263},
  year={2020}
}
#+end_src

Randomly initialized CNN to evaluate generative models
#+begin_src bibtex
@inproceedings{naeem2020reliable,
  title={Reliable fidelity and diversity metrics for generative models},
  author={Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  booktitle={International Conference on Machine Learning},
  pages={7176--7185},
  year={2020},
  organization={PMLR}
}
#+end_src

GAN work on metric disturbances:
#+begin_src bibtex
@article{xu2018empirical,
  title={An empirical study on evaluation metrics of generative adversarial networks},
  author={Xu, Qiantong and Huang, Gao and Yuan, Yang and Guo, Chuan and Sun, Yu and Wu, Felix and Weinberger, Kilian},
  journal={arXiv preprint arXiv:1806.07755},
  year={2018}
}
#+end_src


** Failure modes in molecule generation and optimization
   Blunt way to eliminate molecules:
+ penalize high logP (high bioaccumulation, highly lipophillic) .
+ GuacaMol, check also MOSES
+ Frechet Chemnet distance
+ Difficult to assess whether or not the model capture intrinsic patterns of the data or just copies it. (/copy problem/, e.g. AddCarbon model)
+ likelihood on test set should be reported
+ scores not optimizing for what practicioner wants.
+ models generate molecules numerically superior but not useful, refinement of scoring function to account for unexpected behaviour.
  + This phenomenon of optimization of a score in unexpected ways, has been observed in other applications [39]. In one illustrative setting,
    the aim was to develop a body capable of locomotion, but the optimization procedure instead discovered the simpler solution of a tall
    body falling over, which also satisfied the specified scoring function.
+ Things difficult to quantify: bioactivites, constraints "respected" by medicinal chemists
+ Set up an experiment with optimization score and control scores, to get
  insight into how a generative model optimizes the score and whether it is
  sensitive to mentioned biases (data splits) in the bioactivity model. they see
  that the optimization score increases rapidly compared to data control score.

#+begin_src bibtex
@article{renz2019failure,
  title={On failure modes in molecule generation and optimization},
  author={Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={Drug Discovery Today: Technologies},
  volume={32},
  pages={55--63},
  year={2019},
  publisher={Elsevier}
}
#+end_src

Protein embeddings for binding residues prediction for multiple ligand classes.
#+begin_src bibtex
@article{littmann2021protein,
  title={Protein embeddings and deep learning predict binding residues for various ligand classes},
  author={Littmann, Maria and Heinzinger, Michael and Dallago, Christian and Weissenow, Konstantin and Rost, Burkhard},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--15},
  year={2021},
  publisher={Nature Publishing Group}
}
#+end_src

Graph Kernels for comparative analysis of protein active sites (NADH, ATP binding site). Use random walk, shortest path and fingerprint kernel.
#+begin_src bibtex
@inproceedings{fober2009graph,
  title={Graph-kernels for the comparative analysis of protein active sites},
  author={Fober, Thomas and Mernberger, Marco and Moritz, Ralph and H{\"u}llermeier, Eyke},
  booktitle={German conference on bioinformatics 2009},
  year={2009},
  organization={Gesellschaft f{\"u}r Informatik eV}
}
#+end_src

Persistence Fisher Kernel
#+begin_src bibtex
@article{le2018persistence,
  title={Persistence fisher kernel: A riemannian manifold kernel for persistence diagrams},
  author={Le, Tam and Yamada, Makoto},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
#+end_src

Sliced Wasserstein kernel (approximates the wasserstein distance in kernel space)
#+begin_src bibtex
@inproceedings{carriere2017sliced,
  title={Sliced Wasserstein kernel for persistence diagrams},
  author={Carriere, Mathieu and Cuturi, Marco and Oudot, Steve},
  booktitle={International conference on machine learning},
  pages={664--673},
  year={2017},
  organization={PMLR}
}
#+end_src

Proof that the Wasserstein distance is not negative definite and that
transforming it to a similarity measure is not possible. (See Appendix A.)
#+begin_src bibtex
@inproceedings{reininghaus2015stable,
  title={A stable multi-scale kernel for topological machine learning},
  author={Reininghaus, Jan and Huber, Stefan and Bauer, Ulrich and Kwitt, Roland},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4741--4748},
  year={2015}
}
#+end_src

Gudhi references
#+begin_src bibtex
@book{gudhi:urm,
 title  = "{GUDHI} User and Reference Manual",
 author = "{The GUDHI Project}",
 publisher  = "{GUDHI Editorial Board}",
 year = 2015,
 url = "http://gudhi.gforge.inria.fr/doc/latest/",
}


@incollection{gudhi:PersistenceRepresentations,
 author =  "Pawel Dlotko",
 title =   "Persistence representations",
 publisher =  "{GUDHI Editorial Board}",
 booktitle =   "{GUDHI} User and Reference Manual",
 url = "http://gudhi.gforge.inria.fr/doc/latest/group___persistence__representations.html",
 year = 2017
}
#+end_src

Persistence weighted Gaussian kernel (slower than Fisher-based kernel)
#+begin_src bibtex
@inproceedings{kusano2016persistence,
  title={Persistence weighted Gaussian kernel for topological data analysis},
  author={Kusano, Genki and Hiraoka, Yasuaki and Fukumizu, Kenji},
  booktitle={International Conference on Machine Learning},
  pages={2004--2013},
  year={2016},
  organization={PMLR}
}
#+end_src


Survey of graph kernels
#+begin_src bibtex
@article{borgwardt2020graph,
  title={Graph kernels: State-of-the-art and future challenges},
  author={Borgwardt, Karsten and Ghisu, Elisabetta and Llinares-L{\'o}pez, Felipe and O'Bray, Leslie and Rieck, Bastian},
  journal={arXiv preprint arXiv:2011.03854},
  year={2020}
}
#+end_src

Kernel using Wasserstein distance
#+begin_src bibtex
@article{oh2019kernel,
    title={Kernel wasserstein distance},
    author={Oh, Jung Hun and Pouryahya, Maryam and Iyer, Aditi and Apte, Aditya P and Tannenbaum, Allen and Deasy, Joseph O},
    journal={arXiv preprint arXiv:1905.09314},
    year={2019}
  }
#+end_src


Some good work on persistent homology on proteins:
#+begin_src bibtex
@article{xia2014persistent,
  title={Persistent homology analysis of protein structure, flexibility, and folding},
  author={Xia, Kelin and Wei, Guo-Wei},
  journal={International journal for numerical methods in biomedical engineering},
  volume={30},
  number={8},
  pages={814--844},
  year={2014},
  publisher={Wiley Online Library}
}
#+end_src

Persistent homology used to analyse protein folding states
#+begin_src bibtex
@article{ichinomiya2020protein,
  title={Protein-folding analysis using features obtained by persistent homology},
  author={Ichinomiya, Takashi and Obayashi, Ippei and Hiraoka, Yasuaki},
  journal={Biophysical Journal},
  volume={118},
  number={12},
  pages={2926--2937},
  year={2020},
  publisher={Elsevier}
}
#+end_src

Reviews different types of PH and applies it to analyze DNA data.
#+begin_src bibtex
@article{meng2020weighted,
  title={Weighted persistent homology for biomolecular data analysis},
  author={Meng, Zhenyu and Anand, D Vijay and Lu, Yunpeng and Wu, Jie and Xia, Kelin},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}
#+end_src

Check criteria to be admitted to pdb
\href{https://www.wwpdb.org/validation/2017/XrayValidationReportHelp}{relative}
This is all "relative (percentile rank) compared to the rest of the database"


Good review of kernels, applications and other theoretical considerations
#+begin_src bibtex
@article{ghojogh2021reproducing,
  title={Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr$\backslash$" om Method, and Use of Kernels in Machine Learning: Tutorial and Survey},
  author={Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  journal={arXiv preprint arXiv:2106.08443},
  year={2021}
}
#+end_src

Example of ramachandran plot
#+begin_src bibtex
@article{nayak2018identification,
  title={Identification of potential inhibitors for mycobacterial uridine diphosphogalactofuranose-galactopyranose mutase enzyme: A novel drug target through in silico approach},
  author={Nayak, Tapaswini and Jena, Lingaraja and Waghmare, Pranita and Harinath, Bhaskar C and others},
  journal={International Journal of Mycobacteriology},
  volume={7},
  number={1},
  pages={61},
  year={2018},
  publisher={Medknow Publications}
}
#+end_src

* Matrix of experiments to perform
| Graph extraction | Perturbations | Descriptor   | Kernels            | Organism | Sample size |
| --               | twist         | tda          | persistence fisher | human    |         100 |
| --               | twist         | ramachandran | linear             | human    |         100 |
| --               | mutation      | esm          | linear             | human    |         100 |
| eps              | rewire        | degree       | linear             | human    |         100 |
| eps              | rewire        | degree       | gaussian           | human    |         100 |
| eps              | twist         | degree       | linear             | human    |         100 |
| eps              | twist         | degree       | gaussian           | human    |         100 |
| k-nn             | rewire        | degree       | linear             | human    |         100 |
| k-nn             | rewire        | degree       | gaussian           | human    |         100 |
| k-nn             | twist         | degree       | linear             | human    |         100 |
| k-nn             | twist         | degree       | gaussian           | human    |         100 |
| eps              | twist         | --           | w-l                | human    |         100 |
| eps              | shear         | --           | w-l                | human    |         100 |
| eps              | rewire        | --           | w-l                | human    |         100 |
| k-nn             | twist         | --           | w-l                | human    |         100 |
| k-nn             | shear         | --           | w-l                | human    |         100 |
| k-nn             | rewire        | --           | w-l                | human    |         100 |
| --               | twist         | tda          | persistence fisher | ecoli    |         100 |
| --               | twist         | ramachandran | linear             | ecoli    |         100 |
| --               | mutation      | esm          | linear             | ecoli    |         100 |
| eps              | rewire        | degree       | linear             | ecoli    |         100 |
| eps              | rewire        | degree       | gaussian           | ecoli    |         100 |
| eps              | twist         | degree       | linear             | ecoli    |         100 |
| eps              | twist         | degree       | gaussian           | ecoli    |         100 |
| k-nn             | rewire        | degree       | linear             | ecoli    |         100 |
| k-nn             | rewire        | degree       | gaussian           | ecoli    |         100 |
| k-nn             | twist         | degree       | linear             | ecoli    |         100 |
| k-nn             | twist         | degree       | gaussian           | ecoli    |         100 |
| eps              | twist         | --           | w-l                | ecoli    |         100 |
| eps              | shear         | --           | w-l                | ecoli    |         100 |
| eps              | rewire        | --           | w-l                | ecoli    |         100 |
| k-nn             | twist         | --           | w-l                | ecoli    |         100 |
| k-nn             | shear         | --           | w-l                | ecoli    |         100 |
| k-nn             | rewire        | --           | w-l                | ecoli    |         100 |


List of references from TDA internship
#+begin_src bibtex
@article{toniolo2018patterns,
  title={Patterns of cerebellar gray matter atrophy across Alzheimer’s disease progression},
  author={Toniolo, Sofia and Serra, Laura and Olivito, Giusy and Marra, Camillo and Bozzali, Marco and Cercignani, Mara},
  journal={Frontiers in Cellular Neuroscience},
  volume={12},
  pages={430},
  year={2018},
  publisher={Frontiers}
}
@article{amezquita2020shape,
  title={The shape of things to come: Topological data analysis and biology, from molecules to organisms},
  author={Am{\'e}zquita, Erik J and Quigley, Michelle Y and Ophelders, Tim and Munch, Elizabeth and Chitwood, Daniel H},
  journal={Developmental Dynamics},
  year={2020},
  publisher={Wiley Online Library}
}
@article{world2017global,
  title={Global action plan on the public health response to dementia 2017--2025},
  author={WHO},
  year={2017},
  publisher={World Health Organization}
}
@article{sleeman2019escalating,
  title={The escalating global burden of serious health-related suffering: projections to 2060 by world regions, age groups, and health conditions},
  author={Sleeman, Katherine E and de Brito, Maja and Etkind, Simon and Nkhoma, Kennedy and Guo, Ping and Higginson, Irene J and Gomes, Barbara and Harding, Richard},
  journal={The Lancet Global Health},
  volume={7},
  number={7},
  pages={e883--e892},
  year={2019},
  publisher={Elsevier}
}
@article{yiannopoulou2020current,
  title={Current and Future Treatments in Alzheimer Disease: An Update},
  author={Yiannopoulou, Konstantina G and Papageorgiou, Sokratis G},
  journal={Journal of Central Nervous System Disease},
  volume={12},
  pages={1179573520907397},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{mckhann2011diagnosis,
  title={The diagnosis of dementia due to Alzheimer's disease: recommendations from the National Institute on Aging-Alzheimer's Association workgroups on diagnostic guidelines for Alzheimer's disease},
  author={McKhann, Guy M and Knopman, David S and Chertkow, Howard and Hyman, Bradley T and Jack Jr, Clifford R and Kawas, Claudia H and Klunk, William E and Koroshetz, Walter J and Manly, Jennifer J and Mayeux, Richard and others},
  journal={Alzheimer's \& dementia},
  volume={7},
  number={3},
  pages={263--269},
  year={2011},
  publisher={Wiley Online Library}
}
@article{lehmann2016biomarkers,
  title={Biomarkers of Alzheimer’s disease: The present and the future},
  author={Lehmann, Sylvain and Teunissen, Charlotte Elisabeth},
  journal={Frontiers in Neurology},
  volume={7},
  pages={158},
  year={2016},
  publisher={Frontiers}
}
@article{smits2012early,
  title={Early onset Alzheimer's disease is associated with a distinct neuropsychological profile},
  author={Smits, Lieke L and Pijnenburg, Yolande AL and Koedam, Esther LGE and van der Vlies, Annelies E and Reuling, Ilona EW and Koene, Teddy and Teunissen, Charlotte E and Scheltens, Philip and van der Flier, Wiesje M},
  journal={Journal of Alzheimer's Disease},
  volume=30,
  number=1,
  pages={101--108},
  year=2012,
  publisher={IOS Press}
}
@article{hur2020innate,
  title={The innate immunity protein IFITM3 modulates $\gamma$-secretase in Alzheimer’s disease},
  author={Hur, Ji-Yeun and Frost, Georgia R and Wu, Xianzhong and Crump, Christina and Pan, Si Jia and Wong, Eitan and Barros, Marilia and Li, Thomas and Nie, Pengju and Zhai, Yujia and others},
  journal={Nature},
  pages={1--6},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{tharp2013origins,
  title={Origins of amyloid-$\beta$},
  author={Tharp, William G and Sarkar, Indra Neil},
  journal={BMC genomics},
  volume={14},
  number={1},
  pages={1--15},
  year={2013},
  publisher={BioMed Central}
}
@article{fulop2018can,
  title={Can an infection hypothesis explain the beta amyloid hypothesis of Alzheimer’s disease?},
  author={Fulop, Tamas and Witkowski, Jacek M and Bourgade, Karine and Khalil, Abdelouahed and Zerif, Echarki and Larbi, Anis and Hirokawa, Katsuiku and Pawelec, Graham and Bocti, Christian and Lacombe, Guy and others},
  journal={Frontiers in aging neuroscience},
  volume={10},
  pages={224},
  year={2018},
  publisher={Frontiers}
}
@article{frisoni2010clinical,
  title={The clinical use of structural MRI in Alzheimer disease},
  author={Frisoni, Giovanni B and Fox, Nick C and Jack, Clifford R and Scheltens, Philip and Thompson, Paul M},
  journal={Nature Reviews Neurology},
  volume={6},
  number={2},
  pages={67--77},
  year={2010},
  publisher={Nature Publishing Group}
}
@article{dawkins2014insights,
  title={Insights into the physiological function of the $\beta$-amyloid precursor protein: beyond Alzheimer's disease},
  author={Dawkins, Edgar and Small, David H},
  journal={Journal of neurochemistry},
  volume={129},
  number={5},
  pages={756--769},
  year={2014},
  publisher={Wiley Online Library}
}
@article{da2016insights,
  title={Insights on the pathophysiology of Alzheimer's disease: The crosstalk between amyloid pathology, neuroinflammation and the peripheral immune system},
  author={D{\'a} Mesquita, Sandro and Ferreira, Ana Catarina and Sousa, Jo{\~a}o Carlos and Correia-Neves, Margarida and Sousa, Nuno and Marques, Fernanda},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={68},
  pages={547--562},
  year={2016},
  publisher={Elsevier}
}
@article{poulakis2018heterogeneous,
  title={Heterogeneous patterns of brain atrophy in Alzheimer's disease},
  author={Poulakis, Konstantinos and Pereira, Joana B and Mecocci, Patrizia and Vellas, Bruno and Tsolaki, Magda and K{\l}oszewska, Iwona and Soininen, Hilkka and Lovestone, Simon and Simmons, Andrew and Wahlund, Lars-Olof and others},
  journal={Neurobiology of aging},
  volume={65},
  pages={98--108},
  year={2018},
  publisher={Elsevier}
}
@book{poincare1895analysis,
  title={Analysis situs},
  author={Poincar{\'e}, Henri},
  year={1895},
  publisher={Gauthier-Villars}
}
@article{dey1999computational,
  title={Computational topology},
  author={Dey, Tamal K and Edelsbrunner, Herbert and Guha, Sumanta},
  journal={Contemporary mathematics},
  volume=223,
  pages={109--144},
  year=1999,
  publisher={Providence, RI: American Mathematical Society}
}
@book{james1999history,
  title={History of topology},
  author={James, Ioan Mackenzie},
  year={1999},
  publisher={Elsevier}
}
@article{freedman2009algebraic,
  title={Algebraic topology for computer vision},
  author={Freedman, Daniel and Chen, Chao},
  journal={Computer Vision},
  pages={239--268},
  year={2009}
}
@book{edelsbrunner2010computational,
  title={Computational topology: an introduction},
  author={Edelsbrunner, Herbert and Harer, John},
  year={2010},
  publisher={American Mathematical Soc.}
}
@article{adams2017persistence,
  title={Persistence images: A stable vector representation of persistent homology},
  author={Adams, Henry and Emerson, Tegan and Kirby, Michael and Neville, Rachel and Peterson, Chris and Shipman, Patrick and Chepushtanova, Sofya and Hanson, Eric and Motta, Francis and Ziegelmeier, Lori},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={218--252},
  year={2017},
  publisher={JMLR. org}
}
@article{mileyko2011probability,
  title={Probability measures on the space of persistence diagrams},
  author={Mileyko, Yuriy and Mukherjee, Sayan and Harer, John},
  journal={Inverse Problems},
  volume={27},
  number={12},
  pages={124007},
  year={2011},
  publisher={IOP Publishing}
}
@article{ghrist2008barcodes,
  title={Barcodes: the persistent topology of data},
  author={Ghrist, Robert},
  journal={Bulletin of the American Mathematical Society},
  volume={45},
  number={1},
  pages={61--75},
  year={2008}
}
@incollection{bubenik2020persistence,
  title={The persistence landscape and some of its properties},
  author={Bubenik, Peter},
  booktitle={Topological Data Analysis},
  pages={97--117},
  year={2020},
  publisher={Springer}
}
@article{bubenik2015statistical,
  title={Statistical topological data analysis using persistence landscapes},
  author={Bubenik, Peter},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={77--102},
  year={2015},
  publisher={JMLR. org}
}
@article{bubenik2017persistence,
  title={A persistence landscapes toolbox for topological statistics},
  author={Bubenik, Peter and D{\l}otko, Pawe{\l}},
  journal={Journal of Symbolic Computation},
  volume={78},
  pages={91--114},
  year={2017},
  publisher={Elsevier}
}
@article{berwald2018computing,
  title={Computing Wasserstein distance for persistence diagrams on a quantum computer},
  author={Berwald, Jesse J and Gottlieb, Joel M and Munch, Elizabeth},
  journal={arXiv preprint arXiv:1809.06433},
  year={2018}
}
@article{goedert2006century,
  title={A century of Alzheimer's disease},
  author={Goedert, Michel and Spillantini, Maria Grazia},
  journal={science},
  volume={314},
  number={5800},
  pages={777--781},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@article{jack2008alzheimer,
  title={The Alzheimer's disease neuroimaging initiative (ADNI): MRI methods},
  author={Jack Jr, Clifford R and Bernstein, Matt A and Fox, Nick C and Thompson, Paul and Alexander, Gene and Harvey, Danielle and Borowski, Bret and Britson, Paula J and L. Whitwell, Jennifer and Ward, Chadwick and others},
  journal={Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine},
  volume={27},
  number={4},
  pages={685--691},
  year={2008},
  publisher={Wiley Online Library}
}
@misc{tauzin2020giottotda,
      title={giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration},
      author={Guillaume Tauzin and Umberto Lupo and Lewis Tunstall and Julian Burella Pérez and Matteo Caorsi and Anibal Medina-Mardones and Alberto Dassatti and Kathryn Hess},
      year={2020},
      eprint={2004.02551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Franpo\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}
@article{wen2020convolutional,
  title={Convolutional Neural Networks for Classification of Alzheimer's Disease: Overview and Reproducible Evaluation},
  author={Wen, Junhao and Thibeau-Sutre, Elina and Diaz-Melo, Mauricio and Samper-Gonz{\'a}lez, Jorge and Routier, Alexandre and Bottani, Simona and Dormont, Didier and Durrleman, Stanley and Burgos, Ninon and Colliot, Olivier and others},
  journal={Medical Image Analysis},
  pages={101694},
  year={2020},
  publisher={Elsevier}
}
@article{tijms2020pathophysiological,
  title={Pathophysiological subtypes of Alzheimer's disease based on cerebrospinal fluid proteomics.},
  author={Tijms, Betty M and Gobom, Johan and Reus, Lianne and Jansen, Iris and Hong, Shengjun and Dobricic, Valerija and Kilpert, Fabian and ten Kate, Mara and Barkhof, Frederik and Tsolaki, Magda and others},
  journal={medRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory Press}
}
@article{poulakis2018heterogeneous,
  title={Heterogeneous patterns of brain atrophy in Alzheimer's disease},
  author={Poulakis, Konstantinos and Pereira, Joana B and Mecocci, Patrizia and Vellas, Bruno and Tsolaki, Magda and K{\l}oszewska, Iwona and Soininen, Hilkka and Lovestone, Simon and Simmons, Andrew and Wahlund, Lars-Olof and others},
  journal={Neurobiology of aging},
  volume={65},
  pages={98--108},
  year={2018},
  publisher={Elsevier}
}
@article{min2018survey,
  title={A survey of clustering with deep learning: From the perspective of network architecture},
  author={Min, Erxue and Guo, Xifeng and Liu, Qiang and Zhang, Gen and Cui, Jianjing and Long, Jun},
  journal={IEEE Access},
  volume={6},
  pages={39501--39514},
  year={2018},
  publisher={IEEE}
}
@inproceedings{hofer2017deep,
  title={Deep learning with topological signatures},
  author={Hofer, Christoph and Kwitt, Roland and Niethammer, Marc and Uhl, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1634--1644},
  year={2017}
}
@article{scarmeas2004cognitive,
  title={Cognitive reserve: implications for diagnosis and prevention of Alzheimer’s disease},
  author={Scarmeas, Nikolaos and Stern, Yaakov},
  journal={Current neurology and neuroscience reports},
  volume={4},
  number={5},
  pages={374--380},
  year={2004},
  publisher={Springer}
}
@article{van2017neuroimaging,
  title={A neuroimaging approach to capture cognitive reserve: application to Alzheimer's disease},
  author={van Loenhoud, Anna C and Wink, Alle Meije and Groot, Colin and Verfaillie, Sander CJ and Twisk, Jos and Barkhof, Frederik and van Berckel, Bart and Scheltens, Philip and van der Flier, Wiesje M and Ossenkoppele, Rik},
  journal={Human brain mapping},
  volume={38},
  number={9},
  pages={4703--4715},
  year={2017},
  publisher={Wiley Online Library}
}
@article{gauthier2006mild,
  title={Mild cognitive impairment},
  author={Gauthier, Serge and Reisberg, Barry and Zaudig, Michael and Petersen, Ronald C and Ritchie, Karen and Broich, Karl and Belleville, Sylvie and Brodaty, Henry and Bennett, David and Chertkow, Howard and others},
  journal={The lancet},
  volume={367},
  number={9518},
  pages={1262--1270},
  year={2006},
  publisher={Elsevier}
}
@article{bruningk2020image,
  title={Image analysis for Alzheimer's disease prediction: Embracing pathological hallmarks for model architecture design},
  author={Br{\"u}ningk, Sarah C and Hensel, Felix and Jutzeler, Catherine R and Rieck, Bastian},
  journal={arXiv preprint arXiv:2011.06531},
  year={2020}
}
@article{liu2018anatomical,
  title={Anatomical landmark based deep feature representation for MR images in brain disease diagnosis},
  author={Liu, Mingxia and Zhang, Jun and Nie, Dong and Yap, Pew-Thian and Shen, Dinggang},
  journal={IEEE journal of biomedical and health informatics},
  volume={22},
  number={5},
  pages={1476--1485},
  year={2018},
  publisher={IEEE}
}
@article{mrabah2019deep,
  title={Deep clustering with a dynamic autoencoder},
  author={Mrabah, Nairouz and Khan, Naimul Mefraz and Ksantini, Riadh and Lachiri, Z},
  journal={CoRR},
  year={2019}
}
@article{collins19943d,
  title={3D Model-based segmentation of individual brain structures from magnetic resonance imaging data},
  author={Collins, D Louis},
  year={1994},
  publisher={McGill University}
}
@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year=1947,
  publisher={JSTOR}
}
@article{halliday1998regional,
  title={Regional specificity of brain atrophy in Huntington's disease},
  author={Halliday, GM and McRitchie, DA and Macdonald, V and Double, KL and Trent, RJ and McCusker, E},
  journal={Experimental neurology},
  volume={154},
  number={2},
  pages={663--672},
  year={1998},
  publisher={Elsevier}
}
@article{kuhl1982cerebral,
  title={Cerebral metabolism and atrophy in Huntington's disease determined by 18FDG and computed tomographic scan},
  author={Kuhl, David E and Phelps, Michael E and Markham, Charles H and Metter, E Jeffrey and Riege, Walter H and Winter, James},
  journal={Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society},
  volume={12},
  number={5},
  pages={425--434},
  year={1982},
  publisher={Wiley Online Library}
}
@article{kassubek2004topography,
  title={Topography of cerebral atrophy in early Huntington’s disease: a voxel based morphometric MRI study},
  author={Kassubek, J and Juengling, FD and Kioschies, T and Henkel, K and Karitzky, J and Kramer, B and Ecker, D and Andrich, J and Saft, C and Kraus, P and others},
  journal={Journal of Neurology, Neurosurgery \& Psychiatry},
  volume={75},
  number={2},
  pages={213--220},
  year={2004},
  publisher={BMJ Publishing Group Ltd}
}
@article{pini2016brain,
  title={Brain atrophy in Alzheimer’s disease and aging},
  author={Pini, Lorenzo and Pievani, Michela and Bocchetta, Martina and Altomare, Daniele and Bosco, Paolo and Cavedo, Enrica and Galluzzi, Samantha and Marizzoni, Moira and Frisoni, Giovanni B},
  journal={Ageing research reviews},
  volume={30},
  pages={25--48},
  year={2016},
  publisher={Elsevier}
}
@article{skraba2020wasserstein,
  title={Wasserstein Stability for Persistence Diagrams},
  author={Skraba, Primoz and Turner, Katharine},
  journal={arXiv preprint arXiv:2006.16824},
  year={2020}
}
@article{cohen2007stability,
  title={Stability of persistence diagrams},
  author={Cohen-Steiner, David and Edelsbrunner, Herbert and Harer, John},
  journal={Discrete \& computational geometry},
  volume={37},
  number={1},
  pages={103--120},
  year={2007},
  publisher={Springer}
}
@article{shapiro1965analysis,
  title={An analysis of variance test for normality (complete samples)},
  author={Shapiro, Samuel Sanford and Wilk, Martin B},
  journal={Biometrika},
  volume={52},
  number={3/4},
  pages={591--611},
  year={1965},
  publisher={JSTOR}
}
@article{vanherpe2016framework,
  title={Framework for efficient synthesis of spatially embedded morphologies},
  author={Vanherpe, Liesbeth and Kanari, Lida and Atenekeng, Guy and Palacios, Juan and Shillcock, Julian},
  journal={Physical Review E},
  volume=94,
  number=2,
  pages=023315,
  year=2016,
  publisher={APS}
}
#+end_src

topology & geometrical analysis links
#+begin_src bibtex
@article{huguet2022time,
  title={Time-inhomogeneous diffusion geometry and topology},
  author={Huguet, Guillaume and Tong, Alexander and Rieck, Bastian and Huang, Jessie and Kuchroo, Manik and Hirn, Matthew and Wolf, Guy and Krishnaswamy, Smita},
  journal={arXiv preprint arXiv:2203.14860},
  year={2022}
}
#+end_src

original paper from vietoris on VR complex filtrations
#+begin_src bibtex
@article{vietoris1927hoheren,
  title={{{\"U}ber den h{\"o}heren Zusammenhang kompakter R{\"a}ume und eine Klasse von zusammenhangstreuen Abbildungen}},
  author={Vietoris, Leopold},
  journal={Mathematische Annalen},
  volume={97},
  number={1},
  pages={454--472},
  year={1927},
  publisher={Springer}
}
#+end_src

original paper by goodfellow on GANs
#+begin_src bibtex
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
#+end_src

original VAE paper

#+begin_src bibtex
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
#+end_src

SOTA GAN
#+begin_src bibtex
@article{sauer2022stylegan,
  title={StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
  author={Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
  journal={arXiv preprint arXiv:2202.00273},
  year={2022}
}
#+end_src

Review of techniques used to do generative modelling
#+begin_src bibtex
@article{bond2021deep,
  title={Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models},
  author={Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G},
  journal={arXiv preprint arXiv:2103.04922},
  year={2021}
}
#+end_src

Inception v3 (on arxiv) paper used as inputs to GANs. Network primarily used for
object detection

#+begin_src bibtex
@article{szegedy2015rethinking,
  title={Rethinking the inception architecture for computer vision. 2015},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  journal={arXiv preprint arXiv:1512.00567},
  year={2015},
  publisher={CoRR}
}
#+end_src

Graph embedding method review

#+begin_src bibtex
@article{xu2021understanding,
  title={Understanding graph embedding methods and their applications},
  author={Xu, Mengjia},
  journal={SIAM Review},
  volume={63},
  number={4},
  pages={825--853},
  year={2021},
  publisher={SIAM}
}
#+end_src

#+begin_src bibtex
@article{preuer2018frechet,
  title={Fr{\'e}chet ChemNet distance: a metric for generative models for molecules in drug discovery},
  author={Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  journal={Journal of chemical information and modeling},
  volume={58},
  number={9},
  pages={1736--1741},
  year={2018},
  publisher={ACS Publications}
}
#+end_src


GNNs are able to extract meaningful representations without any training:
#+begin_src bibtex

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}
@inproceedings{morris2019weisfeiler,
  title={Weisfeiler and leman go neural: Higher-order graph neural networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4602--4609},
  year={2019}
}
@article{xu2018powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:1810.00826},
  year={2018}
}
#+end_src

Empirical studies of graph neural networks are keys xu2018empirical and o2021evaluation

Original Weisfeiler-Lehman paper.

#+begin_src bibtex
@article{shervashidze2011weisfeiler,
  title={Weisfeiler-lehman graph kernels.},
  author={Shervashidze, Nino and Schweitzer, Pascal and Van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={9},
  year={2011}
}
#+end_src

* References
#+LaTeX: \printbibliography[heading=none]
