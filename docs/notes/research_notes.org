#+BIND: org-export-use-babel nil
#+TITLE: Research notes on metrics for GNNs appliedp to biological problems
#+AUTHOR: Philip Hartout
#+EMAIL: <philip.hartout@protonmail.com>
#+DATE: Monday January 24, 2022
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS:[a4paper,12pt,twoside]
#+LaTeX_HEADER:\usepackage[usenames,dvipsnames,figures]{xcolor}
#+LaTeX_HEADER:\usepackage[autostyle]{csquotes}
#+LaTeX_HEADER:\usepackage[final]{pdfpages}
#+LaTeX_HEADER:\usepackage{amsfonts, amssymb}            % Math symbols
#+LaTeX_HEADER:\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
#+LATEX_HEADER_EXTRA:\hypersetup{colorlinks=false, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black}
#+LATEX_HEADER_EXTRA:\newtheorem{definition}{Definition}[section]
#+MACRO: NEWLINE @@latex:\@@ @@html:<br>@@
#+PROPERTY: header-args :exports both :session python_emacs_session :cache :results value
#+PROPERTY: header-args :exports none :tangle "~/Documents/Git/msc_thesis/thesis/refs.bib"
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+LATEX_COMPILER: pdflatexorg-mode restarted

#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage[natbib=true]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Documents/Git/msc_thesis/thesis/refs.bib}
#+LATEX_HEADER: \usepackage{parskip}
#+OPTIONS: <:nil c:nil todo:nil H:5

* Generative modelling
** Methods
*** Real-valued non volume-preserving used on images
#+begin_src bibtex
@article{dinh2016density,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}
#+end_src
* Graph Neural Networks
** Reviews
*** Graph neural networks:
    A review of methods and applications
#+begin_src bibtex
@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
#+end_src
Zhou et al mention several GNN approaches in use today
Generative models popular today:
**** Sequential graph generation process
 + GraphRNN - generates the adjacency matrix of a graph by generating the adjacency vector of each node step by step, with graph outputs with different number of nodes.
 #+begin_src bibtex
@inproceedings{you2018graphrnn,
  title={Graphrnn: Generating realistic graphs with deep auto-regressive models},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  booktitle={International conference on machine learning},
  pages={5708--5717},
  year={2018},
  organization={PMLR}
}

 #+end_src
 + Li 2018 - also generates nodes and edges sequentially uses the hidden state to decide what to do at the next step
 #+begin_src bibtex
@article{li2018learning,
  title={Learning deep generative models of graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  journal={arXiv preprint arXiv:1803.03324},
  year={2018}
}

 #+end_src
 + GraphAF - also a sequential process, Conducts a validity check of each molecule generated at each step to see if it's valid.
 #+begin_src bibtex
@article{shi2020graphaf,
  title={{GraphAF: a flow-based autoregressive model for molecular graph generation}},
  author={Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  journal={arXiv preprint arXiv:2001.09382},
  year={2020}
}
 #+end_src
**** Non-sequential graph generation process
 + MolGAN - to generate small molecules. Uses a permutation-invariant to solve the node adjacency matrix at once. Also implements an RL-based optimization toward desired chemical properties
#+begin_src bibtex
@article{de2018molgan,
  title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}
#+end_src
 + Ma et al 2018 - constrained VAE for semantic validity of generated graph
   #+begin_src bibtex
@article{ma2018constrained,
  title={Constrained generation of semantically valid graphs via regularizing variational autoencoders},
  author={Ma, Tengfei and Chen, Jie and Xiao, Cao},
  journal={arXiv preprint arXiv:1809.02630},
  year={2018}
}

   #+end_src
 + GCPN similar to MolGAN, uses RL based methods to ensure validity of domain-specific rules
   #+begin_src bibtex
@article{you2018graph,
  title={Graph convolutional policy network for goal-directed molecular graph generation},
  author={You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  journal={arXiv preprint arXiv:1806.02473},
  year={2018}
}
   #+end_src
 + Graph Normalizing Flows
   #+begin_src bibtex
@article{liu2019graph,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={arXiv preprint arXiv:1905.13177},
  year={2019}
}
   #+end_src
This one has a fairly comprehensive website: https://sites.google.com/view/graph-normalizing-flows/
Full architecture

#+NAME: fig:figure name
#+CAPTION: figure name
#+ATTR_ORG: :width 400
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200
[[./images/full_arch_gnf.png]]
 + Graphite isotropic gaussian for VAE + iterative refinement for decoding
   #+begin_src bibtex
@inproceedings{grover2019graphite,
  title={Graphite: Iterative generative modeling of graphs},
  author={Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  booktitle={International conference on machine learning},
  pages={2434--2444},
  year={2019},
  organization={PMLR}
}
   #+end_src

** Three most popular according to O'Bray 2021:
+ GraphRNN, GRAN, Graph Score Matching.
+ Graph Recurrent Attention Networks
  #+begin_src bibtex
@article{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S},
  journal={arXiv preprint arXiv:1910.00760},
  year={2019}
}
  #+end_src
+ Graph Score Matching
  #+begin_src bibtex
@inproceedings{niu2020permutation,
  title={Permutation invariant graph generation via score-based generative modeling},
  author={Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4474--4484},
  year={2020},
  organization={PMLR}
}
  #+end_src
* Objective:
** Generative graph dist close to the input graph dist
** (pseudo)-metric to assess dissimilarity between G (generated graphs) and G* (input graphs)

* Criteria for good metrics:
1. Robust to noise
2. Expressive, if they don't arise from the same dist, then metric should detect this.
3. Computationally efficient.

* Problems with Frechet Inception Distance
+ Used only for images.
+ Perceived differences are not possible for graphs.
+ How about LPIPS?

* Why comparing graphs is hard:
+ Metrics need to deal with spatial invariances such as cycles.
+ Graph edit distance is NP-hard (Zeng 2009) and therefore does not satisfy efficiency criterion.

* MMD - current accepted method to evaluate generative GNNs
+ The MMD formula goes as follows:\begin{equation*}
$\text{MMD}(X, Y) := {1\over n^2} \sum_{i,j=1}^{n}k(x_i, x_j) + {1\over m^2} \sum_{i,j=1}^{n}k(y_i, y_j) - {2\over nm} \sum_{i=1}^{n}\sum_{j=1}^{m}k(y_i, y_j)$
+ use it for hypothesis/two-sample testing.
+ In practice, we evaluate $d_{MMD}(\mathcal{G},\mathcal{G*}) :=
  MMD(f(\mathcal{G}),f(\mathcal{G}*))$ for a distribution $\mathcal{G}$. Given
  multiple distributions $G_1, G_2, \hdots$, the values of $d_{MMD}$ can be used
  to rank models, where smaller values are assumed to indicate a larger
  agreement with the original distribution $\mathcal{G}*$.
+ Commonly used kernels: first Wasserstein distance, total variation distance,
  radial basis function.
+ Commonly used descriptor functions: degree distribution histogram, clustering
  coefficient, laplacian spectrum histogram.
** Pitfalls of descriptors
+ Degree distributions are ok seemingly
+ Custering does not distinguish fully connected vs disconnected cliques
+ Spectral methods are not clearly expressive. Does not seem to be for certain classes of graphs.

* MMD pitfalls
** Parameters and descriptors are set a priori in the best case
** Model performance is highly dependent on parameters and descriptor functions.



* Computer Vision
** Image-to-Image Translation
*** CycleGAN: Unpaired Image-to-Image Translation using
        Cycle-Consistent Adversarial Networks citep:zhu2017CycleGAN.



        #+begin_src bibtex
@inproceedings{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}
        #+end_src
  This is one of my favorite papers. The authors extending some
  of the classic work done in Pix2Pix citep:isola2017pix2pix to
  /unpaired/ sets of images. At the core of the CycleGAN
  procedure are two Generative Adversarial Networks that learn
  to map images between two domains. The key addition that makes
  this process work is an additional loss term, which enforces
  that images passed through both generators should be as close
  as possible to the input image. This has practical motivation:
  if we translate one way and then translate back, we should
  expect the input to be unchanged. The results are impressive
  and eye catching. This work inspired a paper of mine:
  GeneSIS-RT.

* References
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
  #+LaTeX: \printbibliography[heading=none]
